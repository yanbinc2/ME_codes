{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; Feb, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 5 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@stat.sinica.edu.tw </p>\n",
    "### <p> Institute of Statistical Science, Academia Sinica, Taipei, Taiwan.</p>  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import random \n",
    "import time\n",
    "import datetime\n",
    "from itertools import chain\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "from CNN_Modules_1D import ME_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "MNIST     = True\n",
    "NUM_CASE  = 1\n",
    "INTE_bool = False  #True: Integrate two networks VGG+ResNet    False: single network\n",
    "SAVE_bool = True\n",
    "ITE_FROM  = 5 # This setting is ONLY for Integration\n",
    "# if set to Integrated networks, we need two files \"case1/accu_history.csv\" and \"case1/merged_region_image_9.pickle\"\n",
    "#Region_Index_loc = 6 # column location; Others is 6\n",
    "REGION_INDEX_LOC = 4 # column location; MNIST is 4\n",
    "\n",
    "\n",
    "PATH4='../phase3/data/MNIST_Labels_Spec20.csv'\n",
    "PATH5='../phase3/data/Small_MNIST_tSNE_embeddings.pickle'\n",
    "PATH6='./data/MNIST_mergedseedclasslabels.txt'\n",
    "PATH7='../phase3/data/region_for_phase5.pickle'\n",
    "if (INTE_bool):\n",
    "    ITE_START=ITE_FROM\n",
    "    ITE_END=10\n",
    "else:\n",
    "    ITE_START=0\n",
    "    ITE_END=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_csv(x, path):\n",
    "    with open(path,'a+', newline='') as f:\n",
    "        csv_file = csv.writer(f)#   = f.write()\n",
    "        csv_file.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only for single network. No necessary in Integrated networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not INTE_bool):\n",
    "    def create_image_0(PATH6, case_i):\n",
    "        # ===================\n",
    "        #\n",
    "        #  prepare  merged_region_image_0\n",
    "        #\n",
    "        #====================\n",
    "        # (A)\n",
    "        #get \"(1)merged_region\"      only seed regions, no neighboring regions\n",
    "        df = pandas.read_csv(PATH6, delim_whitespace=' ', header=0,  index_col=None)\n",
    "        table = df.to_numpy()\n",
    "        print(\"mergedseedclasslabels table\")\n",
    "        display(table)\n",
    "\n",
    "        merged_region=[]\n",
    "        for i in range(min(table.T[case_i+1]), max(table.T[case_i+1])+1):  #18 ---merge to --> 10\n",
    "            addr=np.where(table.T[case_i+1]==i)[0] # 2nd column equal to 0(min),1,2,3...10(max); DO NOT consider 3rd column, which is hidden\n",
    "            if(len(addr) and i>0): #if not empty and i=0 is the invalid seed region.\n",
    "                merged_region.append(table[addr][:,0].tolist())\n",
    "        print(\"merged_region\")\n",
    "        display(merged_region)\n",
    "\n",
    "\n",
    "        # (B)\n",
    "        #get \"merged_reg_and_nei\"\n",
    "        #get \"merged_reg_and_nei_image\"\n",
    "        #generate \"merged_region_image_0.pickle\"\n",
    "\n",
    "        # (B_a)=== without neighbors ====\n",
    "        #if ((DATASET == 2) or (DATASET == 4)): \n",
    "        ##20240105\n",
    "        if (not True): \n",
    "            # ==== collect regions. No neighbors, just use merged regions ====\n",
    "            merged_reg_and_nei=merged_region.copy()\n",
    "\n",
    "            # ==== collect images ====\n",
    "            img_temp=[]\n",
    "            for i in range(len(merged_region)):\n",
    "                addr=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    temp=np.where(all_region_index==merged_region[i][j])[0].tolist()   #tolist(): convert temp into list\n",
    "                    addr=addr+temp\n",
    "                    print(len(temp),end=' ')\n",
    "                img_temp.append(addr)\n",
    "                print(\"=\",len(img_temp[i]))\n",
    "            merged_reg_and_nei_image = img_temp.copy()\n",
    "\n",
    "\n",
    "        # (B_b)=== with neighbors ==== \n",
    "        else: \n",
    "            with open(PATH7, 'rb') as f:\n",
    "                pre_region, pre_reg_nei, pre_region_image_pure, pre_region_image= pickle.load(f)\n",
    "            #    1reg         2reg+nei        1's img            2's img\n",
    "\n",
    "            # ==== collect regions with neighbors====\n",
    "            # remove duplicate  -->  https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them\n",
    "            merged_reg_and_nei=[]\n",
    "            NUM_region=len(merged_region)\n",
    "            for i in range(NUM_region):\n",
    "                temp=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0] \n",
    "                    temp=temp+pre_reg_nei[idx]\n",
    "                    print(idx,pre_region[idx])\n",
    "                merged_reg_and_nei.append(temp)\n",
    "\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(merged_reg_and_nei[i]) != len(set(merged_reg_and_nei[i]))):\n",
    "                    a=merged_reg_and_nei[i].copy()\n",
    "\n",
    "                    # find the duplicate.\n",
    "                    seen = set()\n",
    "                    dupli                 = [x for x in a if (x in seen or seen.add(x))]\n",
    "                    print(\"***duplicates:\",dupli)\n",
    "\n",
    "                    # keep fisrt one, remove succeeding duplicates.\n",
    "                    seen = set()\n",
    "                    merged_reg_and_nei[i] = [x for x in a if not (x in seen or seen.add(x))]  # a is the data to process; x is a working varialbe\n",
    "                    print(\"unique:\",merged_reg_and_nei[i])\n",
    "\n",
    "                print(\"total\",len(merged_reg_and_nei[i]),end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei\")\n",
    "            for i in range(len(merged_reg_and_nei)):\n",
    "                print(merged_reg_and_nei[i])\n",
    "\n",
    "\n",
    "            # Collect images\n",
    "            merged_reg_and_nei_image=[]\n",
    "            for i in range(NUM_region):\n",
    "                #search and add\n",
    "                img=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0]\n",
    "                    print(len(pre_region_image[idx]),\"(\",idx,\")\",end=' ')\n",
    "                    img=img+pre_region_image[idx] \n",
    "                print(\"=\",len(img),end=\" \")\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(img) != len(set(img))):\n",
    "                    img=list(set(img)) #remove duplicates\n",
    "                    print(\"     **duplicate, shrink to\",len(img),end=\"\\n\")  \n",
    "                else:\n",
    "                    print(end=\"\\n\")\n",
    "\n",
    "                #append\n",
    "                merged_reg_and_nei_image.append(img)\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei_image\")\n",
    "            for i in range(len(merged_reg_and_nei_image)):\n",
    "                print(len(merged_reg_and_nei_image[i]),merged_reg_and_nei_image[i][:5],\"...\")\n",
    "\n",
    "        # save\n",
    "        if (SAVE_bool):\n",
    "            with open(newpath+'/merged_region_image_0.pickle', 'wb') as f:\n",
    "                pickle.dump([merged_reg_and_nei, merged_reg_and_nei_image], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_part(PATH5,ITE):\n",
    "    TRIALS          = 5\n",
    "\n",
    "    savelog_path = newpath+'/' + 'log.txt'\n",
    "\n",
    "    # ==== test_array ====\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "\n",
    "    #20240105\n",
    "    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    \n",
    "    #if((DATASET==2) or (DATASET==4)):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #elif(DATASET==1):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #    test_array /= 255\n",
    "    #elif(DATASET==0):\n",
    "    #    test_array /= 255\n",
    "    #display(np.shape(test_array))\n",
    "\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    region_image=merged_region_image.copy()\n",
    "    del merged_reg_and_nei\n",
    "\n",
    "\n",
    "    NUM_region=len(region_image)\n",
    "    print(\"NUM_region\",NUM_region)\n",
    "\n",
    "\n",
    "    from itertools import chain\n",
    "    region_image_flatten=list(chain.from_iterable(region_image))\n",
    "    print(\"number of clean images\",len(region_image_flatten))\n",
    "\n",
    "\n",
    "    ROUND_start = time.time()\n",
    "    #========  merge ==========\n",
    "    #prepare selected_region, region\n",
    "    for n in range(1): #extra_original\n",
    "    #   #reset\n",
    "        region=region_image.copy()\n",
    "        region=list(region)\n",
    "        selected_region = list(range(NUM_region))  #[0,1,2, ... ,29]\n",
    "\n",
    "        #merge\n",
    "        if (n > 4):\n",
    "            p1=comb[n-1][0]\n",
    "            p2=comb[n-1][1]\n",
    "            region[p1]=region[p1]+region[p2]\n",
    "            region.pop(p2)\n",
    "            selected_region.pop(-1)  # remove last region index\n",
    "        #original\n",
    "        else:  #n=0\n",
    "            p1=0\n",
    "            p2=0\n",
    "\n",
    "        print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "        # ===== one CNN =============\n",
    "        NUM_CLASSES = len(selected_region)  #NUM_CLASSES should be here to update for each loop\n",
    "\n",
    "        # input image and label\n",
    "        Input_img     = []\n",
    "        Input_img_len = []\n",
    "        for c,sel in enumerate(selected_region, start=0):\n",
    "            Input_img = Input_img + list(region[sel])\n",
    "            Input_img_len.append(len(region[sel])) #can only concatenate list (not \"int\") to list\n",
    "\n",
    "        #20240105\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W)  \n",
    "        train_array = np.expand_dims(train_array, axis = -1)\n",
    "            \n",
    "        # fill up the training label to each training image\n",
    "        current_train_label = np.zeros(len(train_array), dtype=int)  # Assign 0 to the label\n",
    "        accum_base=0  #accumulate\n",
    "        for label in range(1,NUM_CLASSES):\n",
    "            sector = Input_img_len[label-1]\n",
    "            accum_base = accum_base + sector  # sector is the sector length\n",
    "            current_train_label[accum_base:] = label  # fill the label\n",
    "\n",
    "\n",
    "        # CNN\n",
    "        #===============================================\n",
    "        one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "        one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "        model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "        for r in range(TRIALS):  #10\n",
    "            one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                    x_train     = train_array,\n",
    "                    train_label = current_train_label,\n",
    "                    test_array  = test_array,\n",
    "                    true_answer = test_label_answer,\n",
    "                    Num_Classes = NUM_CLASSES\n",
    "                    )\n",
    "            print(type(model_history))\n",
    "\n",
    "\n",
    "            # ===== delete CNN tensors =====\n",
    "            from keras import backend as K\n",
    "            K.clear_session()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"One CNN, r: \",r)\n",
    "            ROUND_duration = time.time() - ROUND_start\n",
    "            print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "\n",
    "        # === save to file ===\n",
    "        #This is useless in phase IV. Prepare for further checking in the future.\n",
    "        savefile_path = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'_'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path, 'wb') as f:\n",
    "            pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "        savefile_path2 = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_5_tests_simple_ITE'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path2, 'wb') as f:\n",
    "            pickle.dump([one_predicted_results, one_predict_percentage], f)\n",
    "\n",
    "        # === save to log ===    \n",
    "        savelog = open(savelog_path, 'a+')\n",
    "        print(\"\\n\", savefile_path, file = savelog)\n",
    "        print(\"Saved parameters: Input_img, Input_img_len, one_predicted_results, one_predict_percentage\", file = savelog) #0722\n",
    "\n",
    "        # total time\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Completion time: \", datetime.datetime.now(), file = savelog)\n",
    "        print(\"Total Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)), file = savelog)\n",
    "\n",
    "        savelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_method(PATH5,NUM_region,region_label,table_1D):\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "    \n",
    "    dist_table_truth=np.zeros((NUM_region,NUM_region),dtype=int)\n",
    "    region_correct=[]\n",
    "    region_amount=[]\n",
    "    overall_correct=0\n",
    "    overall_amount=0\n",
    "\n",
    "    for i in range(NUM_region):\n",
    "        #(1) input\n",
    "        region_image=np.where(table_1D==i)[0]\n",
    "        #region_image=merged_region_image[i].copy()\n",
    "        \n",
    "        #(2) establish confusion matrix\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(test_label_answer[region_image]==j)[0]) #the number of images which equals to true answer \n",
    "        \n",
    "        #(3) statisitc\n",
    "        region_correct.append(dist_table_truth[i][region_label[i]])\n",
    "        region_amount.append(len(region_image))\n",
    "      \n",
    "    #(4) statistic for overall\n",
    "    overall_correct=sum(region_correct)\n",
    "    overall_amount=sum(region_amount)\n",
    "\n",
    "    return region_correct,region_amount,overall_correct,overall_amount,dist_table_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(PATH5,ITE):\n",
    "    # input 1:\n",
    "    # (1)merged_region_image_(ITE)\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    del merged_reg_and_nei\n",
    "    NUM_region=len(merged_region_image)\n",
    "    \n",
    "    # (2)test_label_answer\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "    # (3)get consistent result table\n",
    "    with open(newpath+'/(classes=' + str(NUM_region) + ')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "    LENGTH=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    " \n",
    "    # (4) get region_label \n",
    "    region_label=[] #true label by selecting dominate ones\n",
    "    for i in range(NUM_region):\n",
    "        region_image=merged_region_image[i].copy()\n",
    "        region_label.append(collections.Counter(test_label_answer[region_image]).most_common()[0][0])  #images --> true label --> most_common label\n",
    "\n",
    "\n",
    "   #========================================     \n",
    "   # (1)train + test\n",
    "    a2,b2,c2,d2,e2=statistic_method(PATH5,NUM_region,region_label,Original_result)\n",
    "    na2=np.asarray(a2)\n",
    "    nb2=np.asarray(b2)\n",
    "    nc2=np.asarray(c2)\n",
    "    nd2=np.asarray(d2)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c2, d2,      round(nc2/nd2    ,3), \"5con over all, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c2, all_num, round(nc2/all_num,3), \"5con over all\"], csv_path1)\n",
    " \n",
    "\n",
    "        \n",
    "    # (2)train\n",
    "    train_results=-1*np.ones(LENGTH,dtype=int)\n",
    "    for i in range(NUM_region):\n",
    "        images=merged_region_image[i]\n",
    "        train_results[images]=i\n",
    "        print(\"num of merged_region_image\",i,len(merged_region_image[i]))\n",
    "    print(collections.Counter(train_results))\n",
    "        \n",
    "    a1,b1,c1,d1,e1=statistic_method(PATH5,NUM_region,region_label,train_results)\n",
    "    na1=np.asarray(a1)  #region_correct\n",
    "    nb1=np.asarray(b1)  #region_amount\n",
    "    nc1=np.asarray(c1)  #overall_correct\n",
    "    nd1=np.asarray(d1)  #overall_amount\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c1, d1, round(nc1/nd1,3), \"5con over trained\"], csv_path1)\n",
    "    append_csv([ITE, c1, all_num, round(nc1/all_num,3), \"5con over all\"], csv_path1)\n",
    "    \n",
    "        \n",
    "    # (3)test\n",
    "    # remove training data(good images), only check test data(bad images)\n",
    "    from itertools import chain\n",
    "    used_image=merged_region_image.copy()\n",
    "    used_image=list(chain.from_iterable(used_image))\n",
    "    Original_result2=Original_result.copy()\n",
    "    Original_result2[used_image]=-2\n",
    "\n",
    "        \n",
    "    a4,b4,c4,d4,e4=statistic_method(PATH5,NUM_region,region_label, Original_result2)\n",
    "    na4=np.asarray(a4)\n",
    "    nb4=np.asarray(b4)\n",
    "    nc4=np.asarray(c4)\n",
    "    nd4=np.asarray(d4) #this is len(Original_result)-len(used_image)-len(unconsistent)\n",
    "    untrain=len(Original_result)-len(used_image)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c4, untrain, round(nc4/untrain, 3), \"5con over untrained, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c4, all_num, round(nc4/all_num, 3), \"5con over untrained (unclean)\"], csv_path1)\n",
    "\n",
    "        \n",
    "    # (4)set majority as label for each image        \n",
    "    # set majority from 5 trias as label for each image\n",
    "    predicted_results_major=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        predicted_results_major[i]=collections.Counter(one_predicted_results.T[i]).most_common()[0][0]\n",
    "    \n",
    "\n",
    "    a3,b3,c3,d3,e3=statistic_method(PATH5,NUM_region,region_label,predicted_results_major)\n",
    "    na3=np.asarray(a3)\n",
    "    nb3=np.asarray(b3)\n",
    "    nc3=np.asarray(c3)\n",
    "    nd3=np.asarray(d3) #this is all in majority criterion\n",
    "    append_csv([ITE, c3, d3, round(nc3/nd3    ,3), \"majo over all\"], csv_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_and_expand(PATH5,ITE):\n",
    "# all4.     \n",
    "    # load\n",
    "    with open('./region_initials.pickle', 'rb') as f:\n",
    "        all_region_index, all_region_image = pickle.load(f)\n",
    "    MAX_region = max(all_region_index)\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    NUM_region = len(merged_reg_and_nei) # NUM_region is the number of clusters\n",
    "\n",
    "    with open(newpath+'/(classes='+str(NUM_region)+')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "\n",
    "    # choose absolutely consistent images\n",
    "    NUM_test=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(NUM_test,dtype=int)\n",
    "\n",
    "    # (***)\n",
    "    # As set contains only unique elements, so convert the list to set.\n",
    "    # If set size is 1 then it means all elements in given list are same\n",
    "    for i in range(NUM_test):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    "    \n",
    "    used_img=list(chain.from_iterable(merged_region_image))\n",
    "    used_img=np.sort(used_img)\n",
    "    working_img = np.asarray(list(  set(range(NUM_test))-set(used_img)  ))  #working_img means the unclean ones for working on the further adding process\n",
    "    print(\"===========  ITE =\",ITE, \"  ===========\")    \n",
    "    print(\"used_img\",len(used_img), len(set(used_img)))    \n",
    "    print(\"working_img(=other images=unclean images)\",len(working_img), len(set(working_img)))\n",
    "\n",
    "    # save clean and unclean images\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/clean_and_unclean_image_ITE='+str(ITE)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([used_img, working_img], f) #used_img is clean, working_img is others\n",
    "\n",
    "    # other_regions\n",
    "    # ==== Process of other regions. Generate \"other_regions\" ====\n",
    "    merged_reg_and_nei_flatten=list(chain.from_iterable(merged_reg_and_nei))\n",
    "    print(\"merged regions\", len(merged_reg_and_nei_flatten), len(set(merged_reg_and_nei_flatten)))\n",
    "    other_regions       = list(  set(range(1,MAX_region+1))-set(merged_reg_and_nei_flatten)  ) #region index exclude used regions. 1 to 200.\n",
    "    print(\"other_regions\",len(other_regions), len(set(other_regions)))\n",
    "    \n",
    "    dmn_img             = [] # Index of dmn_img is consistent with other_regions\n",
    "    NUM_other_regions   = len(other_regions) # number of clusters in other regions\n",
    "    dist_table_truth    = np.zeros((NUM_other_regions,NUM_region),dtype=int)\n",
    "    p_reg_label_dmn     = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in predicted lagels.\n",
    "    grd_reg_answer_dmn  = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in true answers.\n",
    "    p_reg_dmn_rate      = np.zeros(NUM_other_regions,dtype=float) #one value. dominate ratio in a region\n",
    "\n",
    "    #(1) other_regions      --> establish all other region table \n",
    "    for i,region_name in enumerate(other_regions): #check all other regions\n",
    "\n",
    "        #(a)===== predicted images (multiple values) =====\n",
    "        p_img        = all_region_image[region_name-1] # In this region, get their images. \"region_name-1\" is due to region index starts from 1 to 200\n",
    "        p_img_label  = Original_result[p_img]   # Predicted labels in the region.\n",
    "        p_img_total  = len(p_img)\n",
    "        # the value of predicted labels is the index of trainning region. These indices are the labels\n",
    "        # but these p_img_answer are predicted, may not always be the truth.\n",
    "\n",
    "\n",
    "        #(b)===== region dominate; one value =====\n",
    "        #region label\n",
    "        p_reg_label_dmn[i] = collections.Counter(p_img_label).most_common()[0][0] # one value\n",
    "        # region dominate rate\n",
    "        if(p_reg_label_dmn[i]>=0):\n",
    "            p_reg_dmn_rate[i] = collections.Counter(p_img_label).most_common()[0][1]/p_img_total\n",
    "        else:              # means invalid label\n",
    "            p_reg_dmn_rate[i] = 0\n",
    "\n",
    "\n",
    "        #(c)==== ground truth =====\n",
    "        grd_label                 = test_label_answer[p_img]  #multiple values\n",
    "        grd_reg_answer_dmn[i]     = collections.Counter(grd_label).most_common()[0][0] #one value\n",
    "\n",
    "\n",
    "        #(d)==== establish confusion table=====\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(grd_label==j)[0])\n",
    "\n",
    "\n",
    "        #(e)=== collect dominated images =============\n",
    "        addr2=np.where( (p_img_label==p_reg_label_dmn[i]) & (p_img_label>=0) )[0] # ignore -1 which are non-consistency\n",
    "        #         the labels which  == 7               the labels which >= 0\n",
    "        temp=[]\n",
    "        for k in range(len(addr2)):\n",
    "            temp.append(p_img[addr2[k]])\n",
    "        dmn_img.append(temp)\n",
    "        #=============================================\n",
    "\n",
    "\n",
    "    df1 = pandas.DataFrame({\"other index\":other_regions}) # 1 to 200   other region index\n",
    "    df2 = pandas.DataFrame({\"pred label\":p_reg_label_dmn})      \n",
    "    df4 = pandas.DataFrame({\"truth\":grd_reg_answer_dmn})\n",
    "    df6 = pandas.DataFrame({\"rate\":np.round(p_reg_dmn_rate,2)})\n",
    "    df7 = pandas.DataFrame(dist_table_truth)\n",
    "    entire_table=pandas.concat([df1, df2, df4, df6, df7], axis=1)\n",
    "    print(\"All other regions\")\n",
    "    display(entire_table)\n",
    "\n",
    "\n",
    "\n",
    "    #(2)get regions according to conditions\n",
    "    NN=5 #choose top 5 regions\n",
    "    RATE=0.7\n",
    "    candidate_reg_by_top_NN=[]\n",
    "\n",
    "    # === get candidate regions by the order of dmn label 0 to 9 ====\n",
    "    for i in range(NUM_region):\n",
    "        # (2-1) ==== select region by rate > 0.7 and top 5 ====\n",
    "        index     = np.where(p_reg_label_dmn==i)[0] # index is the index of other_regions(0~183), rather than original entire region index 1 to 200        \n",
    "        working_table = entire_table.iloc[index]\n",
    "        working_table = working_table.sort_values(by=['rate'], ascending=False)\n",
    "        working_table = working_table.loc[working_table['rate'] > RATE]  #rate > 0.7\n",
    "        NUM_region_in_one_class = len(working_table.iloc[:NN])  #top 5\n",
    "               \n",
    "        # (2-2) ==== get candidate regions ====\n",
    "        # get top N records; save only the column 'other_reg', and transfer it to list from DataFrame by \"tolist()\"\n",
    "        candidate_reg_by_top_NN.append(working_table[:NN]['other index'].tolist())\n",
    "         \n",
    "    #(3) add regions and images\n",
    "    for i in range(NUM_region):\n",
    "        added_img=[]\n",
    "        if (len(candidate_reg_by_top_NN[i])>0):\n",
    "            for j in range(len(candidate_reg_by_top_NN[i])):\n",
    "                reg_addr  = np.where( np.array(other_regions)==candidate_reg_by_top_NN[i][j] )[0][0].tolist()\n",
    "                added_img = added_img + dmn_img[reg_addr]\n",
    "            # (3-1) add image\n",
    "            temp=len(merged_region_image[i])\n",
    "            merged_region_image[i] = merged_region_image[i] + added_img\n",
    "            merged_region_image[i] = list(set(merged_region_image[i]))\n",
    "            img_amount=len(merged_region_image[i])-temp\n",
    "            \n",
    "            # (3-2) add region\n",
    "            merged_reg_and_nei[i]  = merged_reg_and_nei[i] + candidate_reg_by_top_NN[i]\n",
    "            \n",
    "            # (3-3) print out\n",
    "            print(\"added label, regions, img amount:\", set(Original_result[added_img]), candidate_reg_by_top_NN[i], img_amount)\n",
    "\n",
    "            \n",
    "    # (4) collect residual images\n",
    "    # This works only for CIFAR10. All images in the MNIST and MNIST-TRAN are clean. No this issue.        \n",
    "    #20240105\n",
    "    if (not MNIST):\n",
    "    #if ((DATASET==2) or (DATASET==4)):\n",
    "        if (len(list(chain.from_iterable(candidate_reg_by_top_NN))) == 0):  #if no extra regions\n",
    "            #20240105\n",
    "            if (True):\n",
    "            #if(DATASET==4):\n",
    "                df = pandas.read_csv(PATH4)\n",
    "                tSNE_table = df.to_numpy()[:,:3]\n",
    "            else:\n",
    "                df = pandas.read_csv(PATH8)\n",
    "                tSNE_table = df.to_numpy()\n",
    "            print(\"tSNE_table\",np.shape(tSNE_table))\n",
    "\n",
    "            working_table=tSNE_table[working_img]\n",
    "            pairwise_dist=squareform(pdist(working_table, 'euclidean'))\n",
    "            print(\"pairwise_dist\",np.shape(pairwise_dist)) #value of data point, rather than image index\n",
    "\n",
    "            TopN=10\n",
    "            M=len(working_img)\n",
    "            nei_table_images  = np.zeros((M,TopN),dtype=int)  #contain top 10 images\n",
    "            nei_table_label   = np.zeros((M,TopN),dtype=int)\n",
    "            working_img_label = np.zeros(M,dtype=int)\n",
    "            for i in range(M):   \n",
    "                # fill up top 10 \n",
    "                addr=np.argsort(pairwise_dist[i])\n",
    "                for j in range(TopN):\n",
    "                    nei_table_images[i][j]=working_img[ addr[j+1] ] #Ignore first one. First one is itself\n",
    "                    nei_table_label[i][j] =Original_result[nei_table_images[i][j]]\n",
    "                # consistent\n",
    "                if (len(set(nei_table_label[i])) == 1): #only get the one which is entire consistent\n",
    "                    working_img_label[i]=nei_table_label[i][0]\n",
    "                else:\n",
    "                    working_img_label[i]=-1\n",
    "\n",
    "            print(\"nei_table_images\",np.shape(nei_table_images))\n",
    "            print(\"working_img_label\",working_img_label)\n",
    "\n",
    "            new_img=[] # just  for monitoring\n",
    "            for i in range(NUM_region):\n",
    "                addr=np.where(working_img_label==i)[0].tolist()\n",
    "                new_img.append(working_img[addr])\n",
    "                merged_region_image[i].extend(working_img[addr])\n",
    "            print(\"add residuals \",len(list(chain.from_iterable(new_img))))\n",
    "            print(\"number of next merged_region_image\", len(list(chain.from_iterable(merged_region_image))))\n",
    "        else:\n",
    "            print(\"Not getting into residuals\")\n",
    "\n",
    "    #save\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/merged_region_image_'+str(ITE+1)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([merged_reg_and_nei, merged_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup region_initials.pickle\n",
    "#### For both single network and integrate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00001.png</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00002.png</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00003.png</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00004.png</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00005.png</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  D  E         V1   V2  V3\n",
       "0  0  0  0  0  0  00001.png  196   0\n",
       "1  0  0  0  0  0  00002.png  132   0\n",
       "2  0  0  0  0  0  00003.png  196   0\n",
       "3  0  0  0  0  0  00004.png   85   0\n",
       "4  0  0  0  0  0  00005.png   78   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv(PATH4)\n",
    "display(df.head())\n",
    "all_region_index = df.to_numpy().T[REGION_INDEX_LOC].astype(int)\n",
    "print(len(all_region_index))\n",
    "\n",
    "all_region_image=[]\n",
    "MAX_region=max(all_region_index)\n",
    "for i in range(MAX_region):\n",
    "    addr=list(np.where(all_region_index==i+1)[0])\n",
    "    all_region_image.append(addr)    \n",
    "\n",
    "#save\n",
    "if (SAVE_bool):\n",
    "    with open('./region_initials.pickle', 'wb') as f:\n",
    "        pickle.dump([all_region_index, all_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case= 1\n",
      "mergedseedclasslabels table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[134,   9],\n",
       "       [138,   5],\n",
       "       [123,   1],\n",
       "       [137,   2],\n",
       "       [109,   6],\n",
       "       [ 38,  10],\n",
       "       [  2,   3],\n",
       "       [135,   8],\n",
       "       [117,   9],\n",
       "       [100,   1],\n",
       "       [191,   3],\n",
       "       [146,   6],\n",
       "       [182,   4],\n",
       "       [ 61,   2],\n",
       "       [185,   8],\n",
       "       [180,   7],\n",
       "       [175,   5],\n",
       "       [  6,   1],\n",
       "       [154,  10],\n",
       "       [ 70,   4],\n",
       "       [ 67,   4],\n",
       "       [ 93,   7],\n",
       "       [ 20,   0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_region\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[123, 100, 6],\n",
       " [137, 61],\n",
       " [2, 191],\n",
       " [182, 70, 67],\n",
       " [138, 175],\n",
       " [109, 146],\n",
       " [180, 93],\n",
       " [135, 185],\n",
       " [134, 117],\n",
       " [38, 154]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 123\n",
      "9 100\n",
      "17 6\n",
      "total 18\n",
      "\n",
      "3 137\n",
      "13 61\n",
      "total 12\n",
      "\n",
      "6 2\n",
      "10 191\n",
      "total 12\n",
      "\n",
      "12 182\n",
      "19 70\n",
      "20 67\n",
      "total 18\n",
      "\n",
      "1 138\n",
      "16 175\n",
      "total 12\n",
      "\n",
      "4 109\n",
      "11 146\n",
      "total 12\n",
      "\n",
      "15 180\n",
      "21 93\n",
      "total 12\n",
      "\n",
      "7 135\n",
      "14 185\n",
      "total 12\n",
      "\n",
      "0 134\n",
      "8 117\n",
      "total 12\n",
      "\n",
      "5 38\n",
      "18 154\n",
      "total 12\n",
      "\n",
      "\n",
      "merged_reg_and_nei\n",
      "[123, 120, 177, 82, 85, 43, 100, 195, 53, 78, 179, 17, 6, 102, 174, 13, 27, 99]\n",
      "[137, 44, 97, 178, 129, 169, 61, 130, 55, 66, 4, 62]\n",
      "[2, 87, 118, 103, 111, 19, 191, 155, 108, 88, 56, 31]\n",
      "[182, 60, 18, 162, 198, 30, 70, 107, 76, 160, 28, 54, 67, 166, 168, 89, 156, 186]\n",
      "[138, 95, 14, 36, 128, 83, 175, 161, 5, 73, 8, 110]\n",
      "[109, 45, 113, 23, 35, 148, 146, 200, 189, 164, 96, 94]\n",
      "[180, 122, 183, 11, 170, 140, 93, 63, 39, 199, 98, 157]\n",
      "[135, 29, 165, 125, 33, 10, 185, 65, 173, 158, 133, 25]\n",
      "[134, 71, 21, 16, 41, 105, 117, 114, 26, 40, 86, 194]\n",
      "[38, 58, 79, 115, 49, 104, 154, 81, 124, 80, 112, 153]\n",
      "1686 ( 2 ) 1326 ( 9 ) 1407 ( 17 ) = 4419 \n",
      "1465 ( 3 ) 1750 ( 13 ) = 3215 \n",
      "1585 ( 6 ) 1888 ( 10 ) = 3473 \n",
      "1532 ( 12 ) 1598 ( 19 ) 1456 ( 20 ) = 4586 \n",
      "1308 ( 1 ) 1425 ( 16 ) = 2733 \n",
      "1578 ( 4 ) 1698 ( 11 ) = 3276 \n",
      "1499 ( 15 ) 1880 ( 21 ) = 3379 \n",
      "2331 ( 7 ) 1669 ( 14 ) = 4000 \n",
      "1752 ( 0 ) 2171 ( 8 ) = 3923 \n",
      "1619 ( 5 ) 1809 ( 18 ) = 3428 \n",
      "\n",
      "merged_reg_and_nei_image\n",
      "4419 [24, 46, 127, 150, 155] ...\n",
      "3215 [22747, 22767, 22789, 22852, 22853] ...\n",
      "3473 [38771, 42569, 49546, 49579, 49607] ...\n",
      "4586 [5534, 8232, 10627, 24238, 25195] ...\n",
      "2733 [5447, 5450, 5529, 5606, 5716] ...\n",
      "3276 [33061, 33133, 33135, 33137, 33138] ...\n",
      "3379 [44174, 44183, 44213, 44228, 44351] ...\n",
      "4000 [2942, 4201, 11635, 11639, 11640] ...\n",
      "3923 [1129, 3059, 3060, 21914, 28042] ...\n",
      "3428 [17095, 17107, 17129, 17140, 17152] ...\n",
      "NUM_region 10\n",
      "number of clean images 36432\n",
      "n, p1, p2 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32788 samples, validate on 3644 samples\n",
      "Epoch 1/80\n",
      "32788/32788 [==============================] - 3s 94us/step - loss: 1.3308 - accuracy: 0.4750 - val_loss: 1.0184 - val_accuracy: 0.6188\n",
      "Epoch 2/80\n",
      "32788/32788 [==============================] - ETA: 0s - loss: 0.9863 - accuracy: 0.60 - 1s 43us/step - loss: 0.9861 - accuracy: 0.6024 - val_loss: 0.9598 - val_accuracy: 0.6136\n",
      "Epoch 3/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.9440 - accuracy: 0.6108 - val_loss: 0.9406 - val_accuracy: 0.6279\n",
      "Epoch 4/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.9197 - accuracy: 0.6120 - val_loss: 0.9091 - val_accuracy: 0.6218\n",
      "Epoch 5/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.9022 - accuracy: 0.6158 - val_loss: 0.8931 - val_accuracy: 0.6306\n",
      "Epoch 6/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8888 - accuracy: 0.6190 - val_loss: 0.8890 - val_accuracy: 0.6378\n",
      "Epoch 7/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8812 - accuracy: 0.6210 - val_loss: 0.8706 - val_accuracy: 0.6336\n",
      "Epoch 8/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8741 - accuracy: 0.6227 - val_loss: 0.8778 - val_accuracy: 0.6367\n",
      "Epoch 9/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8702 - accuracy: 0.6252 - val_loss: 0.8672 - val_accuracy: 0.6375\n",
      "Epoch 10/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8651 - accuracy: 0.6290 - val_loss: 0.8784 - val_accuracy: 0.6306\n",
      "Epoch 11/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8628 - accuracy: 0.6291 - val_loss: 0.8735 - val_accuracy: 0.6339\n",
      "Epoch 12/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8574 - accuracy: 0.6300 - val_loss: 0.8529 - val_accuracy: 0.6556\n",
      "Epoch 13/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8566 - accuracy: 0.6314 - val_loss: 0.8558 - val_accuracy: 0.6443\n",
      "Epoch 14/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8532 - accuracy: 0.6303 - val_loss: 0.8634 - val_accuracy: 0.6457\n",
      "Epoch 15/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8507 - accuracy: 0.6335 - val_loss: 0.8477 - val_accuracy: 0.6463\n",
      "Epoch 16/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8469 - accuracy: 0.6350 - val_loss: 0.8467 - val_accuracy: 0.6520\n",
      "Epoch 17/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8439 - accuracy: 0.6327 - val_loss: 0.8617 - val_accuracy: 0.6454\n",
      "Epoch 18/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8420 - accuracy: 0.6344 - val_loss: 0.8392 - val_accuracy: 0.6509\n",
      "Epoch 19/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8399 - accuracy: 0.6361 - val_loss: 0.8315 - val_accuracy: 0.6526\n",
      "Epoch 20/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8361 - accuracy: 0.6375 - val_loss: 0.8261 - val_accuracy: 0.6529\n",
      "Epoch 21/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8361 - accuracy: 0.6342 - val_loss: 0.8440 - val_accuracy: 0.6435\n",
      "Epoch 22/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8329 - accuracy: 0.6404 - val_loss: 0.8232 - val_accuracy: 0.6548\n",
      "Epoch 23/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8317 - accuracy: 0.6373 - val_loss: 0.8376 - val_accuracy: 0.6471\n",
      "Epoch 24/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8308 - accuracy: 0.6383 - val_loss: 0.8325 - val_accuracy: 0.6463\n",
      "Epoch 25/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8310 - accuracy: 0.6375 - val_loss: 0.8195 - val_accuracy: 0.6501\n",
      "Epoch 26/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8270 - accuracy: 0.6386 - val_loss: 0.8198 - val_accuracy: 0.6545\n",
      "Epoch 27/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8279 - accuracy: 0.6395 - val_loss: 0.8269 - val_accuracy: 0.6504\n",
      "Epoch 28/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8271 - accuracy: 0.6401 - val_loss: 0.8126 - val_accuracy: 0.6597\n",
      "Epoch 29/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8241 - accuracy: 0.6405 - val_loss: 0.8205 - val_accuracy: 0.6564\n",
      "Epoch 30/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8229 - accuracy: 0.6407 - val_loss: 0.8053 - val_accuracy: 0.6627\n",
      "Epoch 31/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8197 - accuracy: 0.6431 - val_loss: 0.8148 - val_accuracy: 0.6614\n",
      "Epoch 32/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8187 - accuracy: 0.6426 - val_loss: 0.8159 - val_accuracy: 0.6474\n",
      "Epoch 33/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8196 - accuracy: 0.6424 - val_loss: 0.8207 - val_accuracy: 0.6512\n",
      "Epoch 34/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8177 - accuracy: 0.6426 - val_loss: 0.8280 - val_accuracy: 0.6482\n",
      "Epoch 35/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8162 - accuracy: 0.6451 - val_loss: 0.8075 - val_accuracy: 0.6589\n",
      "Epoch 36/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8149 - accuracy: 0.6459 - val_loss: 0.8099 - val_accuracy: 0.6592\n",
      "Epoch 37/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8117 - accuracy: 0.6469 - val_loss: 0.8097 - val_accuracy: 0.6570\n",
      "Epoch 38/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8125 - accuracy: 0.6443 - val_loss: 0.8139 - val_accuracy: 0.6608\n",
      "Epoch 39/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8112 - accuracy: 0.6452 - val_loss: 0.8086 - val_accuracy: 0.6496\n",
      "Epoch 40/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8108 - accuracy: 0.6476 - val_loss: 0.8011 - val_accuracy: 0.6636\n",
      "Epoch 41/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8098 - accuracy: 0.6457 - val_loss: 0.8060 - val_accuracy: 0.6625\n",
      "Epoch 42/80\n",
      "32788/32788 [==============================] - ETA: 0s - loss: 0.8108 - accuracy: 0.64 - 1s 45us/step - loss: 0.8097 - accuracy: 0.6465 - val_loss: 0.7957 - val_accuracy: 0.6718\n",
      "Epoch 43/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8073 - accuracy: 0.6431 - val_loss: 0.8167 - val_accuracy: 0.6589\n",
      "Epoch 44/80\n",
      "32788/32788 [==============================] - 1s 39us/step - loss: 0.8076 - accuracy: 0.6477 - val_loss: 0.8030 - val_accuracy: 0.6608\n",
      "Epoch 45/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8073 - accuracy: 0.6435 - val_loss: 0.7989 - val_accuracy: 0.6633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8027 - accuracy: 0.6480 - val_loss: 0.8122 - val_accuracy: 0.6614\n",
      "Epoch 47/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8032 - accuracy: 0.6469 - val_loss: 0.7978 - val_accuracy: 0.6633\n",
      "Epoch 48/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8023 - accuracy: 0.6474 - val_loss: 0.7930 - val_accuracy: 0.6715\n",
      "Epoch 49/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8014 - accuracy: 0.6482 - val_loss: 0.8074 - val_accuracy: 0.6630\n",
      "Epoch 50/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8000 - accuracy: 0.6495 - val_loss: 0.7983 - val_accuracy: 0.6677\n",
      "Epoch 51/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8001 - accuracy: 0.6481 - val_loss: 0.7938 - val_accuracy: 0.6652\n",
      "Epoch 52/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7987 - accuracy: 0.6497 - val_loss: 0.7942 - val_accuracy: 0.6603\n",
      "Epoch 53/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7979 - accuracy: 0.6476 - val_loss: 0.7924 - val_accuracy: 0.6696\n",
      "Epoch 54/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7984 - accuracy: 0.6503 - val_loss: 0.8079 - val_accuracy: 0.6578\n",
      "Epoch 55/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7971 - accuracy: 0.6505 - val_loss: 0.7936 - val_accuracy: 0.6674\n",
      "Epoch 56/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.7957 - accuracy: 0.6502 - val_loss: 0.8042 - val_accuracy: 0.6597\n",
      "Epoch 57/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7928 - accuracy: 0.6518 - val_loss: 0.7948 - val_accuracy: 0.6619\n",
      "Epoch 58/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7963 - accuracy: 0.6502 - val_loss: 0.7923 - val_accuracy: 0.6729\n",
      "Epoch 59/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7922 - accuracy: 0.6519 - val_loss: 0.8003 - val_accuracy: 0.6636\n",
      "Epoch 60/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7923 - accuracy: 0.6511 - val_loss: 0.8068 - val_accuracy: 0.6531\n",
      "Epoch 61/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7899 - accuracy: 0.6546 - val_loss: 0.7916 - val_accuracy: 0.6690\n",
      "Epoch 62/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7883 - accuracy: 0.6532 - val_loss: 0.7893 - val_accuracy: 0.6652\n",
      "Epoch 63/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7931 - accuracy: 0.6512 - val_loss: 0.7913 - val_accuracy: 0.6644\n",
      "Epoch 64/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7869 - accuracy: 0.6556 - val_loss: 0.8000 - val_accuracy: 0.6633\n",
      "Epoch 65/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7882 - accuracy: 0.6522 - val_loss: 0.7887 - val_accuracy: 0.6603\n",
      "Epoch 66/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7854 - accuracy: 0.6555 - val_loss: 0.7955 - val_accuracy: 0.6586\n",
      "Epoch 67/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7868 - accuracy: 0.6551 - val_loss: 0.7902 - val_accuracy: 0.6630\n",
      "Epoch 68/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7861 - accuracy: 0.6548 - val_loss: 0.7923 - val_accuracy: 0.6644\n",
      "Epoch 69/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7835 - accuracy: 0.6577 - val_loss: 0.8059 - val_accuracy: 0.6498\n",
      "Epoch 70/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7841 - accuracy: 0.6562 - val_loss: 0.8038 - val_accuracy: 0.6674\n",
      "Epoch 71/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7816 - accuracy: 0.6566 - val_loss: 0.7905 - val_accuracy: 0.6625\n",
      "Epoch 72/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7813 - accuracy: 0.6562 - val_loss: 0.7894 - val_accuracy: 0.6682\n",
      "Epoch 73/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7831 - accuracy: 0.6551 - val_loss: 0.8019 - val_accuracy: 0.6630\n",
      "Epoch 74/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7803 - accuracy: 0.6555 - val_loss: 0.8072 - val_accuracy: 0.6605\n",
      "Epoch 75/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7810 - accuracy: 0.6567 - val_loss: 0.7864 - val_accuracy: 0.6767\n",
      "Epoch 76/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7803 - accuracy: 0.6578 - val_loss: 0.7998 - val_accuracy: 0.6688\n",
      "Epoch 77/80\n",
      "32788/32788 [==============================] - 1s 46us/step - loss: 0.7788 - accuracy: 0.6605 - val_loss: 0.8001 - val_accuracy: 0.6622\n",
      "Epoch 78/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7791 - accuracy: 0.6590 - val_loss: 0.7832 - val_accuracy: 0.6767\n",
      "Epoch 79/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7775 - accuracy: 0.6573 - val_loss: 0.7910 - val_accuracy: 0.6666\n",
      "Epoch 80/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7776 - accuracy: 0.6566 - val_loss: 0.7938 - val_accuracy: 0.6726\n",
      "[[6.05075300e-01 2.90113996e-04 6.44193869e-03 ... 3.63047421e-03\n",
      "  6.14328422e-02 4.20199111e-02]\n",
      " [2.37568885e-01 3.10796924e-04 2.17679329e-03 ... 5.71267563e-04\n",
      "  7.37552404e-01 1.91188767e-03]\n",
      " [6.93257868e-01 8.87829956e-05 7.05962593e-04 ... 1.80421770e-03\n",
      "  9.13613103e-03 7.26440642e-03]\n",
      " ...\n",
      " [5.42814331e-03 9.31807980e-03 9.47364211e-01 ... 6.33869568e-05\n",
      "  2.51596272e-02 1.30482280e-04]\n",
      " [4.83136982e-01 1.44963886e-03 2.40312308e-01 ... 2.70602206e-04\n",
      "  1.96858659e-01 1.04906496e-04]\n",
      " [5.12176037e-01 1.81289285e-03 1.72181591e-01 ... 2.79895932e-04\n",
      "  2.35453993e-01 1.11467278e-04]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:02:06.045850\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32788 samples, validate on 3644 samples\n",
      "Epoch 1/80\n",
      "32788/32788 [==============================] - 2s 51us/step - loss: 1.3531 - accuracy: 0.4612 - val_loss: 1.0562 - val_accuracy: 0.5873\n",
      "Epoch 2/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9847 - accuracy: 0.5985 - val_loss: 0.9942 - val_accuracy: 0.5991\n",
      "Epoch 3/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9400 - accuracy: 0.6074 - val_loss: 0.9577 - val_accuracy: 0.6037\n",
      "Epoch 4/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.9105 - accuracy: 0.6130 - val_loss: 0.9377 - val_accuracy: 0.6177\n",
      "Epoch 5/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8931 - accuracy: 0.6190 - val_loss: 0.9186 - val_accuracy: 0.6059\n",
      "Epoch 6/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8810 - accuracy: 0.6221 - val_loss: 0.9178 - val_accuracy: 0.6117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8766 - accuracy: 0.6210 - val_loss: 0.9054 - val_accuracy: 0.6227\n",
      "Epoch 8/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8691 - accuracy: 0.6258 - val_loss: 0.9114 - val_accuracy: 0.6155\n",
      "Epoch 9/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8658 - accuracy: 0.6243 - val_loss: 0.9045 - val_accuracy: 0.6224\n",
      "Epoch 10/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8600 - accuracy: 0.6261 - val_loss: 0.8969 - val_accuracy: 0.6158\n",
      "Epoch 11/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8549 - accuracy: 0.6294 - val_loss: 0.8953 - val_accuracy: 0.6186\n",
      "Epoch 12/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8533 - accuracy: 0.6282 - val_loss: 0.8912 - val_accuracy: 0.6287\n",
      "Epoch 13/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8482 - accuracy: 0.6338 - val_loss: 0.8797 - val_accuracy: 0.6301\n",
      "Epoch 14/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8435 - accuracy: 0.6335 - val_loss: 0.8802 - val_accuracy: 0.6290\n",
      "Epoch 15/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8447 - accuracy: 0.6360 - val_loss: 0.8747 - val_accuracy: 0.6408\n",
      "Epoch 16/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8403 - accuracy: 0.6356 - val_loss: 0.8792 - val_accuracy: 0.6273\n",
      "Epoch 17/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8362 - accuracy: 0.6360 - val_loss: 0.8867 - val_accuracy: 0.6221\n",
      "Epoch 18/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8349 - accuracy: 0.6393 - val_loss: 0.8864 - val_accuracy: 0.6216\n",
      "Epoch 19/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8320 - accuracy: 0.6390 - val_loss: 0.8648 - val_accuracy: 0.6314\n",
      "Epoch 20/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8302 - accuracy: 0.6377 - val_loss: 0.8639 - val_accuracy: 0.6386\n",
      "Epoch 21/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8293 - accuracy: 0.6389 - val_loss: 0.8605 - val_accuracy: 0.6350\n",
      "Epoch 22/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8275 - accuracy: 0.6389 - val_loss: 0.8653 - val_accuracy: 0.6290\n",
      "Epoch 23/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8251 - accuracy: 0.6397 - val_loss: 0.8582 - val_accuracy: 0.6408\n",
      "Epoch 24/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8251 - accuracy: 0.6435 - val_loss: 0.8635 - val_accuracy: 0.6339\n",
      "Epoch 25/80\n",
      "32788/32788 [==============================] - 1s 46us/step - loss: 0.8232 - accuracy: 0.6426 - val_loss: 0.8628 - val_accuracy: 0.6323\n",
      "Epoch 26/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8209 - accuracy: 0.6442 - val_loss: 0.8583 - val_accuracy: 0.6380\n",
      "Epoch 27/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8221 - accuracy: 0.6422 - val_loss: 0.8529 - val_accuracy: 0.6476\n",
      "Epoch 28/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8180 - accuracy: 0.6461 - val_loss: 0.8733 - val_accuracy: 0.6265\n",
      "Epoch 29/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8165 - accuracy: 0.6439 - val_loss: 0.8533 - val_accuracy: 0.6389\n",
      "Epoch 30/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8159 - accuracy: 0.6439 - val_loss: 0.8812 - val_accuracy: 0.6265\n",
      "Epoch 31/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8157 - accuracy: 0.6477 - val_loss: 0.8579 - val_accuracy: 0.6427\n",
      "Epoch 32/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8139 - accuracy: 0.6451 - val_loss: 0.8627 - val_accuracy: 0.6334\n",
      "Epoch 33/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8144 - accuracy: 0.6462 - val_loss: 0.8592 - val_accuracy: 0.6422\n",
      "Epoch 34/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8122 - accuracy: 0.6468 - val_loss: 0.8514 - val_accuracy: 0.6386\n",
      "Epoch 35/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8121 - accuracy: 0.6484 - val_loss: 0.8449 - val_accuracy: 0.6449\n",
      "Epoch 36/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8102 - accuracy: 0.6486 - val_loss: 0.8567 - val_accuracy: 0.6424\n",
      "Epoch 37/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8085 - accuracy: 0.6510 - val_loss: 0.8582 - val_accuracy: 0.6485\n",
      "Epoch 38/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8100 - accuracy: 0.6488 - val_loss: 0.8479 - val_accuracy: 0.6413\n",
      "Epoch 39/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8077 - accuracy: 0.6490 - val_loss: 0.8545 - val_accuracy: 0.6413\n",
      "Epoch 40/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8082 - accuracy: 0.6475 - val_loss: 0.8640 - val_accuracy: 0.6325\n",
      "Epoch 41/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8090 - accuracy: 0.6529 - val_loss: 0.8502 - val_accuracy: 0.6432\n",
      "Epoch 42/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8075 - accuracy: 0.6490 - val_loss: 0.8426 - val_accuracy: 0.6572\n",
      "Epoch 43/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8050 - accuracy: 0.6511 - val_loss: 0.8477 - val_accuracy: 0.6391\n",
      "Epoch 44/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8038 - accuracy: 0.6494 - val_loss: 0.8465 - val_accuracy: 0.6358\n",
      "Epoch 45/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8024 - accuracy: 0.6528 - val_loss: 0.8491 - val_accuracy: 0.6408\n",
      "Epoch 46/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8017 - accuracy: 0.6530 - val_loss: 0.8507 - val_accuracy: 0.6386\n",
      "Epoch 47/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8025 - accuracy: 0.6501 - val_loss: 0.8452 - val_accuracy: 0.6564\n",
      "Epoch 48/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8016 - accuracy: 0.6528 - val_loss: 0.8450 - val_accuracy: 0.6389\n",
      "Epoch 49/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8021 - accuracy: 0.6519 - val_loss: 0.8501 - val_accuracy: 0.6438\n",
      "Epoch 50/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8014 - accuracy: 0.6498 - val_loss: 0.8375 - val_accuracy: 0.6485\n",
      "Epoch 51/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7987 - accuracy: 0.6541 - val_loss: 0.8480 - val_accuracy: 0.6369\n",
      "Epoch 52/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7975 - accuracy: 0.6539 - val_loss: 0.8405 - val_accuracy: 0.6507\n",
      "Epoch 53/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7986 - accuracy: 0.6541 - val_loss: 0.8386 - val_accuracy: 0.6507\n",
      "Epoch 54/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7972 - accuracy: 0.6561 - val_loss: 0.8299 - val_accuracy: 0.6531\n",
      "Epoch 55/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7963 - accuracy: 0.6567 - val_loss: 0.8377 - val_accuracy: 0.6496\n",
      "Epoch 56/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7941 - accuracy: 0.6551 - val_loss: 0.8439 - val_accuracy: 0.6402\n",
      "Epoch 57/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7958 - accuracy: 0.6529 - val_loss: 0.8424 - val_accuracy: 0.6441\n",
      "Epoch 58/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7941 - accuracy: 0.6551 - val_loss: 0.8347 - val_accuracy: 0.6468\n",
      "Epoch 59/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7944 - accuracy: 0.6526 - val_loss: 0.8304 - val_accuracy: 0.6594\n",
      "Epoch 60/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7922 - accuracy: 0.6551 - val_loss: 0.8299 - val_accuracy: 0.6556\n",
      "Epoch 61/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7909 - accuracy: 0.6567 - val_loss: 0.8303 - val_accuracy: 0.6594\n",
      "Epoch 62/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7888 - accuracy: 0.6579 - val_loss: 0.8407 - val_accuracy: 0.6427\n",
      "Epoch 63/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7891 - accuracy: 0.6580 - val_loss: 0.8226 - val_accuracy: 0.6501\n",
      "Epoch 64/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7871 - accuracy: 0.6564 - val_loss: 0.8344 - val_accuracy: 0.6476\n",
      "Epoch 65/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7849 - accuracy: 0.6573 - val_loss: 0.8315 - val_accuracy: 0.6553\n",
      "Epoch 66/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7868 - accuracy: 0.6579 - val_loss: 0.8288 - val_accuracy: 0.6507\n",
      "Epoch 67/80\n",
      "32788/32788 [==============================] - 1s 37us/step - loss: 0.7829 - accuracy: 0.6597 - val_loss: 0.8198 - val_accuracy: 0.6509\n",
      "Epoch 68/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7817 - accuracy: 0.6592 - val_loss: 0.8283 - val_accuracy: 0.6468\n",
      "Epoch 69/80\n",
      "32788/32788 [==============================] - 1s 35us/step - loss: 0.7808 - accuracy: 0.6584 - val_loss: 0.8160 - val_accuracy: 0.6526\n",
      "Epoch 70/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7804 - accuracy: 0.6602 - val_loss: 0.8178 - val_accuracy: 0.6537\n",
      "Epoch 71/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7785 - accuracy: 0.6600 - val_loss: 0.8193 - val_accuracy: 0.6512\n",
      "Epoch 72/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7791 - accuracy: 0.6590 - val_loss: 0.8116 - val_accuracy: 0.6550\n",
      "Epoch 73/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7760 - accuracy: 0.6631 - val_loss: 0.8221 - val_accuracy: 0.6611\n",
      "Epoch 74/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7726 - accuracy: 0.6636 - val_loss: 0.8074 - val_accuracy: 0.6572\n",
      "Epoch 75/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7727 - accuracy: 0.6629 - val_loss: 0.8057 - val_accuracy: 0.6614\n",
      "Epoch 76/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7710 - accuracy: 0.6621 - val_loss: 0.8066 - val_accuracy: 0.6603\n",
      "Epoch 77/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7727 - accuracy: 0.6623 - val_loss: 0.8099 - val_accuracy: 0.6487\n",
      "Epoch 78/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7718 - accuracy: 0.6612 - val_loss: 0.8192 - val_accuracy: 0.6471\n",
      "Epoch 79/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7702 - accuracy: 0.6644 - val_loss: 0.8241 - val_accuracy: 0.6548\n",
      "Epoch 80/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7709 - accuracy: 0.6602 - val_loss: 0.8098 - val_accuracy: 0.6496\n",
      "[[7.2046447e-01 5.9090159e-04 6.1950024e-04 ... 1.8549169e-03\n",
      "  8.0267072e-02 2.5783243e-02]\n",
      " [3.4795862e-01 9.7920094e-04 1.9807850e-03 ... 1.5983995e-04\n",
      "  6.2470728e-01 2.0449739e-03]\n",
      " [8.0224526e-01 2.0237725e-04 3.0372894e-04 ... 1.2441401e-03\n",
      "  9.5039979e-03 6.2163742e-03]\n",
      " ...\n",
      " [1.3374269e-02 7.9997852e-03 8.9073318e-01 ... 7.9287909e-04\n",
      "  6.0354386e-02 1.8252983e-03]\n",
      " [5.8844072e-01 9.6320087e-04 1.2809080e-01 ... 5.1903090e-04\n",
      "  2.1692427e-01 5.9568119e-04]\n",
      " [5.9762841e-01 9.7635825e-04 1.0894363e-01 ... 4.5458623e-04\n",
      "  2.2398522e-01 5.7352078e-04]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:04:05.167067\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32788 samples, validate on 3644 samples\n",
      "Epoch 1/80\n",
      "32788/32788 [==============================] - 2s 55us/step - loss: 1.3442 - accuracy: 0.4755 - val_loss: 0.9895 - val_accuracy: 0.6015\n",
      "Epoch 2/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.9750 - accuracy: 0.6044 - val_loss: 0.9306 - val_accuracy: 0.6240\n",
      "Epoch 3/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9249 - accuracy: 0.6167 - val_loss: 0.8962 - val_accuracy: 0.6227\n",
      "Epoch 4/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9047 - accuracy: 0.6179 - val_loss: 0.9077 - val_accuracy: 0.6172\n",
      "Epoch 5/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8936 - accuracy: 0.6218 - val_loss: 0.8958 - val_accuracy: 0.6128\n",
      "Epoch 6/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8820 - accuracy: 0.6246 - val_loss: 0.8698 - val_accuracy: 0.6298\n",
      "Epoch 7/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8718 - accuracy: 0.6307 - val_loss: 0.8628 - val_accuracy: 0.6334\n",
      "Epoch 8/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.8682 - accuracy: 0.6302 - val_loss: 0.8626 - val_accuracy: 0.6257\n",
      "Epoch 9/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8638 - accuracy: 0.6318 - val_loss: 0.8651 - val_accuracy: 0.6246\n",
      "Epoch 10/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8606 - accuracy: 0.6300 - val_loss: 0.8555 - val_accuracy: 0.6331\n",
      "Epoch 11/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8571 - accuracy: 0.6311 - val_loss: 0.8506 - val_accuracy: 0.6342\n",
      "Epoch 12/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8521 - accuracy: 0.6339 - val_loss: 0.8536 - val_accuracy: 0.6336\n",
      "Epoch 13/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8484 - accuracy: 0.6330 - val_loss: 0.8431 - val_accuracy: 0.6309\n",
      "Epoch 14/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8494 - accuracy: 0.6331 - val_loss: 0.8531 - val_accuracy: 0.6306\n",
      "Epoch 15/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8433 - accuracy: 0.6393 - val_loss: 0.8360 - val_accuracy: 0.6356\n",
      "Epoch 16/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8420 - accuracy: 0.6387 - val_loss: 0.8430 - val_accuracy: 0.6320\n",
      "Epoch 17/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8385 - accuracy: 0.6419 - val_loss: 0.8430 - val_accuracy: 0.6386\n",
      "Epoch 18/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8362 - accuracy: 0.6409 - val_loss: 0.8347 - val_accuracy: 0.6383\n",
      "Epoch 19/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8359 - accuracy: 0.6399 - val_loss: 0.8324 - val_accuracy: 0.6422\n",
      "Epoch 20/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8329 - accuracy: 0.6398 - val_loss: 0.8314 - val_accuracy: 0.6430\n",
      "Epoch 21/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8327 - accuracy: 0.6424 - val_loss: 0.8411 - val_accuracy: 0.6408\n",
      "Epoch 22/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8291 - accuracy: 0.6421 - val_loss: 0.8283 - val_accuracy: 0.6394\n",
      "Epoch 23/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8275 - accuracy: 0.6431 - val_loss: 0.8296 - val_accuracy: 0.6325\n",
      "Epoch 24/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8279 - accuracy: 0.6431 - val_loss: 0.8261 - val_accuracy: 0.6411\n",
      "Epoch 25/80\n",
      "32788/32788 [==============================] - 2s 46us/step - loss: 0.8281 - accuracy: 0.6436 - val_loss: 0.8406 - val_accuracy: 0.6400\n",
      "Epoch 26/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8265 - accuracy: 0.6443 - val_loss: 0.8318 - val_accuracy: 0.6402\n",
      "Epoch 27/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8253 - accuracy: 0.6435 - val_loss: 0.8254 - val_accuracy: 0.6416\n",
      "Epoch 28/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8218 - accuracy: 0.6456 - val_loss: 0.8369 - val_accuracy: 0.6380\n",
      "Epoch 29/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8229 - accuracy: 0.6460 - val_loss: 0.8419 - val_accuracy: 0.6347\n",
      "Epoch 30/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8219 - accuracy: 0.6460 - val_loss: 0.8302 - val_accuracy: 0.6438\n",
      "Epoch 31/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8209 - accuracy: 0.6442 - val_loss: 0.8205 - val_accuracy: 0.6435\n",
      "Epoch 32/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8205 - accuracy: 0.6469 - val_loss: 0.8219 - val_accuracy: 0.6427\n",
      "Epoch 33/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8186 - accuracy: 0.6466 - val_loss: 0.8240 - val_accuracy: 0.6443\n",
      "Epoch 34/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8156 - accuracy: 0.6465 - val_loss: 0.8318 - val_accuracy: 0.6397\n",
      "Epoch 35/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8143 - accuracy: 0.6491 - val_loss: 0.8244 - val_accuracy: 0.6427\n",
      "Epoch 36/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8160 - accuracy: 0.6469 - val_loss: 0.8483 - val_accuracy: 0.6364\n",
      "Epoch 37/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8142 - accuracy: 0.6487 - val_loss: 0.8227 - val_accuracy: 0.6375\n",
      "Epoch 38/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8128 - accuracy: 0.6491 - val_loss: 0.8206 - val_accuracy: 0.6443\n",
      "Epoch 39/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8145 - accuracy: 0.6472 - val_loss: 0.8258 - val_accuracy: 0.6471\n",
      "Epoch 40/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.8123 - accuracy: 0.6472 - val_loss: 0.8206 - val_accuracy: 0.6520\n",
      "Epoch 41/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8102 - accuracy: 0.6501 - val_loss: 0.8214 - val_accuracy: 0.6443\n",
      "Epoch 00041: early stopping\n",
      "[[7.27193296e-01 2.73123645e-04 1.91758061e-03 ... 1.06513721e-03\n",
      "  4.35834602e-02 1.29656363e-02]\n",
      " [2.86497712e-01 1.03019457e-03 8.92749056e-03 ... 2.56265805e-04\n",
      "  6.75233006e-01 2.40983162e-03]\n",
      " [6.30787790e-01 2.71329598e-04 1.98423141e-03 ... 1.32091867e-03\n",
      "  1.10850967e-02 1.24409059e-02]\n",
      " ...\n",
      " [7.14743184e-03 2.88410131e-02 8.70789051e-01 ... 7.79991271e-04\n",
      "  2.57070996e-02 1.71768435e-04]\n",
      " [4.84755248e-01 5.91463782e-03 1.87470570e-01 ... 4.74588538e-04\n",
      "  2.82556832e-01 6.59682381e-04]\n",
      " [5.26976109e-01 4.66534449e-03 1.23813644e-01 ... 4.31059394e-04\n",
      "  3.04578006e-01 7.04189297e-04]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:05:09.709219\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32788 samples, validate on 3644 samples\n",
      "Epoch 1/80\n",
      "32788/32788 [==============================] - 2s 52us/step - loss: 1.4411 - accuracy: 0.4282 - val_loss: 1.1232 - val_accuracy: 0.5672\n",
      "Epoch 2/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 1.0301 - accuracy: 0.5919 - val_loss: 0.9978 - val_accuracy: 0.5864\n",
      "Epoch 3/80\n",
      "32788/32788 [==============================] - 2s 46us/step - loss: 0.9631 - accuracy: 0.6044 - val_loss: 0.9637 - val_accuracy: 0.6117\n",
      "Epoch 4/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.9314 - accuracy: 0.6099 - val_loss: 0.9578 - val_accuracy: 0.5993\n",
      "Epoch 5/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.9136 - accuracy: 0.6153 - val_loss: 0.9305 - val_accuracy: 0.6158\n",
      "Epoch 6/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.9037 - accuracy: 0.6178 - val_loss: 0.9239 - val_accuracy: 0.6133\n",
      "Epoch 7/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8974 - accuracy: 0.6194 - val_loss: 0.9098 - val_accuracy: 0.6238\n",
      "Epoch 8/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8902 - accuracy: 0.6227 - val_loss: 0.8944 - val_accuracy: 0.6262\n",
      "Epoch 9/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8855 - accuracy: 0.6238 - val_loss: 0.8893 - val_accuracy: 0.6246\n",
      "Epoch 10/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8796 - accuracy: 0.6247 - val_loss: 0.8956 - val_accuracy: 0.6265\n",
      "Epoch 11/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8760 - accuracy: 0.6265 - val_loss: 0.8836 - val_accuracy: 0.6210\n",
      "Epoch 12/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8744 - accuracy: 0.6244 - val_loss: 0.8797 - val_accuracy: 0.6199\n",
      "Epoch 13/80\n",
      "32788/32788 [==============================] - 2s 46us/step - loss: 0.8681 - accuracy: 0.6268 - val_loss: 0.8778 - val_accuracy: 0.6295\n",
      "Epoch 14/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8672 - accuracy: 0.6299 - val_loss: 0.9046 - val_accuracy: 0.6153\n",
      "Epoch 15/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8636 - accuracy: 0.6289 - val_loss: 0.8832 - val_accuracy: 0.6251\n",
      "Epoch 16/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8646 - accuracy: 0.6308 - val_loss: 0.8767 - val_accuracy: 0.6361\n",
      "Epoch 17/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8589 - accuracy: 0.6307 - val_loss: 0.8874 - val_accuracy: 0.6257\n",
      "Epoch 18/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8602 - accuracy: 0.6291 - val_loss: 0.8693 - val_accuracy: 0.6331\n",
      "Epoch 19/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8528 - accuracy: 0.6335 - val_loss: 0.8673 - val_accuracy: 0.6306\n",
      "Epoch 20/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8544 - accuracy: 0.6320 - val_loss: 0.8648 - val_accuracy: 0.6358\n",
      "Epoch 21/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8489 - accuracy: 0.6347 - val_loss: 0.8770 - val_accuracy: 0.6309\n",
      "Epoch 22/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8473 - accuracy: 0.6330 - val_loss: 0.8574 - val_accuracy: 0.6369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8483 - accuracy: 0.6365 - val_loss: 0.8593 - val_accuracy: 0.6405\n",
      "Epoch 24/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8451 - accuracy: 0.6340 - val_loss: 0.8570 - val_accuracy: 0.6380\n",
      "Epoch 25/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.8426 - accuracy: 0.6349 - val_loss: 0.8582 - val_accuracy: 0.6364\n",
      "Epoch 26/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8404 - accuracy: 0.6363 - val_loss: 0.8628 - val_accuracy: 0.6342\n",
      "Epoch 27/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8383 - accuracy: 0.6368 - val_loss: 0.8651 - val_accuracy: 0.6295\n",
      "Epoch 28/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8354 - accuracy: 0.6377 - val_loss: 0.8573 - val_accuracy: 0.6402\n",
      "Epoch 29/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8350 - accuracy: 0.6392 - val_loss: 0.8576 - val_accuracy: 0.6356\n",
      "Epoch 30/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8332 - accuracy: 0.6409 - val_loss: 0.8527 - val_accuracy: 0.6378\n",
      "Epoch 31/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8350 - accuracy: 0.6385 - val_loss: 0.8494 - val_accuracy: 0.6443\n",
      "Epoch 32/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8319 - accuracy: 0.6379 - val_loss: 0.8462 - val_accuracy: 0.6427\n",
      "Epoch 33/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8311 - accuracy: 0.6394 - val_loss: 0.8479 - val_accuracy: 0.6394\n",
      "Epoch 34/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8299 - accuracy: 0.6422 - val_loss: 0.8439 - val_accuracy: 0.6460\n",
      "Epoch 35/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8254 - accuracy: 0.6426 - val_loss: 0.8484 - val_accuracy: 0.6328\n",
      "Epoch 36/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8274 - accuracy: 0.6405 - val_loss: 0.8443 - val_accuracy: 0.6386\n",
      "Epoch 37/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8231 - accuracy: 0.6407 - val_loss: 0.8451 - val_accuracy: 0.6408\n",
      "Epoch 38/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8220 - accuracy: 0.6437 - val_loss: 0.8495 - val_accuracy: 0.6279\n",
      "Epoch 39/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8219 - accuracy: 0.6408 - val_loss: 0.8409 - val_accuracy: 0.6432\n",
      "Epoch 40/80\n",
      "32788/32788 [==============================] - 1s 39us/step - loss: 0.8175 - accuracy: 0.6467 - val_loss: 0.8386 - val_accuracy: 0.6411\n",
      "Epoch 41/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8188 - accuracy: 0.6450 - val_loss: 0.8361 - val_accuracy: 0.6375\n",
      "Epoch 42/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8156 - accuracy: 0.6472 - val_loss: 0.8308 - val_accuracy: 0.6493\n",
      "Epoch 43/80\n",
      "32788/32788 [==============================] - 1s 39us/step - loss: 0.8154 - accuracy: 0.6453 - val_loss: 0.8285 - val_accuracy: 0.6435\n",
      "Epoch 44/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8150 - accuracy: 0.6469 - val_loss: 0.8353 - val_accuracy: 0.6378\n",
      "Epoch 45/80\n",
      "32788/32788 [==============================] - 1s 39us/step - loss: 0.8109 - accuracy: 0.6482 - val_loss: 0.8324 - val_accuracy: 0.6438\n",
      "Epoch 46/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8097 - accuracy: 0.6469 - val_loss: 0.8345 - val_accuracy: 0.6411\n",
      "Epoch 47/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8104 - accuracy: 0.6474 - val_loss: 0.8279 - val_accuracy: 0.6498\n",
      "Epoch 48/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.8075 - accuracy: 0.6492 - val_loss: 0.8358 - val_accuracy: 0.6446\n",
      "Epoch 49/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8060 - accuracy: 0.6519 - val_loss: 0.8492 - val_accuracy: 0.6218\n",
      "Epoch 50/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8045 - accuracy: 0.6489 - val_loss: 0.8248 - val_accuracy: 0.6460\n",
      "Epoch 51/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8029 - accuracy: 0.6502 - val_loss: 0.8229 - val_accuracy: 0.6416\n",
      "Epoch 52/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8015 - accuracy: 0.6519 - val_loss: 0.8316 - val_accuracy: 0.6339\n",
      "Epoch 53/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8003 - accuracy: 0.6520 - val_loss: 0.8166 - val_accuracy: 0.6518\n",
      "Epoch 54/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8009 - accuracy: 0.6541 - val_loss: 0.8214 - val_accuracy: 0.6520\n",
      "Epoch 55/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7971 - accuracy: 0.6555 - val_loss: 0.8245 - val_accuracy: 0.6397\n",
      "Epoch 56/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7954 - accuracy: 0.6570 - val_loss: 0.8183 - val_accuracy: 0.6487\n",
      "Epoch 57/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7939 - accuracy: 0.6580 - val_loss: 0.8124 - val_accuracy: 0.6567\n",
      "Epoch 58/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7919 - accuracy: 0.6572 - val_loss: 0.8233 - val_accuracy: 0.6402\n",
      "Epoch 59/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7908 - accuracy: 0.6542 - val_loss: 0.8173 - val_accuracy: 0.6509\n",
      "Epoch 60/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7920 - accuracy: 0.6569 - val_loss: 0.8155 - val_accuracy: 0.6474\n",
      "Epoch 61/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7893 - accuracy: 0.6572 - val_loss: 0.8228 - val_accuracy: 0.6372\n",
      "Epoch 62/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7857 - accuracy: 0.6566 - val_loss: 0.8201 - val_accuracy: 0.6540\n",
      "Epoch 63/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7853 - accuracy: 0.6586 - val_loss: 0.8122 - val_accuracy: 0.6454\n",
      "Epoch 64/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7871 - accuracy: 0.6570 - val_loss: 0.8429 - val_accuracy: 0.6309\n",
      "Epoch 65/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7872 - accuracy: 0.6569 - val_loss: 0.8051 - val_accuracy: 0.6556\n",
      "Epoch 66/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7832 - accuracy: 0.6605 - val_loss: 0.8113 - val_accuracy: 0.6380\n",
      "Epoch 67/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7834 - accuracy: 0.6591 - val_loss: 0.8237 - val_accuracy: 0.6378\n",
      "Epoch 68/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7828 - accuracy: 0.6601 - val_loss: 0.8264 - val_accuracy: 0.6314\n",
      "Epoch 69/80\n",
      "32788/32788 [==============================] - 2s 46us/step - loss: 0.7825 - accuracy: 0.6594 - val_loss: 0.8090 - val_accuracy: 0.6465\n",
      "Epoch 70/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7800 - accuracy: 0.6614 - val_loss: 0.8016 - val_accuracy: 0.6581\n",
      "Epoch 71/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7822 - accuracy: 0.6590 - val_loss: 0.8049 - val_accuracy: 0.6476\n",
      "Epoch 72/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7782 - accuracy: 0.6609 - val_loss: 0.8003 - val_accuracy: 0.6537\n",
      "Epoch 73/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7778 - accuracy: 0.6591 - val_loss: 0.8025 - val_accuracy: 0.6542\n",
      "Epoch 74/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7794 - accuracy: 0.6584 - val_loss: 0.8027 - val_accuracy: 0.6586\n",
      "Epoch 75/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7785 - accuracy: 0.6606 - val_loss: 0.8138 - val_accuracy: 0.6386\n",
      "Epoch 76/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.7790 - accuracy: 0.6587 - val_loss: 0.8065 - val_accuracy: 0.6542\n",
      "Epoch 77/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7779 - accuracy: 0.6623 - val_loss: 0.8007 - val_accuracy: 0.6561\n",
      "Epoch 78/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7755 - accuracy: 0.6607 - val_loss: 0.8151 - val_accuracy: 0.6443\n",
      "Epoch 79/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7759 - accuracy: 0.6625 - val_loss: 0.7979 - val_accuracy: 0.6567\n",
      "Epoch 80/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7747 - accuracy: 0.6608 - val_loss: 0.8083 - val_accuracy: 0.6496\n",
      "[[7.4032193e-01 4.3072523e-05 3.7067843e-04 ... 2.0996451e-03\n",
      "  6.6232368e-02 6.2500532e-03]\n",
      " [4.4402736e-01 1.0215059e-03 5.4440321e-03 ... 6.3395721e-04\n",
      "  5.2582902e-01 4.7758818e-04]\n",
      " [7.1626544e-01 4.2263204e-05 2.6651268e-04 ... 2.5138916e-03\n",
      "  1.9821169e-02 6.2365257e-03]\n",
      " ...\n",
      " [2.9687011e-03 8.6225756e-03 9.3972117e-01 ... 1.4355365e-04\n",
      "  2.7013902e-02 1.4053594e-05]\n",
      " [4.7904161e-01 2.1429032e-03 3.0419269e-01 ... 1.2203893e-04\n",
      "  1.6461897e-01 1.3196818e-05]\n",
      " [5.2497524e-01 2.3547758e-03 2.6059043e-01 ... 1.3754646e-04\n",
      "  1.5716845e-01 1.3383630e-05]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:07:08.146854\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 32788 samples, validate on 3644 samples\n",
      "Epoch 1/80\n",
      "32788/32788 [==============================] - 2s 50us/step - loss: 1.4629 - accuracy: 0.4139 - val_loss: 1.0866 - val_accuracy: 0.5524\n",
      "Epoch 2/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 1.0074 - accuracy: 0.5953 - val_loss: 0.9698 - val_accuracy: 0.5886\n",
      "Epoch 3/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9498 - accuracy: 0.6062 - val_loss: 0.9314 - val_accuracy: 0.6048\n",
      "Epoch 4/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9167 - accuracy: 0.6165 - val_loss: 0.9138 - val_accuracy: 0.6078\n",
      "Epoch 5/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.9014 - accuracy: 0.6185 - val_loss: 0.8965 - val_accuracy: 0.6147\n",
      "Epoch 6/80\n",
      "32788/32788 [==============================] - 1s 46us/step - loss: 0.8891 - accuracy: 0.6219 - val_loss: 0.9162 - val_accuracy: 0.6161\n",
      "Epoch 7/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8832 - accuracy: 0.6242 - val_loss: 0.9077 - val_accuracy: 0.6213\n",
      "Epoch 8/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8779 - accuracy: 0.6245 - val_loss: 0.8789 - val_accuracy: 0.6218\n",
      "Epoch 9/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8709 - accuracy: 0.6267 - val_loss: 0.8765 - val_accuracy: 0.6183\n",
      "Epoch 10/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8680 - accuracy: 0.6263 - val_loss: 0.8704 - val_accuracy: 0.6249\n",
      "Epoch 11/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8650 - accuracy: 0.6289 - val_loss: 0.8652 - val_accuracy: 0.6298\n",
      "Epoch 12/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8599 - accuracy: 0.6312 - val_loss: 0.8670 - val_accuracy: 0.6334\n",
      "Epoch 13/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8577 - accuracy: 0.6296 - val_loss: 0.8595 - val_accuracy: 0.6331\n",
      "Epoch 14/80\n",
      "32788/32788 [==============================] - 1s 45us/step - loss: 0.8560 - accuracy: 0.6338 - val_loss: 0.8584 - val_accuracy: 0.6331\n",
      "Epoch 15/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8515 - accuracy: 0.6348 - val_loss: 0.8588 - val_accuracy: 0.6262\n",
      "Epoch 16/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8509 - accuracy: 0.6325 - val_loss: 0.8649 - val_accuracy: 0.6240\n",
      "Epoch 17/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8510 - accuracy: 0.6297 - val_loss: 0.8574 - val_accuracy: 0.6328\n",
      "Epoch 18/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8473 - accuracy: 0.6350 - val_loss: 0.8559 - val_accuracy: 0.6314\n",
      "Epoch 19/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8450 - accuracy: 0.6358 - val_loss: 0.8624 - val_accuracy: 0.6356\n",
      "Epoch 20/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8455 - accuracy: 0.6344 - val_loss: 0.8470 - val_accuracy: 0.6432\n",
      "Epoch 21/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8420 - accuracy: 0.6352 - val_loss: 0.8538 - val_accuracy: 0.6364\n",
      "Epoch 22/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8377 - accuracy: 0.6383 - val_loss: 0.8473 - val_accuracy: 0.6293\n",
      "Epoch 23/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8378 - accuracy: 0.6374 - val_loss: 0.8451 - val_accuracy: 0.6353\n",
      "Epoch 24/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8371 - accuracy: 0.6348 - val_loss: 0.8511 - val_accuracy: 0.6323\n",
      "Epoch 25/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8404 - accuracy: 0.6330 - val_loss: 0.8423 - val_accuracy: 0.6394\n",
      "Epoch 26/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8337 - accuracy: 0.6377 - val_loss: 0.8457 - val_accuracy: 0.6282\n",
      "Epoch 27/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8317 - accuracy: 0.6410 - val_loss: 0.8451 - val_accuracy: 0.6293\n",
      "Epoch 28/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8294 - accuracy: 0.6398 - val_loss: 0.8466 - val_accuracy: 0.6446\n",
      "Epoch 29/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8312 - accuracy: 0.6405 - val_loss: 0.8362 - val_accuracy: 0.6452\n",
      "Epoch 30/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8327 - accuracy: 0.6400 - val_loss: 0.8327 - val_accuracy: 0.6369\n",
      "Epoch 31/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8288 - accuracy: 0.6400 - val_loss: 0.8312 - val_accuracy: 0.6432\n",
      "Epoch 32/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.8257 - accuracy: 0.6422 - val_loss: 0.8404 - val_accuracy: 0.6391\n",
      "Epoch 33/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8256 - accuracy: 0.6405 - val_loss: 0.8255 - val_accuracy: 0.6446\n",
      "Epoch 34/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8237 - accuracy: 0.6414 - val_loss: 0.8215 - val_accuracy: 0.6507\n",
      "Epoch 35/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8250 - accuracy: 0.6412 - val_loss: 0.8307 - val_accuracy: 0.6471\n",
      "Epoch 36/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8198 - accuracy: 0.6420 - val_loss: 0.8310 - val_accuracy: 0.6419\n",
      "Epoch 37/80\n",
      "32788/32788 [==============================] - 1s 40us/step - loss: 0.8191 - accuracy: 0.6428 - val_loss: 0.8334 - val_accuracy: 0.6347\n",
      "Epoch 38/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8187 - accuracy: 0.6432 - val_loss: 0.8253 - val_accuracy: 0.6471\n",
      "Epoch 39/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8173 - accuracy: 0.6428 - val_loss: 0.8426 - val_accuracy: 0.6331\n",
      "Epoch 40/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8149 - accuracy: 0.6451 - val_loss: 0.8241 - val_accuracy: 0.6537\n",
      "Epoch 41/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8140 - accuracy: 0.6466 - val_loss: 0.8144 - val_accuracy: 0.6457\n",
      "Epoch 42/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8147 - accuracy: 0.6445 - val_loss: 0.8234 - val_accuracy: 0.6468\n",
      "Epoch 43/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8118 - accuracy: 0.6462 - val_loss: 0.8275 - val_accuracy: 0.6432\n",
      "Epoch 44/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.8122 - accuracy: 0.6447 - val_loss: 0.8213 - val_accuracy: 0.6474\n",
      "Epoch 45/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8097 - accuracy: 0.6481 - val_loss: 0.8118 - val_accuracy: 0.6474\n",
      "Epoch 46/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8081 - accuracy: 0.6471 - val_loss: 0.8178 - val_accuracy: 0.6482\n",
      "Epoch 47/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8069 - accuracy: 0.6483 - val_loss: 0.8197 - val_accuracy: 0.6490\n",
      "Epoch 48/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.8068 - accuracy: 0.6481 - val_loss: 0.8041 - val_accuracy: 0.6572\n",
      "Epoch 49/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.8020 - accuracy: 0.6497 - val_loss: 0.8195 - val_accuracy: 0.6465\n",
      "Epoch 50/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.8008 - accuracy: 0.6501 - val_loss: 0.8083 - val_accuracy: 0.6529\n",
      "Epoch 51/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7985 - accuracy: 0.6527 - val_loss: 0.8011 - val_accuracy: 0.6542\n",
      "Epoch 52/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7978 - accuracy: 0.6496 - val_loss: 0.8045 - val_accuracy: 0.6490\n",
      "Epoch 53/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7953 - accuracy: 0.6521 - val_loss: 0.7970 - val_accuracy: 0.6619\n",
      "Epoch 54/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7922 - accuracy: 0.6541 - val_loss: 0.8094 - val_accuracy: 0.6531\n",
      "Epoch 55/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7938 - accuracy: 0.6544 - val_loss: 0.8023 - val_accuracy: 0.6537\n",
      "Epoch 56/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7892 - accuracy: 0.6549 - val_loss: 0.8018 - val_accuracy: 0.6534\n",
      "Epoch 57/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7890 - accuracy: 0.6546 - val_loss: 0.7950 - val_accuracy: 0.6531\n",
      "Epoch 58/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7871 - accuracy: 0.6546 - val_loss: 0.7977 - val_accuracy: 0.6583\n",
      "Epoch 59/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7867 - accuracy: 0.6546 - val_loss: 0.7958 - val_accuracy: 0.6622\n",
      "Epoch 60/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7868 - accuracy: 0.6554 - val_loss: 0.7924 - val_accuracy: 0.6578\n",
      "Epoch 61/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7869 - accuracy: 0.6537 - val_loss: 0.7894 - val_accuracy: 0.6611\n",
      "Epoch 62/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7877 - accuracy: 0.6523 - val_loss: 0.7938 - val_accuracy: 0.6625\n",
      "Epoch 63/80\n",
      "32788/32788 [==============================] - 1s 46us/step - loss: 0.7840 - accuracy: 0.6535 - val_loss: 0.8054 - val_accuracy: 0.6485\n",
      "Epoch 64/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7840 - accuracy: 0.6557 - val_loss: 0.7941 - val_accuracy: 0.6611\n",
      "Epoch 65/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7816 - accuracy: 0.6556 - val_loss: 0.7828 - val_accuracy: 0.6663\n",
      "Epoch 66/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7818 - accuracy: 0.6551 - val_loss: 0.7895 - val_accuracy: 0.6550\n",
      "Epoch 67/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7807 - accuracy: 0.6576 - val_loss: 0.7862 - val_accuracy: 0.6589\n",
      "Epoch 68/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7802 - accuracy: 0.6588 - val_loss: 0.7881 - val_accuracy: 0.6586\n",
      "Epoch 69/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7781 - accuracy: 0.6568 - val_loss: 0.7871 - val_accuracy: 0.6589\n",
      "Epoch 70/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7777 - accuracy: 0.6569 - val_loss: 0.8009 - val_accuracy: 0.6501\n",
      "Epoch 71/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7779 - accuracy: 0.6566 - val_loss: 0.7885 - val_accuracy: 0.6611\n",
      "Epoch 72/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7772 - accuracy: 0.6578 - val_loss: 0.7829 - val_accuracy: 0.6647\n",
      "Epoch 73/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7749 - accuracy: 0.6577 - val_loss: 0.7870 - val_accuracy: 0.6633\n",
      "Epoch 74/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7759 - accuracy: 0.6580 - val_loss: 0.7791 - val_accuracy: 0.6649\n",
      "Epoch 75/80\n",
      "32788/32788 [==============================] - 1s 41us/step - loss: 0.7737 - accuracy: 0.6584 - val_loss: 0.7873 - val_accuracy: 0.6616\n",
      "Epoch 76/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7762 - accuracy: 0.6587 - val_loss: 0.7795 - val_accuracy: 0.6589\n",
      "Epoch 77/80\n",
      "32788/32788 [==============================] - 1s 43us/step - loss: 0.7739 - accuracy: 0.6602 - val_loss: 0.7843 - val_accuracy: 0.6550\n",
      "Epoch 78/80\n",
      "32788/32788 [==============================] - 1s 42us/step - loss: 0.7739 - accuracy: 0.6568 - val_loss: 0.7764 - val_accuracy: 0.6655\n",
      "Epoch 79/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7719 - accuracy: 0.6590 - val_loss: 0.7823 - val_accuracy: 0.6578\n",
      "Epoch 80/80\n",
      "32788/32788 [==============================] - 1s 44us/step - loss: 0.7708 - accuracy: 0.6612 - val_loss: 0.7838 - val_accuracy: 0.6638\n",
      "[[6.84552848e-01 1.57570990e-04 6.16341177e-03 ... 1.83217309e-03\n",
      "  2.85911206e-02 1.03768501e-02]\n",
      " [3.55548173e-01 3.11754819e-04 5.93088800e-03 ... 4.38746123e-04\n",
      "  6.21798992e-01 1.61104882e-03]\n",
      " [7.46496499e-01 7.09867818e-05 8.56368162e-04 ... 1.29980943e-03\n",
      "  9.15529765e-03 3.25764855e-03]\n",
      " ...\n",
      " [2.87202187e-03 6.62935153e-03 9.40513372e-01 ... 1.36291259e-04\n",
      "  3.24101336e-02 3.15572356e-06]\n",
      " [2.51553446e-01 2.29603495e-03 4.88510132e-01 ... 9.24675260e-05\n",
      "  1.29428357e-01 7.67867459e-05]\n",
      " [2.65529931e-01 2.53556645e-03 4.57381546e-01 ... 1.00775804e-04\n",
      "  1.30465791e-01 8.06513199e-05]]\n",
      "[0 8 0 ... 2 2 2]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:09:06.809733\n",
      "num of merged_region_image 0 4419\n",
      "num of merged_region_image 1 3215\n",
      "num of merged_region_image 2 3473\n",
      "num of merged_region_image 3 4586\n",
      "num of merged_region_image 4 2733\n",
      "num of merged_region_image 5 3276\n",
      "num of merged_region_image 6 3379\n",
      "num of merged_region_image 7 4000\n",
      "num of merged_region_image 8 3923\n",
      "num of merged_region_image 9 3428\n",
      "Counter({-1: 18568, 3: 4586, 0: 4419, 7: 4000, 8: 3923, 2: 3473, 9: 3428, 6: 3379, 5: 3276, 1: 3215, 4: 2733})\n",
      "===========  ITE = 0   ===========\n",
      "used_img 36432 36432\n",
      "working_img(=other images=unclean images) 18568 18568\n",
      "merged regions 132 132\n",
      "other_regions 68 68\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>132</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>116</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.76</td>\n",
       "      <td>247</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>127</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>310</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    other index  pred label  truth  rate    0    1    2    3    4  5  6    7  \\\n",
       "0             1           9      3  0.60    0    0    0  244    0  2  0    0   \n",
       "1             3           7      3  0.42    0    0    0  202    0  0  0    1   \n",
       "2           131           4      1  0.86    0  273    0    0    0  0  0    0   \n",
       "3           132          -1      0  0.00  223    0    1    0    0  1  0    0   \n",
       "4             7           2      4  0.64    0    2    0    0  258  0  0    2   \n",
       "..          ...         ...    ...   ...  ...  ...  ...  ...  ... .. ..  ...   \n",
       "63          116           6      8  0.90    0    0    0    1    0  0  0    0   \n",
       "64          119           3      7  0.55    0    0   18    2    0  0  0  250   \n",
       "65          121           0      0  0.76  247    0    3    0    0  0  1    0   \n",
       "66          126           2      4  0.69    0    7    0    0  235  0  0    4   \n",
       "67          127           9      2  0.83    2    0  310    2    0  0  0    0   \n",
       "\n",
       "      8  9  \n",
       "0     0  6  \n",
       "1     3  1  \n",
       "2     0  0  \n",
       "3     0  0  \n",
       "4     0  7  \n",
       "..  ... ..  \n",
       "63  265  0  \n",
       "64    0  1  \n",
       "65    0  0  \n",
       "66    4  3  \n",
       "67    2  1  \n",
       "\n",
       "[68 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [121] 192\n",
      "added label, regions, img amount: {2} [147, 68, 181] 591\n",
      "added label, regions, img amount: {4} [176, 188, 131] 591\n",
      "added label, regions, img amount: {6} [116, 167, 84] 674\n",
      "added label, regions, img amount: {7} [145] 241\n",
      "added label, regions, img amount: {8} [48] 276\n",
      "added label, regions, img amount: {9} [144, 187, 74, 141, 127] 993\n",
      "NUM_region 10\n",
      "number of clean images 39990\n",
      "n, p1, p2 0 0 0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35991 samples, validate on 3999 samples\n",
      "Epoch 1/80\n",
      "35991/35991 [==============================] - 2s 56us/step - loss: 1.3536 - accuracy: 0.4768 - val_loss: 1.0215 - val_accuracy: 0.5799\n",
      "Epoch 2/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.9571 - accuracy: 0.6326 - val_loss: 0.9257 - val_accuracy: 0.6389\n",
      "Epoch 3/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.9055 - accuracy: 0.6394 - val_loss: 0.8810 - val_accuracy: 0.6477\n",
      "Epoch 4/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8714 - accuracy: 0.6466 - val_loss: 0.8555 - val_accuracy: 0.6529\n",
      "Epoch 5/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.8566 - accuracy: 0.6501 - val_loss: 0.8458 - val_accuracy: 0.6544\n",
      "Epoch 6/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8399 - accuracy: 0.6549 - val_loss: 0.8464 - val_accuracy: 0.6559\n",
      "Epoch 7/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.8287 - accuracy: 0.6592 - val_loss: 0.8236 - val_accuracy: 0.6559\n",
      "Epoch 8/80\n",
      "35991/35991 [==============================] - 2s 46us/step - loss: 0.8230 - accuracy: 0.6624 - val_loss: 0.8470 - val_accuracy: 0.6502\n",
      "Epoch 9/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8204 - accuracy: 0.6602 - val_loss: 0.8516 - val_accuracy: 0.6452\n",
      "Epoch 10/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8128 - accuracy: 0.6640 - val_loss: 0.8095 - val_accuracy: 0.6632\n",
      "Epoch 11/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8098 - accuracy: 0.6656 - val_loss: 0.8116 - val_accuracy: 0.6609\n",
      "Epoch 12/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8051 - accuracy: 0.6659 - val_loss: 0.8227 - val_accuracy: 0.6549\n",
      "Epoch 13/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8055 - accuracy: 0.6649 - val_loss: 0.7981 - val_accuracy: 0.6689\n",
      "Epoch 14/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7984 - accuracy: 0.6686 - val_loss: 0.7946 - val_accuracy: 0.6769\n",
      "Epoch 15/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7957 - accuracy: 0.6695 - val_loss: 0.7968 - val_accuracy: 0.6722\n",
      "Epoch 16/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7925 - accuracy: 0.6699 - val_loss: 0.8261 - val_accuracy: 0.6599\n",
      "Epoch 17/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7925 - accuracy: 0.6684 - val_loss: 0.7970 - val_accuracy: 0.6607\n",
      "Epoch 18/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7891 - accuracy: 0.6725 - val_loss: 0.7872 - val_accuracy: 0.6622\n",
      "Epoch 19/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7863 - accuracy: 0.6722 - val_loss: 0.8024 - val_accuracy: 0.6649\n",
      "Epoch 20/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7859 - accuracy: 0.6721 - val_loss: 0.7855 - val_accuracy: 0.6747\n",
      "Epoch 21/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7828 - accuracy: 0.6719 - val_loss: 0.7813 - val_accuracy: 0.6784\n",
      "Epoch 22/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7792 - accuracy: 0.6756 - val_loss: 0.7853 - val_accuracy: 0.6697\n",
      "Epoch 23/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7802 - accuracy: 0.6746 - val_loss: 0.7817 - val_accuracy: 0.6779\n",
      "Epoch 24/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7798 - accuracy: 0.6715 - val_loss: 0.7823 - val_accuracy: 0.6717\n",
      "Epoch 25/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7773 - accuracy: 0.6764 - val_loss: 0.7859 - val_accuracy: 0.6744\n",
      "Epoch 26/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7753 - accuracy: 0.6745 - val_loss: 0.7836 - val_accuracy: 0.6634\n",
      "Epoch 27/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7744 - accuracy: 0.6775 - val_loss: 0.7853 - val_accuracy: 0.6752\n",
      "Epoch 28/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7715 - accuracy: 0.6769 - val_loss: 0.7873 - val_accuracy: 0.6689\n",
      "Epoch 29/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7715 - accuracy: 0.6771 - val_loss: 0.7740 - val_accuracy: 0.6732\n",
      "Epoch 30/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7683 - accuracy: 0.6776 - val_loss: 0.7846 - val_accuracy: 0.6697\n",
      "Epoch 31/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7684 - accuracy: 0.6779 - val_loss: 0.7759 - val_accuracy: 0.6702\n",
      "Epoch 32/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7679 - accuracy: 0.6799 - val_loss: 0.7761 - val_accuracy: 0.6642\n",
      "Epoch 33/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7663 - accuracy: 0.6797 - val_loss: 0.7714 - val_accuracy: 0.6822\n",
      "Epoch 34/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7656 - accuracy: 0.6804 - val_loss: 0.7638 - val_accuracy: 0.6879\n",
      "Epoch 35/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7648 - accuracy: 0.6805 - val_loss: 0.7826 - val_accuracy: 0.6682\n",
      "Epoch 36/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7644 - accuracy: 0.6809 - val_loss: 0.7723 - val_accuracy: 0.6834\n",
      "Epoch 37/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7627 - accuracy: 0.6818 - val_loss: 0.7657 - val_accuracy: 0.6764\n",
      "Epoch 38/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7605 - accuracy: 0.6828 - val_loss: 0.7674 - val_accuracy: 0.6674\n",
      "Epoch 39/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7601 - accuracy: 0.6810 - val_loss: 0.7621 - val_accuracy: 0.6832\n",
      "Epoch 40/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7583 - accuracy: 0.6814 - val_loss: 0.7679 - val_accuracy: 0.6804\n",
      "Epoch 41/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7565 - accuracy: 0.6822 - val_loss: 0.7716 - val_accuracy: 0.6814\n",
      "Epoch 42/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7566 - accuracy: 0.6824 - val_loss: 0.7644 - val_accuracy: 0.6839\n",
      "Epoch 43/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7556 - accuracy: 0.6843 - val_loss: 0.7683 - val_accuracy: 0.6687\n",
      "Epoch 44/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7562 - accuracy: 0.6835 - val_loss: 0.7699 - val_accuracy: 0.6817\n",
      "Epoch 45/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7580 - accuracy: 0.6828 - val_loss: 0.7662 - val_accuracy: 0.6799\n",
      "Epoch 46/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7530 - accuracy: 0.6843 - val_loss: 0.7650 - val_accuracy: 0.6797\n",
      "Epoch 47/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7531 - accuracy: 0.6853 - val_loss: 0.7745 - val_accuracy: 0.6814\n",
      "Epoch 48/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7544 - accuracy: 0.6838 - val_loss: 0.7690 - val_accuracy: 0.6807\n",
      "Epoch 49/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7513 - accuracy: 0.6848 - val_loss: 0.7647 - val_accuracy: 0.6817\n",
      "Epoch 00049: early stopping\n",
      "[[7.2174585e-01 4.3296776e-04 3.0903544e-03 ... 2.7630243e-03\n",
      "  6.0965497e-02 1.0053289e-02]\n",
      " [2.7808413e-01 8.0288074e-04 5.1699327e-03 ... 1.6962586e-04\n",
      "  6.8836755e-01 3.1372570e-03]\n",
      " [6.9501466e-01 2.4222236e-04 2.4409373e-03 ... 3.9536720e-03\n",
      "  1.8306140e-02 1.3062961e-02]\n",
      " ...\n",
      " [6.7432313e-03 2.9393747e-02 8.5624313e-01 ... 6.7490054e-04\n",
      "  1.3853410e-02 1.3612791e-04]\n",
      " [3.6151576e-01 9.5346272e-03 3.1724116e-01 ... 1.6503794e-04\n",
      "  2.5348923e-01 3.9350954e-04]\n",
      " [4.2456153e-01 8.1208609e-03 2.1592890e-01 ... 1.4383117e-04\n",
      "  2.8787166e-01 3.3099175e-04]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:01:21.649776\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35991 samples, validate on 3999 samples\n",
      "Epoch 1/80\n",
      "35991/35991 [==============================] - 2s 48us/step - loss: 1.3171 - accuracy: 0.4781 - val_loss: 1.0021 - val_accuracy: 0.6174\n",
      "Epoch 2/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.9352 - accuracy: 0.6347 - val_loss: 0.9174 - val_accuracy: 0.6257\n",
      "Epoch 3/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.8880 - accuracy: 0.6411 - val_loss: 0.8889 - val_accuracy: 0.6337\n",
      "Epoch 4/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.8597 - accuracy: 0.6488 - val_loss: 0.8656 - val_accuracy: 0.6382\n",
      "Epoch 5/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.8471 - accuracy: 0.6486 - val_loss: 0.8523 - val_accuracy: 0.6472\n",
      "Epoch 6/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.8322 - accuracy: 0.6527 - val_loss: 0.8492 - val_accuracy: 0.6499\n",
      "Epoch 7/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8267 - accuracy: 0.6548 - val_loss: 0.8371 - val_accuracy: 0.6502\n",
      "Epoch 8/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.8216 - accuracy: 0.6571 - val_loss: 0.8402 - val_accuracy: 0.6517\n",
      "Epoch 9/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8153 - accuracy: 0.6591 - val_loss: 0.8333 - val_accuracy: 0.6544\n",
      "Epoch 10/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.8100 - accuracy: 0.6582 - val_loss: 0.8206 - val_accuracy: 0.6612\n",
      "Epoch 11/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8064 - accuracy: 0.6590 - val_loss: 0.8326 - val_accuracy: 0.6642\n",
      "Epoch 12/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.8037 - accuracy: 0.6638 - val_loss: 0.8265 - val_accuracy: 0.6589\n",
      "Epoch 13/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7983 - accuracy: 0.6641 - val_loss: 0.8087 - val_accuracy: 0.6732\n",
      "Epoch 14/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7955 - accuracy: 0.6655 - val_loss: 0.8113 - val_accuracy: 0.6639\n",
      "Epoch 15/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.7940 - accuracy: 0.6657 - val_loss: 0.8182 - val_accuracy: 0.6597\n",
      "Epoch 16/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7917 - accuracy: 0.6673 - val_loss: 0.8199 - val_accuracy: 0.6714\n",
      "Epoch 17/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7894 - accuracy: 0.6688 - val_loss: 0.8261 - val_accuracy: 0.6602\n",
      "Epoch 18/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7875 - accuracy: 0.6676 - val_loss: 0.8020 - val_accuracy: 0.6617\n",
      "Epoch 19/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7855 - accuracy: 0.6679 - val_loss: 0.8108 - val_accuracy: 0.6567\n",
      "Epoch 20/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.7819 - accuracy: 0.6707 - val_loss: 0.8025 - val_accuracy: 0.6694\n",
      "Epoch 21/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7826 - accuracy: 0.6689 - val_loss: 0.8002 - val_accuracy: 0.6634\n",
      "Epoch 22/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7798 - accuracy: 0.6676 - val_loss: 0.8124 - val_accuracy: 0.6687\n",
      "Epoch 23/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7771 - accuracy: 0.6713 - val_loss: 0.7966 - val_accuracy: 0.6669\n",
      "Epoch 24/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7767 - accuracy: 0.6703 - val_loss: 0.8089 - val_accuracy: 0.6707\n",
      "Epoch 25/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7779 - accuracy: 0.6699 - val_loss: 0.7993 - val_accuracy: 0.6739\n",
      "Epoch 26/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7763 - accuracy: 0.6707 - val_loss: 0.7999 - val_accuracy: 0.6769\n",
      "Epoch 27/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7746 - accuracy: 0.6716 - val_loss: 0.7983 - val_accuracy: 0.6697\n",
      "Epoch 28/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7747 - accuracy: 0.6720 - val_loss: 0.8177 - val_accuracy: 0.6669\n",
      "Epoch 29/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7711 - accuracy: 0.6706 - val_loss: 0.7880 - val_accuracy: 0.6764\n",
      "Epoch 30/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7709 - accuracy: 0.6749 - val_loss: 0.7983 - val_accuracy: 0.6659\n",
      "Epoch 31/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7701 - accuracy: 0.6740 - val_loss: 0.7889 - val_accuracy: 0.6749\n",
      "Epoch 32/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7685 - accuracy: 0.6734 - val_loss: 0.8051 - val_accuracy: 0.6754\n",
      "Epoch 33/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7684 - accuracy: 0.6747 - val_loss: 0.7834 - val_accuracy: 0.6747\n",
      "Epoch 34/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7663 - accuracy: 0.6753 - val_loss: 0.7827 - val_accuracy: 0.6732\n",
      "Epoch 35/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7670 - accuracy: 0.6726 - val_loss: 0.7876 - val_accuracy: 0.6644\n",
      "Epoch 36/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7623 - accuracy: 0.6778 - val_loss: 0.7963 - val_accuracy: 0.6727\n",
      "Epoch 37/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7645 - accuracy: 0.6751 - val_loss: 0.7948 - val_accuracy: 0.6707\n",
      "Epoch 38/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7617 - accuracy: 0.6764 - val_loss: 0.7871 - val_accuracy: 0.6742\n",
      "Epoch 39/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7629 - accuracy: 0.6783 - val_loss: 0.7899 - val_accuracy: 0.6769\n",
      "Epoch 40/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7624 - accuracy: 0.6777 - val_loss: 0.7786 - val_accuracy: 0.6757\n",
      "Epoch 41/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7586 - accuracy: 0.6764 - val_loss: 0.7793 - val_accuracy: 0.6769\n",
      "Epoch 42/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7580 - accuracy: 0.6785 - val_loss: 0.7746 - val_accuracy: 0.6819\n",
      "Epoch 43/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7603 - accuracy: 0.6779 - val_loss: 0.7769 - val_accuracy: 0.6752\n",
      "Epoch 44/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7567 - accuracy: 0.6794 - val_loss: 0.7851 - val_accuracy: 0.6784\n",
      "Epoch 45/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7572 - accuracy: 0.6777 - val_loss: 0.7779 - val_accuracy: 0.6832\n",
      "Epoch 46/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7546 - accuracy: 0.6810 - val_loss: 0.7781 - val_accuracy: 0.6757\n",
      "Epoch 47/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7564 - accuracy: 0.6793 - val_loss: 0.7760 - val_accuracy: 0.6822\n",
      "Epoch 48/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7548 - accuracy: 0.6805 - val_loss: 0.7743 - val_accuracy: 0.6792\n",
      "Epoch 49/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7537 - accuracy: 0.6798 - val_loss: 0.7701 - val_accuracy: 0.6857\n",
      "Epoch 50/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7525 - accuracy: 0.6810 - val_loss: 0.7807 - val_accuracy: 0.6799\n",
      "Epoch 51/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7519 - accuracy: 0.6818 - val_loss: 0.7719 - val_accuracy: 0.6772\n",
      "Epoch 52/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7515 - accuracy: 0.6808 - val_loss: 0.7791 - val_accuracy: 0.6752\n",
      "Epoch 53/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7504 - accuracy: 0.6819 - val_loss: 0.7805 - val_accuracy: 0.6727\n",
      "Epoch 54/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7495 - accuracy: 0.6831 - val_loss: 0.7714 - val_accuracy: 0.6789\n",
      "Epoch 55/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7494 - accuracy: 0.6820 - val_loss: 0.7810 - val_accuracy: 0.6742\n",
      "Epoch 56/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7499 - accuracy: 0.6831 - val_loss: 0.7789 - val_accuracy: 0.6722\n",
      "Epoch 57/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7465 - accuracy: 0.6843 - val_loss: 0.7803 - val_accuracy: 0.6809\n",
      "Epoch 58/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7482 - accuracy: 0.6835 - val_loss: 0.7777 - val_accuracy: 0.6757\n",
      "Epoch 59/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.7477 - accuracy: 0.6842 - val_loss: 0.7854 - val_accuracy: 0.6757\n",
      "Epoch 00059: early stopping\n",
      "[[6.9681877e-01 5.7616527e-04 2.1419746e-03 ... 8.1431447e-04\n",
      "  9.5220439e-02 8.4906686e-03]\n",
      " [3.5088414e-01 1.1168759e-03 1.9995514e-03 ... 8.7429606e-04\n",
      "  6.3108313e-01 9.2615577e-04]\n",
      " [7.5614345e-01 4.2944893e-04 9.6540205e-04 ... 1.2006881e-03\n",
      "  1.7522890e-02 1.2065254e-02]\n",
      " ...\n",
      " [5.7405275e-03 4.6158161e-02 8.6796266e-01 ... 1.1217583e-04\n",
      "  1.9495534e-02 2.9597306e-05]\n",
      " [4.8605469e-01 1.0524900e-02 2.5333044e-01 ... 3.6508444e-04\n",
      "  1.9568279e-01 9.3878909e-05]\n",
      " [5.1806611e-01 9.8799784e-03 2.0523328e-01 ... 3.9798874e-04\n",
      "  2.0901428e-01 9.9324832e-05]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:02:54.799198\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35991 samples, validate on 3999 samples\n",
      "Epoch 1/80\n",
      "35991/35991 [==============================] - 2s 48us/step - loss: 1.3315 - accuracy: 0.4865 - val_loss: 1.0141 - val_accuracy: 0.6109\n",
      "Epoch 2/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.9472 - accuracy: 0.6317 - val_loss: 0.9131 - val_accuracy: 0.6462\n",
      "Epoch 3/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.8916 - accuracy: 0.6414 - val_loss: 0.9129 - val_accuracy: 0.6347\n",
      "Epoch 4/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.8652 - accuracy: 0.6494 - val_loss: 0.8607 - val_accuracy: 0.6612\n",
      "Epoch 5/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.8473 - accuracy: 0.6527 - val_loss: 0.8450 - val_accuracy: 0.6637\n",
      "Epoch 6/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8356 - accuracy: 0.6566 - val_loss: 0.8425 - val_accuracy: 0.6599\n",
      "Epoch 7/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.8291 - accuracy: 0.6565 - val_loss: 0.8320 - val_accuracy: 0.6727\n",
      "Epoch 8/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.8212 - accuracy: 0.6603 - val_loss: 0.8303 - val_accuracy: 0.6472\n",
      "Epoch 9/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8160 - accuracy: 0.6608 - val_loss: 0.8129 - val_accuracy: 0.6722\n",
      "Epoch 10/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.8126 - accuracy: 0.6604 - val_loss: 0.8315 - val_accuracy: 0.6717\n",
      "Epoch 11/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.8096 - accuracy: 0.6625 - val_loss: 0.8228 - val_accuracy: 0.6667\n",
      "Epoch 12/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.8071 - accuracy: 0.6636 - val_loss: 0.8105 - val_accuracy: 0.6662\n",
      "Epoch 13/80\n",
      "35991/35991 [==============================] - 1s 32us/step - loss: 0.8037 - accuracy: 0.6616 - val_loss: 0.8261 - val_accuracy: 0.6659\n",
      "Epoch 14/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.8010 - accuracy: 0.6637 - val_loss: 0.8021 - val_accuracy: 0.6642\n",
      "Epoch 15/80\n",
      "35991/35991 [==============================] - 1s 32us/step - loss: 0.8005 - accuracy: 0.6639 - val_loss: 0.8055 - val_accuracy: 0.6687\n",
      "Epoch 16/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7979 - accuracy: 0.6640 - val_loss: 0.7953 - val_accuracy: 0.6744\n",
      "Epoch 17/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7971 - accuracy: 0.6657 - val_loss: 0.8229 - val_accuracy: 0.6639\n",
      "Epoch 18/80\n",
      "35991/35991 [==============================] - 1s 32us/step - loss: 0.7927 - accuracy: 0.6673 - val_loss: 0.8040 - val_accuracy: 0.6684\n",
      "Epoch 19/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7920 - accuracy: 0.6659 - val_loss: 0.7999 - val_accuracy: 0.6804\n",
      "Epoch 20/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7916 - accuracy: 0.6664 - val_loss: 0.8155 - val_accuracy: 0.6612\n",
      "Epoch 21/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7905 - accuracy: 0.6664 - val_loss: 0.8005 - val_accuracy: 0.6744\n",
      "Epoch 22/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7887 - accuracy: 0.6667 - val_loss: 0.7946 - val_accuracy: 0.6737\n",
      "Epoch 23/80\n",
      "35991/35991 [==============================] - 1s 29us/step - loss: 0.7868 - accuracy: 0.6678 - val_loss: 0.7990 - val_accuracy: 0.6632\n",
      "Epoch 24/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7845 - accuracy: 0.6681 - val_loss: 0.7856 - val_accuracy: 0.6764\n",
      "Epoch 25/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7846 - accuracy: 0.6682 - val_loss: 0.7912 - val_accuracy: 0.6727\n",
      "Epoch 26/80\n",
      "35991/35991 [==============================] - 1s 29us/step - loss: 0.7820 - accuracy: 0.6688 - val_loss: 0.7860 - val_accuracy: 0.6697\n",
      "Epoch 27/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7820 - accuracy: 0.6692 - val_loss: 0.7904 - val_accuracy: 0.6752\n",
      "Epoch 28/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7829 - accuracy: 0.6701 - val_loss: 0.7835 - val_accuracy: 0.6789\n",
      "Epoch 29/80\n",
      "35991/35991 [==============================] - 1s 29us/step - loss: 0.7803 - accuracy: 0.6689 - val_loss: 0.7913 - val_accuracy: 0.6684\n",
      "Epoch 30/80\n",
      "35991/35991 [==============================] - 1s 30us/step - loss: 0.7832 - accuracy: 0.6684 - val_loss: 0.7815 - val_accuracy: 0.6729\n",
      "Epoch 31/80\n",
      "35991/35991 [==============================] - 1s 29us/step - loss: 0.7769 - accuracy: 0.6712 - val_loss: 0.7766 - val_accuracy: 0.6819\n",
      "Epoch 32/80\n",
      "35991/35991 [==============================] - 1s 31us/step - loss: 0.7770 - accuracy: 0.6717 - val_loss: 0.7792 - val_accuracy: 0.6764\n",
      "Epoch 33/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7754 - accuracy: 0.6713 - val_loss: 0.7912 - val_accuracy: 0.6739\n",
      "Epoch 34/80\n",
      "35991/35991 [==============================] - 2s 48us/step - loss: 0.7743 - accuracy: 0.6726 - val_loss: 0.7897 - val_accuracy: 0.6649\n",
      "Epoch 35/80\n",
      "35991/35991 [==============================] - 2s 46us/step - loss: 0.7760 - accuracy: 0.6704 - val_loss: 0.7769 - val_accuracy: 0.6767\n",
      "Epoch 36/80\n",
      "35991/35991 [==============================] - 2s 51us/step - loss: 0.7761 - accuracy: 0.6706 - val_loss: 0.7756 - val_accuracy: 0.6819\n",
      "Epoch 37/80\n",
      "35991/35991 [==============================] - 2s 49us/step - loss: 0.7739 - accuracy: 0.6695 - val_loss: 0.7839 - val_accuracy: 0.6722\n",
      "Epoch 38/80\n",
      "35991/35991 [==============================] - 2s 46us/step - loss: 0.7737 - accuracy: 0.6714 - val_loss: 0.7718 - val_accuracy: 0.6807\n",
      "Epoch 39/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7715 - accuracy: 0.6709 - val_loss: 0.7790 - val_accuracy: 0.6767\n",
      "Epoch 40/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7722 - accuracy: 0.6701 - val_loss: 0.7773 - val_accuracy: 0.6762\n",
      "Epoch 41/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7704 - accuracy: 0.6725 - val_loss: 0.7714 - val_accuracy: 0.6772\n",
      "Epoch 42/80\n",
      "35991/35991 [==============================] - 2s 47us/step - loss: 0.7701 - accuracy: 0.6723 - val_loss: 0.7729 - val_accuracy: 0.6774\n",
      "Epoch 43/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7693 - accuracy: 0.6726 - val_loss: 0.7766 - val_accuracy: 0.6759\n",
      "Epoch 44/80\n",
      "35991/35991 [==============================] - 2s 47us/step - loss: 0.7696 - accuracy: 0.6728 - val_loss: 0.7765 - val_accuracy: 0.6772\n",
      "Epoch 45/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7694 - accuracy: 0.6744 - val_loss: 0.7749 - val_accuracy: 0.6794\n",
      "Epoch 46/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7662 - accuracy: 0.6739 - val_loss: 0.7738 - val_accuracy: 0.6757\n",
      "Epoch 47/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7679 - accuracy: 0.6738 - val_loss: 0.7692 - val_accuracy: 0.6792\n",
      "Epoch 48/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7656 - accuracy: 0.6751 - val_loss: 0.7784 - val_accuracy: 0.6754\n",
      "Epoch 49/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7661 - accuracy: 0.6744 - val_loss: 0.7681 - val_accuracy: 0.6829\n",
      "Epoch 50/80\n",
      "35991/35991 [==============================] - 2s 50us/step - loss: 0.7658 - accuracy: 0.6734 - val_loss: 0.7696 - val_accuracy: 0.6812\n",
      "Epoch 51/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7658 - accuracy: 0.6745 - val_loss: 0.7759 - val_accuracy: 0.6834\n",
      "Epoch 52/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7639 - accuracy: 0.6758 - val_loss: 0.7747 - val_accuracy: 0.6772\n",
      "Epoch 53/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7642 - accuracy: 0.6740 - val_loss: 0.7639 - val_accuracy: 0.6807\n",
      "Epoch 54/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7630 - accuracy: 0.6741 - val_loss: 0.7673 - val_accuracy: 0.6807\n",
      "Epoch 55/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7625 - accuracy: 0.6739 - val_loss: 0.7674 - val_accuracy: 0.6832\n",
      "Epoch 56/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.7627 - accuracy: 0.6748 - val_loss: 0.7654 - val_accuracy: 0.6874\n",
      "Epoch 57/80\n",
      "35991/35991 [==============================] - 2s 46us/step - loss: 0.7631 - accuracy: 0.6747 - val_loss: 0.7833 - val_accuracy: 0.6709\n",
      "Epoch 58/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7599 - accuracy: 0.6781 - val_loss: 0.7669 - val_accuracy: 0.6832\n",
      "Epoch 59/80\n",
      "35991/35991 [==============================] - 1s 39us/step - loss: 0.7610 - accuracy: 0.6746 - val_loss: 0.7632 - val_accuracy: 0.6807\n",
      "Epoch 60/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7621 - accuracy: 0.6760 - val_loss: 0.7714 - val_accuracy: 0.6819\n",
      "Epoch 61/80\n",
      "35991/35991 [==============================] - 2s 46us/step - loss: 0.7597 - accuracy: 0.6750 - val_loss: 0.7650 - val_accuracy: 0.6817\n",
      "Epoch 62/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7589 - accuracy: 0.6769 - val_loss: 0.7929 - val_accuracy: 0.6702\n",
      "Epoch 63/80\n",
      "35991/35991 [==============================] - 1s 33us/step - loss: 0.7592 - accuracy: 0.6761 - val_loss: 0.7688 - val_accuracy: 0.6739\n",
      "Epoch 64/80\n",
      "35991/35991 [==============================] - 1s 33us/step - loss: 0.7590 - accuracy: 0.6779 - val_loss: 0.7670 - val_accuracy: 0.6852\n",
      "Epoch 65/80\n",
      "35991/35991 [==============================] - 1s 37us/step - loss: 0.7589 - accuracy: 0.6766 - val_loss: 0.7642 - val_accuracy: 0.6844\n",
      "Epoch 66/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7583 - accuracy: 0.6773 - val_loss: 0.7634 - val_accuracy: 0.6834\n",
      "Epoch 67/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7589 - accuracy: 0.6775 - val_loss: 0.7641 - val_accuracy: 0.6832\n",
      "Epoch 68/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7567 - accuracy: 0.6774 - val_loss: 0.7694 - val_accuracy: 0.6889\n",
      "Epoch 69/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7581 - accuracy: 0.6761 - val_loss: 0.7691 - val_accuracy: 0.6767\n",
      "Epoch 00069: early stopping\n",
      "[[7.1815109e-01 1.1586429e-04 9.8904059e-04 ... 3.2661834e-03\n",
      "  2.1017354e-02 1.4462897e-02]\n",
      " [3.3851641e-01 5.9325836e-04 2.8742217e-03 ... 2.9738038e-04\n",
      "  6.2493181e-01 2.9485754e-03]\n",
      " [6.3180184e-01 1.1697198e-04 8.5088424e-04 ... 3.9099636e-03\n",
      "  1.1224386e-02 9.8729581e-03]\n",
      " ...\n",
      " [6.0265623e-03 2.1034367e-02 8.8614649e-01 ... 6.4410537e-04\n",
      "  4.9048577e-02 6.9548187e-05]\n",
      " [4.5675713e-01 3.2073862e-03 2.1629357e-01 ... 2.2623691e-04\n",
      "  2.4534115e-01 1.5593844e-04]\n",
      " [4.8044553e-01 2.9666931e-03 1.8941265e-01 ... 2.1691380e-04\n",
      "  2.4196135e-01 1.7447393e-04]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:04:38.862522\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35991 samples, validate on 3999 samples\n",
      "Epoch 1/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 1.3144 - accuracy: 0.4922 - val_loss: 0.9691 - val_accuracy: 0.6424\n",
      "Epoch 2/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.9235 - accuracy: 0.6356 - val_loss: 0.8895 - val_accuracy: 0.6519\n",
      "Epoch 3/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.8773 - accuracy: 0.6426 - val_loss: 0.8669 - val_accuracy: 0.6574\n",
      "Epoch 4/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8573 - accuracy: 0.6487 - val_loss: 0.8532 - val_accuracy: 0.6584\n",
      "Epoch 5/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8443 - accuracy: 0.6517 - val_loss: 0.8468 - val_accuracy: 0.6624\n",
      "Epoch 6/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8368 - accuracy: 0.6527 - val_loss: 0.8282 - val_accuracy: 0.6632\n",
      "Epoch 7/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8262 - accuracy: 0.6560 - val_loss: 0.8329 - val_accuracy: 0.6649\n",
      "Epoch 8/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8204 - accuracy: 0.6581 - val_loss: 0.8329 - val_accuracy: 0.6627\n",
      "Epoch 9/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8183 - accuracy: 0.6579 - val_loss: 0.8152 - val_accuracy: 0.6612\n",
      "Epoch 10/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8133 - accuracy: 0.6614 - val_loss: 0.8191 - val_accuracy: 0.6669\n",
      "Epoch 11/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8077 - accuracy: 0.6614 - val_loss: 0.8011 - val_accuracy: 0.6739\n",
      "Epoch 12/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8045 - accuracy: 0.6628 - val_loss: 0.8044 - val_accuracy: 0.6692\n",
      "Epoch 13/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.8021 - accuracy: 0.6623 - val_loss: 0.7927 - val_accuracy: 0.6664\n",
      "Epoch 14/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7965 - accuracy: 0.6667 - val_loss: 0.7931 - val_accuracy: 0.6744\n",
      "Epoch 15/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7967 - accuracy: 0.6648 - val_loss: 0.8011 - val_accuracy: 0.6767\n",
      "Epoch 16/80\n",
      "35991/35991 [==============================] - ETA: 0s - loss: 0.7884 - accuracy: 0.67 - 2s 42us/step - loss: 0.7884 - accuracy: 0.6708 - val_loss: 0.7886 - val_accuracy: 0.6782\n",
      "Epoch 17/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7865 - accuracy: 0.6689 - val_loss: 0.7860 - val_accuracy: 0.6762\n",
      "Epoch 18/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7845 - accuracy: 0.6731 - val_loss: 0.7926 - val_accuracy: 0.6794\n",
      "Epoch 19/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7827 - accuracy: 0.6689 - val_loss: 0.7870 - val_accuracy: 0.6829\n",
      "Epoch 20/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7793 - accuracy: 0.6726 - val_loss: 0.7768 - val_accuracy: 0.6817\n",
      "Epoch 21/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7772 - accuracy: 0.6739 - val_loss: 0.7864 - val_accuracy: 0.6757\n",
      "Epoch 22/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7769 - accuracy: 0.6746 - val_loss: 0.7880 - val_accuracy: 0.6799\n",
      "Epoch 23/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7730 - accuracy: 0.6759 - val_loss: 0.7769 - val_accuracy: 0.6852\n",
      "Epoch 24/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7714 - accuracy: 0.6765 - val_loss: 0.7757 - val_accuracy: 0.6827\n",
      "Epoch 25/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7680 - accuracy: 0.6795 - val_loss: 0.7746 - val_accuracy: 0.6827\n",
      "Epoch 26/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7709 - accuracy: 0.6789 - val_loss: 0.7733 - val_accuracy: 0.6832\n",
      "Epoch 27/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7681 - accuracy: 0.6789 - val_loss: 0.7759 - val_accuracy: 0.6827\n",
      "Epoch 28/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7658 - accuracy: 0.6799 - val_loss: 0.7698 - val_accuracy: 0.6807\n",
      "Epoch 29/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7658 - accuracy: 0.6786 - val_loss: 0.7785 - val_accuracy: 0.6819\n",
      "Epoch 30/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7633 - accuracy: 0.6801 - val_loss: 0.7940 - val_accuracy: 0.6837\n",
      "Epoch 31/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7669 - accuracy: 0.6764 - val_loss: 0.7678 - val_accuracy: 0.6877\n",
      "Epoch 32/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7630 - accuracy: 0.6789 - val_loss: 0.7761 - val_accuracy: 0.6767\n",
      "Epoch 33/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7607 - accuracy: 0.6822 - val_loss: 0.7672 - val_accuracy: 0.6864\n",
      "Epoch 34/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7604 - accuracy: 0.6806 - val_loss: 0.7697 - val_accuracy: 0.6797\n",
      "Epoch 35/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7592 - accuracy: 0.6799 - val_loss: 0.7589 - val_accuracy: 0.6844\n",
      "Epoch 36/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7595 - accuracy: 0.6811 - val_loss: 0.7726 - val_accuracy: 0.6877\n",
      "Epoch 37/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7547 - accuracy: 0.6814 - val_loss: 0.7564 - val_accuracy: 0.6914\n",
      "Epoch 38/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7559 - accuracy: 0.6824 - val_loss: 0.7922 - val_accuracy: 0.6769\n",
      "Epoch 39/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7543 - accuracy: 0.6814 - val_loss: 0.7696 - val_accuracy: 0.6829\n",
      "Epoch 40/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7547 - accuracy: 0.6815 - val_loss: 0.7624 - val_accuracy: 0.6844\n",
      "Epoch 41/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7529 - accuracy: 0.6822 - val_loss: 0.7566 - val_accuracy: 0.6897\n",
      "Epoch 42/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7515 - accuracy: 0.6817 - val_loss: 0.7527 - val_accuracy: 0.6934\n",
      "Epoch 43/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7528 - accuracy: 0.6827 - val_loss: 0.7511 - val_accuracy: 0.6912\n",
      "Epoch 44/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7489 - accuracy: 0.6828 - val_loss: 0.7526 - val_accuracy: 0.6922\n",
      "Epoch 45/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7478 - accuracy: 0.6860 - val_loss: 0.7561 - val_accuracy: 0.6909\n",
      "Epoch 46/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7467 - accuracy: 0.6850 - val_loss: 0.7562 - val_accuracy: 0.6912\n",
      "Epoch 47/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7420 - accuracy: 0.6866 - val_loss: 0.7678 - val_accuracy: 0.6892\n",
      "Epoch 48/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7439 - accuracy: 0.6867 - val_loss: 0.7519 - val_accuracy: 0.6957\n",
      "Epoch 49/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7416 - accuracy: 0.6848 - val_loss: 0.7504 - val_accuracy: 0.6934\n",
      "Epoch 50/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7396 - accuracy: 0.6864 - val_loss: 0.7533 - val_accuracy: 0.6907\n",
      "Epoch 51/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7388 - accuracy: 0.6858 - val_loss: 0.7473 - val_accuracy: 0.6919\n",
      "Epoch 52/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7388 - accuracy: 0.6863 - val_loss: 0.7456 - val_accuracy: 0.6894\n",
      "Epoch 53/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7399 - accuracy: 0.6871 - val_loss: 0.7510 - val_accuracy: 0.6889\n",
      "Epoch 54/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7364 - accuracy: 0.6879 - val_loss: 0.7424 - val_accuracy: 0.6992\n",
      "Epoch 55/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7367 - accuracy: 0.6899 - val_loss: 0.7451 - val_accuracy: 0.6919\n",
      "Epoch 56/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7363 - accuracy: 0.6869 - val_loss: 0.7471 - val_accuracy: 0.6912\n",
      "Epoch 57/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7353 - accuracy: 0.6864 - val_loss: 0.7468 - val_accuracy: 0.6937\n",
      "Epoch 58/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7327 - accuracy: 0.6880 - val_loss: 0.7522 - val_accuracy: 0.6862\n",
      "Epoch 59/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7322 - accuracy: 0.6889 - val_loss: 0.7547 - val_accuracy: 0.6932\n",
      "Epoch 60/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7313 - accuracy: 0.6886 - val_loss: 0.7455 - val_accuracy: 0.6967\n",
      "Epoch 61/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7327 - accuracy: 0.6908 - val_loss: 0.7482 - val_accuracy: 0.6962\n",
      "Epoch 62/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7306 - accuracy: 0.6879 - val_loss: 0.7466 - val_accuracy: 0.6937\n",
      "Epoch 63/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7303 - accuracy: 0.6866 - val_loss: 0.7531 - val_accuracy: 0.6917\n",
      "Epoch 64/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7299 - accuracy: 0.6888 - val_loss: 0.7424 - val_accuracy: 0.6969\n",
      "Epoch 65/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7338 - accuracy: 0.6902 - val_loss: 0.7444 - val_accuracy: 0.6942\n",
      "Epoch 66/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7298 - accuracy: 0.6890 - val_loss: 0.7427 - val_accuracy: 0.6882\n",
      "Epoch 67/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7297 - accuracy: 0.6903 - val_loss: 0.7338 - val_accuracy: 0.6977\n",
      "Epoch 68/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7296 - accuracy: 0.6915 - val_loss: 0.7619 - val_accuracy: 0.6822\n",
      "Epoch 69/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7285 - accuracy: 0.6894 - val_loss: 0.7354 - val_accuracy: 0.6959\n",
      "Epoch 70/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7283 - accuracy: 0.6909 - val_loss: 0.7426 - val_accuracy: 0.6974\n",
      "Epoch 71/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7281 - accuracy: 0.6898 - val_loss: 0.7549 - val_accuracy: 0.6864\n",
      "Epoch 72/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7266 - accuracy: 0.6935 - val_loss: 0.7411 - val_accuracy: 0.6974\n",
      "Epoch 73/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7254 - accuracy: 0.6897 - val_loss: 0.7404 - val_accuracy: 0.6954\n",
      "Epoch 74/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7258 - accuracy: 0.6914 - val_loss: 0.7354 - val_accuracy: 0.6964\n",
      "Epoch 75/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7256 - accuracy: 0.6922 - val_loss: 0.7373 - val_accuracy: 0.6932\n",
      "Epoch 76/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.7242 - accuracy: 0.6915 - val_loss: 0.7339 - val_accuracy: 0.6904\n",
      "Epoch 77/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7239 - accuracy: 0.6940 - val_loss: 0.7460 - val_accuracy: 0.6954\n",
      "Epoch 00077: early stopping\n",
      "[[5.5025005e-01 8.9175483e-05 3.4218056e-03 ... 2.2864582e-03\n",
      "  6.9028407e-02 4.0661328e-02]\n",
      " [4.1334307e-01 7.8661181e-04 1.2996181e-02 ... 7.3833636e-04\n",
      "  5.2621567e-01 1.6796422e-03]\n",
      " [6.7210937e-01 5.1738585e-05 1.0079006e-03 ... 3.3661441e-03\n",
      "  7.0287753e-03 1.2069389e-02]\n",
      " ...\n",
      " [5.0835819e-03 4.9470034e-03 9.5503521e-01 ... 5.8276049e-04\n",
      "  2.4814961e-02 7.3635165e-05]\n",
      " [4.5368350e-01 1.2014378e-03 2.7296975e-01 ... 7.2634709e-04\n",
      "  1.6238716e-01 2.6566733e-04]\n",
      " [4.9805412e-01 1.2168932e-03 2.1456049e-01 ... 6.7512685e-04\n",
      "  1.7116036e-01 2.6484128e-04]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:06:45.319670\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 13,178\n",
      "Trainable params: 13,178\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 35991 samples, validate on 3999 samples\n",
      "Epoch 1/80\n",
      "35991/35991 [==============================] - 2s 50us/step - loss: 1.3193 - accuracy: 0.5014 - val_loss: 0.9788 - val_accuracy: 0.6232\n",
      "Epoch 2/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.9242 - accuracy: 0.6399 - val_loss: 0.8932 - val_accuracy: 0.6299\n",
      "Epoch 3/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.8795 - accuracy: 0.6441 - val_loss: 0.8576 - val_accuracy: 0.6522\n",
      "Epoch 4/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.8563 - accuracy: 0.6502 - val_loss: 0.8447 - val_accuracy: 0.6509\n",
      "Epoch 5/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8439 - accuracy: 0.6524 - val_loss: 0.8325 - val_accuracy: 0.6594\n",
      "Epoch 6/80\n",
      "35991/35991 [==============================] - 2s 45us/step - loss: 0.8333 - accuracy: 0.6560 - val_loss: 0.8400 - val_accuracy: 0.6507\n",
      "Epoch 7/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8295 - accuracy: 0.6576 - val_loss: 0.8139 - val_accuracy: 0.6592\n",
      "Epoch 8/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8247 - accuracy: 0.6594 - val_loss: 0.8153 - val_accuracy: 0.6534\n",
      "Epoch 9/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8151 - accuracy: 0.6609 - val_loss: 0.8092 - val_accuracy: 0.6579\n",
      "Epoch 10/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.8118 - accuracy: 0.6592 - val_loss: 0.8072 - val_accuracy: 0.6609\n",
      "Epoch 11/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8087 - accuracy: 0.6643 - val_loss: 0.8055 - val_accuracy: 0.6619\n",
      "Epoch 12/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.8044 - accuracy: 0.6655 - val_loss: 0.8010 - val_accuracy: 0.6634\n",
      "Epoch 13/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.8040 - accuracy: 0.6639 - val_loss: 0.8085 - val_accuracy: 0.6584\n",
      "Epoch 14/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7987 - accuracy: 0.6659 - val_loss: 0.7967 - val_accuracy: 0.6657\n",
      "Epoch 15/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.8001 - accuracy: 0.6653 - val_loss: 0.8056 - val_accuracy: 0.6622\n",
      "Epoch 16/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7951 - accuracy: 0.6666 - val_loss: 0.7951 - val_accuracy: 0.6627\n",
      "Epoch 17/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7907 - accuracy: 0.6694 - val_loss: 0.7891 - val_accuracy: 0.6684\n",
      "Epoch 18/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7894 - accuracy: 0.6700 - val_loss: 0.7923 - val_accuracy: 0.6747\n",
      "Epoch 19/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7866 - accuracy: 0.6715 - val_loss: 0.7829 - val_accuracy: 0.6664\n",
      "Epoch 20/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7839 - accuracy: 0.6720 - val_loss: 0.7848 - val_accuracy: 0.6614\n",
      "Epoch 21/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7822 - accuracy: 0.6719 - val_loss: 0.7811 - val_accuracy: 0.6687\n",
      "Epoch 22/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7815 - accuracy: 0.6733 - val_loss: 0.7785 - val_accuracy: 0.6624\n",
      "Epoch 23/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7791 - accuracy: 0.6732 - val_loss: 0.7735 - val_accuracy: 0.6729\n",
      "Epoch 24/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7772 - accuracy: 0.6747 - val_loss: 0.7828 - val_accuracy: 0.6707\n",
      "Epoch 25/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7763 - accuracy: 0.6754 - val_loss: 0.7777 - val_accuracy: 0.6707\n",
      "Epoch 26/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7718 - accuracy: 0.6777 - val_loss: 0.7696 - val_accuracy: 0.6717\n",
      "Epoch 27/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7715 - accuracy: 0.6768 - val_loss: 0.7723 - val_accuracy: 0.6749\n",
      "Epoch 28/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7715 - accuracy: 0.6748 - val_loss: 0.7730 - val_accuracy: 0.6737\n",
      "Epoch 29/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7688 - accuracy: 0.6764 - val_loss: 0.7646 - val_accuracy: 0.6754\n",
      "Epoch 30/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7683 - accuracy: 0.6779 - val_loss: 0.7707 - val_accuracy: 0.6677\n",
      "Epoch 31/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7642 - accuracy: 0.6781 - val_loss: 0.7724 - val_accuracy: 0.6694\n",
      "Epoch 32/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7649 - accuracy: 0.6773 - val_loss: 0.7592 - val_accuracy: 0.6774\n",
      "Epoch 33/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7648 - accuracy: 0.6791 - val_loss: 0.7570 - val_accuracy: 0.6812\n",
      "Epoch 34/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7625 - accuracy: 0.6797 - val_loss: 0.7671 - val_accuracy: 0.6699\n",
      "Epoch 35/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7622 - accuracy: 0.6801 - val_loss: 0.7627 - val_accuracy: 0.6772\n",
      "Epoch 36/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7617 - accuracy: 0.6803 - val_loss: 0.7616 - val_accuracy: 0.6807\n",
      "Epoch 37/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7594 - accuracy: 0.6806 - val_loss: 0.7649 - val_accuracy: 0.6759\n",
      "Epoch 38/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7592 - accuracy: 0.6798 - val_loss: 0.7598 - val_accuracy: 0.6799\n",
      "Epoch 39/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7572 - accuracy: 0.6824 - val_loss: 0.7646 - val_accuracy: 0.6749\n",
      "Epoch 40/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7579 - accuracy: 0.6794 - val_loss: 0.7605 - val_accuracy: 0.6674\n",
      "Epoch 41/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7570 - accuracy: 0.6816 - val_loss: 0.7594 - val_accuracy: 0.6769\n",
      "Epoch 42/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7567 - accuracy: 0.6806 - val_loss: 0.7567 - val_accuracy: 0.6782\n",
      "Epoch 43/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7549 - accuracy: 0.6838 - val_loss: 0.7698 - val_accuracy: 0.6627\n",
      "Epoch 44/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7539 - accuracy: 0.6834 - val_loss: 0.7599 - val_accuracy: 0.6717\n",
      "Epoch 45/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7533 - accuracy: 0.6831 - val_loss: 0.7624 - val_accuracy: 0.6792\n",
      "Epoch 46/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7523 - accuracy: 0.6841 - val_loss: 0.7545 - val_accuracy: 0.6777\n",
      "Epoch 47/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7519 - accuracy: 0.6834 - val_loss: 0.7553 - val_accuracy: 0.6792\n",
      "Epoch 48/80\n",
      "35991/35991 [==============================] - 1s 40us/step - loss: 0.7520 - accuracy: 0.6846 - val_loss: 0.7517 - val_accuracy: 0.6754\n",
      "Epoch 49/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7496 - accuracy: 0.6839 - val_loss: 0.7450 - val_accuracy: 0.6774\n",
      "Epoch 50/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7493 - accuracy: 0.6848 - val_loss: 0.7516 - val_accuracy: 0.6789\n",
      "Epoch 51/80\n",
      "35991/35991 [==============================] - ETA: 0s - loss: 0.7481 - accuracy: 0.68 - 2s 42us/step - loss: 0.7483 - accuracy: 0.6837 - val_loss: 0.7660 - val_accuracy: 0.6757\n",
      "Epoch 52/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7472 - accuracy: 0.6857 - val_loss: 0.7472 - val_accuracy: 0.6762\n",
      "Epoch 53/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7458 - accuracy: 0.6829 - val_loss: 0.7465 - val_accuracy: 0.6822\n",
      "Epoch 54/80\n",
      "35991/35991 [==============================] - ETA: 0s - loss: 0.7446 - accuracy: 0.68 - 2s 42us/step - loss: 0.7445 - accuracy: 0.6868 - val_loss: 0.7396 - val_accuracy: 0.6814\n",
      "Epoch 55/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7433 - accuracy: 0.6873 - val_loss: 0.7396 - val_accuracy: 0.6839\n",
      "Epoch 56/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7423 - accuracy: 0.6865 - val_loss: 0.7476 - val_accuracy: 0.6822\n",
      "Epoch 57/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7404 - accuracy: 0.6884 - val_loss: 0.7463 - val_accuracy: 0.6804\n",
      "Epoch 58/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7384 - accuracy: 0.6879 - val_loss: 0.7426 - val_accuracy: 0.6834\n",
      "Epoch 59/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7379 - accuracy: 0.6885 - val_loss: 0.7458 - val_accuracy: 0.6854\n",
      "Epoch 60/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7367 - accuracy: 0.6886 - val_loss: 0.7466 - val_accuracy: 0.6799\n",
      "Epoch 61/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7348 - accuracy: 0.6895 - val_loss: 0.7379 - val_accuracy: 0.6794\n",
      "Epoch 62/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7358 - accuracy: 0.6899 - val_loss: 0.7322 - val_accuracy: 0.6847\n",
      "Epoch 63/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7314 - accuracy: 0.6909 - val_loss: 0.7311 - val_accuracy: 0.6809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7327 - accuracy: 0.6908 - val_loss: 0.7319 - val_accuracy: 0.6874\n",
      "Epoch 65/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7293 - accuracy: 0.6906 - val_loss: 0.7423 - val_accuracy: 0.6769\n",
      "Epoch 66/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7302 - accuracy: 0.6910 - val_loss: 0.7273 - val_accuracy: 0.6864\n",
      "Epoch 67/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7272 - accuracy: 0.6912 - val_loss: 0.7271 - val_accuracy: 0.6817\n",
      "Epoch 68/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7305 - accuracy: 0.6893 - val_loss: 0.7353 - val_accuracy: 0.6837\n",
      "Epoch 69/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7274 - accuracy: 0.6913 - val_loss: 0.7310 - val_accuracy: 0.6867\n",
      "Epoch 70/80\n",
      "35991/35991 [==============================] - 1s 41us/step - loss: 0.7277 - accuracy: 0.6909 - val_loss: 0.7264 - val_accuracy: 0.6884\n",
      "Epoch 71/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7270 - accuracy: 0.6921 - val_loss: 0.7345 - val_accuracy: 0.6857\n",
      "Epoch 72/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7266 - accuracy: 0.6922 - val_loss: 0.7345 - val_accuracy: 0.6764\n",
      "Epoch 73/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7276 - accuracy: 0.6893 - val_loss: 0.7312 - val_accuracy: 0.6894\n",
      "Epoch 74/80\n",
      "35991/35991 [==============================] - 2s 44us/step - loss: 0.7249 - accuracy: 0.6945 - val_loss: 0.7289 - val_accuracy: 0.6842\n",
      "Epoch 75/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7241 - accuracy: 0.6914 - val_loss: 0.7276 - val_accuracy: 0.6852\n",
      "Epoch 76/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7231 - accuracy: 0.6926 - val_loss: 0.7297 - val_accuracy: 0.6807\n",
      "Epoch 77/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7228 - accuracy: 0.6935 - val_loss: 0.7188 - val_accuracy: 0.6862\n",
      "Epoch 78/80\n",
      "35991/35991 [==============================] - 1s 42us/step - loss: 0.7227 - accuracy: 0.6926 - val_loss: 0.7304 - val_accuracy: 0.6809\n",
      "Epoch 79/80\n",
      "35991/35991 [==============================] - 2s 42us/step - loss: 0.7216 - accuracy: 0.6941 - val_loss: 0.7338 - val_accuracy: 0.6774\n",
      "Epoch 80/80\n",
      "35991/35991 [==============================] - 2s 43us/step - loss: 0.7212 - accuracy: 0.6947 - val_loss: 0.7380 - val_accuracy: 0.6752\n",
      "[[7.0563352e-01 8.3914923e-04 5.9561566e-03 ... 8.7020284e-04\n",
      "  7.7632897e-02 3.1867079e-02]\n",
      " [2.8663778e-01 7.6673878e-04 7.2438736e-03 ... 3.1713146e-04\n",
      "  6.8203783e-01 1.6064469e-03]\n",
      " [7.5065178e-01 2.5573795e-04 7.0627814e-04 ... 1.2852900e-03\n",
      "  2.5751263e-02 2.0362306e-02]\n",
      " ...\n",
      " [3.9588758e-03 3.0753186e-03 9.2488748e-01 ... 4.0950923e-05\n",
      "  5.6333333e-02 5.7293480e-05]\n",
      " [3.9014405e-01 1.1676303e-03 2.9054630e-01 ... 1.8051183e-05\n",
      "  2.2908866e-01 7.8060242e-05]\n",
      " [4.2432547e-01 1.3232618e-03 2.3306316e-01 ... 2.0981684e-05\n",
      "  2.4117747e-01 9.8021257e-05]]\n",
      "[0 8 0 ... 2 0 0]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:08:53.608612\n",
      "num of merged_region_image 0 4611\n",
      "num of merged_region_image 1 3215\n",
      "num of merged_region_image 2 4064\n",
      "num of merged_region_image 3 4586\n",
      "num of merged_region_image 4 3324\n",
      "num of merged_region_image 5 3276\n",
      "num of merged_region_image 6 4053\n",
      "num of merged_region_image 7 4241\n",
      "num of merged_region_image 8 4199\n",
      "num of merged_region_image 9 4421\n",
      "Counter({-1: 15010, 0: 4611, 3: 4586, 9: 4421, 7: 4241, 8: 4199, 2: 4064, 6: 4053, 4: 3324, 5: 3276, 1: 3215})\n",
      "===========  ITE = 1   ===========\n",
      "used_img 39990 39990\n",
      "working_img(=other images=unclean images) 15010 15010\n",
      "merged regions 149 149\n",
      "other_regions 51 51\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>136</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>139</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>142</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>143</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>149</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1</td>\n",
       "      <td>487</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>151</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>152</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>374</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>159</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>34</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>517</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>163</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>42</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>171</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>46</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>380</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>50</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>311</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>52</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>184</td>\n",
       "      <td>-1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "      <td>248</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>59</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>274</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>64</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>192</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>224</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>193</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.43</td>\n",
       "      <td>292</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>197</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>72</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>75</td>\n",
       "      <td>-1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>91</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>101</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>106</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>119</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>126</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>235</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    other index  pred label  truth  rate    0    1    2    3    4    5    6  \\\n",
       "0             1           9      3  0.69    0    0    0  244    0    2    0   \n",
       "1             3           7      3  0.37    0    0    0  202    0    0    0   \n",
       "2           132          -1      0  0.00  223    0    1    0    0    1    0   \n",
       "3             7           2      4  0.83    0    2    0    0  258    0    0   \n",
       "4           136          -1      9  0.00    0    0    0    1    3    0    0   \n",
       "5             9           6      8  0.43    0    0    0    2    0    0    0   \n",
       "6           139           6      8  0.83    0    0    0    2    0    0    0   \n",
       "7            12          -1      7  0.00    0    0    0    1    1    0    0   \n",
       "8           142           9      1  0.80    0  201    0    0    0    0    0   \n",
       "9            15           9      3  0.44    0    0    0  220    0    0    0   \n",
       "10          143           9      1  0.64    0  220    0    0    0    0    0   \n",
       "11           20          -1      6  0.00    1    0    0    0    1    0  211   \n",
       "12          149          -1      5  0.00    0    0    0    0    0  275    0   \n",
       "13           22           2      9  0.65    0   40    2    0   82    1    0   \n",
       "14          150           4      1  0.43    1  487    1    1    3   60    0   \n",
       "15           24          -1      0  0.00  218    0    1    0    0    0    1   \n",
       "16          151           2      9  0.66    0    0    0    1   23    1    0   \n",
       "17          152          -1      5  0.00    1    0    0    5    0  374    1   \n",
       "18          159          -1      9  0.00    0    0    0    1    3    0    0   \n",
       "19           32          -1      6  0.00    0    0    0    0    0    0  232   \n",
       "20           34          -1      2  0.00    0    0  517    7    0    0    0   \n",
       "21          163           9      1  0.59    0  214    0    0    0    0    0   \n",
       "22           37          -1      4  0.00    0    0    0    0  311    0    1   \n",
       "23           42          -1      4  0.00    0    0    0    0  219    0    0   \n",
       "24          171          -1      3  0.00    1    0    0  177    0   19    0   \n",
       "25          172           2      5  0.38    0    0    0    0    0  252    1   \n",
       "26           46          -1      2  0.00   15    0   49    5    7   17   16   \n",
       "27           47           9      2  0.52    0    3  380    5    0    0    0   \n",
       "28           50          -1      9  0.00    0    0    4    1    2    1    0   \n",
       "29           51           9      3  0.63    0    0    1  311    0    1    0   \n",
       "30           52          -1      9  0.00    0    0    0    1    1    2    0   \n",
       "31          184          -1      9  0.00    1    0    0    3    2    1    0   \n",
       "32           57           9      1  0.53    0  248    0    0    0    0    0   \n",
       "33           59          -1      4  0.00    0    0    0    0  151    0    0   \n",
       "34          190           2      6  0.50    1    0    0    1    0    7  274   \n",
       "35           64          -1      3  0.00    0    0    0  298    0    0    0   \n",
       "36          192          -1      2  0.00    0    3  224   10    0    0    1   \n",
       "37          193           4      3  0.37    0    0    0  197    0   13    0   \n",
       "38          196           0      0  0.43  292    0    4    0    0    0    0   \n",
       "39           69           2      6  0.58    0    0    0    0    0    0  334   \n",
       "40          197          -1      7  0.00    0    0    0    0    0    1    0   \n",
       "41           72          -1      1  0.00    0  219    0    0    0    0    0   \n",
       "42           75          -1      6  0.00    6    0    0    0    0    0  256   \n",
       "43           77           2      6  0.52    1    0    0    0    3    8  310   \n",
       "44           90           4      1  0.64    0  216    0    1    0    0    1   \n",
       "45           91          -1      8  0.00    1    0    0   23    0   28    5   \n",
       "46           92           1      4  0.49    0    0    0    0  270    5    2   \n",
       "47          101           2      6  0.60    0    0    0    0    0    1  269   \n",
       "48          106           9      1  0.72    0  238    0    0    0    0    0   \n",
       "49          119           3      7  0.61    0    0   18    2    0    0    0   \n",
       "50          126           2      4  0.72    0    7    0    0  235    0    0   \n",
       "\n",
       "      7    8    9  \n",
       "0     0    0    6  \n",
       "1     1    3    1  \n",
       "2     0    0    0  \n",
       "3     2    0    7  \n",
       "4     3    0  338  \n",
       "5     0  264    1  \n",
       "6     0  284    0  \n",
       "7   165    0    8  \n",
       "8     2    0    0  \n",
       "9     0    4    1  \n",
       "10    0    0    0  \n",
       "11    0    2    0  \n",
       "12    0    2    0  \n",
       "13   16    3  205  \n",
       "14  116    9    3  \n",
       "15    0    1    1  \n",
       "16    3    1  241  \n",
       "17    0    4    1  \n",
       "18    7    3  272  \n",
       "19    0    0    0  \n",
       "20    6    1    0  \n",
       "21    0    1    0  \n",
       "22    2    0    5  \n",
       "23    0    0    1  \n",
       "24    0    3    0  \n",
       "25    0   11    2  \n",
       "26   47   15   25  \n",
       "27    9    1    0  \n",
       "28    2    1  214  \n",
       "29    0    4    1  \n",
       "30    2    1  238  \n",
       "31   15    1  251  \n",
       "32    0    0    0  \n",
       "33    2    2    1  \n",
       "34    0    1    0  \n",
       "35    0   14   17  \n",
       "36    5    2    0  \n",
       "37    0    3    1  \n",
       "38    0    0    2  \n",
       "39    0    0    0  \n",
       "40  272    0    6  \n",
       "41    0    0    0  \n",
       "42    0    3    0  \n",
       "43    0    1    0  \n",
       "44    0    3    0  \n",
       "45    0  304   14  \n",
       "46    1    2    6  \n",
       "47    0    0    0  \n",
       "48    1    0    0  \n",
       "49  250    0    1  \n",
       "50    4    4    3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {2} [7, 126] 405\n",
      "added label, regions, img amount: {6} [139] 236\n",
      "added label, regions, img amount: {9} [142, 106] 333\n"
     ]
    }
   ],
   "source": [
    "for case_i in range(NUM_CASE):\n",
    "\n",
    "    #===== create folder case1, case2, case3...\n",
    "    print(\"case=\",case_i+1)\n",
    "    newpath = './case' + str(case_i+1)\n",
    "    if (not INTE_bool):\n",
    "        if not os.path.exists(newpath):   #No necessary in Integration\n",
    "            os.makedirs(newpath)\n",
    "    \n",
    "    #==== open csv 1\n",
    "    csv_path1 = newpath+'/' + 'accu_history.csv'\n",
    "    with open(csv_path1, 'a', newline='') as f:\n",
    "        csv_file = csv.writer(f)\n",
    "        csv_file.writerow(['ITE', 'correct', 'denominator', 'accu', 'description'])\n",
    "\n",
    "# 1.\n",
    "    if (not INTE_bool):\n",
    "        create_image_0(PATH6, case_i)   #No necessary in Integration\n",
    "\n",
    "\n",
    "    for ITE in range(ITE_START, ITE_END):\n",
    "# 2. CNN\n",
    "        CNN_part(PATH5,ITE)\n",
    "\n",
    "# 3. statistic\n",
    "        statistic(PATH5,ITE)\n",
    "\n",
    "# 4. merged_and_expand \n",
    "        merged_and_expand(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
