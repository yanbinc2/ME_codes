{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; Jun, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 5 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@ntu.edu.tw </p>\n",
    "### <p> Master Program in Statistics, CGE, National Taiwan University (NTU), Taipei, Taiwan.</p>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "from itertools import chain\n",
    "from scipy.spatial.distance import squareform, pdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "PATH4='../../phase3/data/ResNet18_PlantDisease_45K_Spec200_sampling.csv'\n",
    "PATH5='../../phase3/data/embedded_data.pickle'\n",
    "PATH6='../data/mergedseedclasslabels.txt'\n",
    "PATH7='../../phase3/data/region_for_phase5.pickle'\n",
    "\n",
    "\n",
    "# === parameters ===================================================\n",
    "MNIST     = False\n",
    "NUM_CASE  = 1\n",
    "INTE_bool = False  #True: Integrate two networks VGG+ResNet    False: single network\n",
    "SAVE_bool = True\n",
    "ITE_FROM  = 5 # This setting is ONLY for Integration\n",
    "REG_COLUMN = \"Spec200\"\n",
    "RAW_2D_DATA = False\n",
    "interpret_path='./case1/accu_history.csv'  #interprete the accuracy results\n",
    "AMOUNT_ITE=3\n",
    "\n",
    "\n",
    "if RAW_2D_DATA: # 2D\n",
    "    from CNN_Modules import ME_CNN\n",
    "else: # 1D\n",
    "    from CNN_Modules_1D import ME_CNN\n",
    "    \n",
    "\n",
    "if (INTE_bool):\n",
    "    ITE_START=ITE_FROM\n",
    "    ITE_END=ITE_FROM+4\n",
    "else:\n",
    "    ITE_START=0\n",
    "    ITE_END=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_csv(x, path):\n",
    "    with open(path,'a+', newline='') as f:\n",
    "        csv_file = csv.writer(f)#   = f.write()\n",
    "        csv_file.writerow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for single network. No necessary in Integrated networks\n",
    "if (not INTE_bool):\n",
    "    def create_image_0(PATH6, case_i):\n",
    "        # ===================\n",
    "        #\n",
    "        #  prepare  merged_region_image_0\n",
    "        #\n",
    "        #====================\n",
    "        # (A)\n",
    "        #get \"(1)merged_region\"      only seed regions, no neighboring regions\n",
    "        df = pandas.read_csv(PATH6, delim_whitespace=' ', header=0,  index_col=None)\n",
    "        table = df.to_numpy()\n",
    "        print(\"mergedseedclasslabels table\")\n",
    "        display(table)\n",
    "\n",
    "        merged_region=[]\n",
    "        for i in range(min(table.T[case_i+1]), max(table.T[case_i+1])+1):  #18 ---merge to --> 10\n",
    "            addr=np.where(table.T[case_i+1]==i)[0] # 2nd column equal to 0(min),1,2,3...10(max); DO NOT consider 3rd column, which is hidden\n",
    "            if(len(addr) and i>0): #if not empty and i=0 is the invalid seed region.\n",
    "                merged_region.append(table[addr][:,0].tolist())\n",
    "        print(\"merged_region\")\n",
    "        display(merged_region)\n",
    "\n",
    "\n",
    "        # (B)\n",
    "        #get \"merged_reg_and_nei\"\n",
    "        #get \"merged_reg_and_nei_image\"\n",
    "        #generate \"merged_region_image_0.pickle\"\n",
    "\n",
    "        # (B_a)=== without neighbors ====\n",
    "        #if ((DATASET == 2) or (DATASET == 4)): \n",
    "        ##20240105\n",
    "        if (not True): \n",
    "            # ==== collect regions. No neighbors, just use merged regions ====\n",
    "            merged_reg_and_nei=merged_region.copy()\n",
    "\n",
    "            # ==== collect images ====\n",
    "            img_temp=[]\n",
    "            for i in range(len(merged_region)):\n",
    "                addr=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    temp=np.where(all_region_index==merged_region[i][j])[0].tolist()   #tolist(): convert temp into list\n",
    "                    addr=addr+temp\n",
    "                    print(len(temp),end=' ')\n",
    "                img_temp.append(addr)\n",
    "                print(\"=\",len(img_temp[i]))\n",
    "            merged_reg_and_nei_image = img_temp.copy()\n",
    "\n",
    "\n",
    "        # (B_b)=== with neighbors ==== \n",
    "        else: \n",
    "            with open(PATH7, 'rb') as f:\n",
    "                pre_region, pre_reg_nei, pre_region_image_pure, pre_region_image= pickle.load(f)\n",
    "            #    1reg         2reg+nei        1's img            2's img\n",
    "\n",
    "            # ==== collect regions with neighbors====\n",
    "            # remove duplicate  -->  https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them\n",
    "            merged_reg_and_nei=[]\n",
    "            NUM_region=len(merged_region)\n",
    "            for i in range(NUM_region):\n",
    "                temp=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0] \n",
    "                    temp=temp+pre_reg_nei[idx]\n",
    "                    print(idx,pre_region[idx])\n",
    "                merged_reg_and_nei.append(temp)\n",
    "\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(merged_reg_and_nei[i]) != len(set(merged_reg_and_nei[i]))):\n",
    "                    a=merged_reg_and_nei[i].copy()\n",
    "\n",
    "                    # find the duplicate.\n",
    "                    seen = set()\n",
    "                    dupli                 = [x for x in a if (x in seen or seen.add(x))]\n",
    "                    print(\"***duplicates:\",dupli)\n",
    "\n",
    "                    # keep fisrt one, remove succeeding duplicates.\n",
    "                    seen = set()\n",
    "                    merged_reg_and_nei[i] = [x for x in a if not (x in seen or seen.add(x))]  # a is the data to process; x is a working varialbe\n",
    "                    print(\"unique:\",merged_reg_and_nei[i])\n",
    "\n",
    "                print(\"total\",len(merged_reg_and_nei[i]),end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei\")\n",
    "            for i in range(len(merged_reg_and_nei)):\n",
    "                print(merged_reg_and_nei[i])\n",
    "\n",
    "\n",
    "            # Collect images\n",
    "            merged_reg_and_nei_image=[]\n",
    "            for i in range(NUM_region):\n",
    "                #search and add\n",
    "                img=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0]\n",
    "                    print(len(pre_region_image[idx]),\"(\",idx,\")\",end=' ')\n",
    "                    img=img+pre_region_image[idx] \n",
    "                print(\"=\",len(img),end=\" \")\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(img) != len(set(img))):\n",
    "                    img=list(set(img)) #remove duplicates\n",
    "                    print(\"     **duplicate, shrink to\",len(img),end=\"\\n\")  \n",
    "                else:\n",
    "                    print(end=\"\\n\")\n",
    "\n",
    "                #append\n",
    "                merged_reg_and_nei_image.append(img)\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei_image\")\n",
    "            for i in range(len(merged_reg_and_nei_image)):\n",
    "                print(len(merged_reg_and_nei_image[i]),merged_reg_and_nei_image[i][:5],\"...\")\n",
    "\n",
    "        # save\n",
    "        if (SAVE_bool):\n",
    "            with open(newpath+'/merged_region_image_0.pickle', 'wb') as f:\n",
    "                pickle.dump([merged_reg_and_nei, merged_reg_and_nei_image], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_part(PATH5,ITE):\n",
    "    TRIALS          = 5\n",
    "\n",
    "    savelog_path = newpath+'/' + 'log.txt'\n",
    "\n",
    "    # ==== test_array ====\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "        \n",
    "    if RAW_2D_DATA: # 2D\n",
    "        print(\"\")\n",
    "    else: # 1D\n",
    "        test_array = np.expand_dims(test_array, axis = -1)\n",
    "\n",
    "    \n",
    "    #if((DATASET==2) or (DATASET==4)):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #elif(DATASET==1):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #    test_array /= 255\n",
    "    #elif(DATASET==0):\n",
    "    #    test_array /= 255\n",
    "    #display(np.shape(test_array))\n",
    "\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    region_image=merged_region_image.copy()\n",
    "    del merged_reg_and_nei\n",
    "\n",
    "\n",
    "    NUM_region=len(region_image)\n",
    "    print(\"NUM_region\",NUM_region)\n",
    "\n",
    "\n",
    "    from itertools import chain\n",
    "    region_image_flatten=list(chain.from_iterable(region_image))\n",
    "    print(\"number of clean images\",len(region_image_flatten))\n",
    "\n",
    "\n",
    "    ROUND_start = time.time()\n",
    "    #========  merge ==========\n",
    "    #prepare selected_region, region\n",
    "    for n in range(1): #extra_original\n",
    "    #   #reset\n",
    "        region=region_image.copy()\n",
    "        region=list(region)\n",
    "        selected_region = list(range(NUM_region))  #[0,1,2, ... ,29]\n",
    "\n",
    "        #merge\n",
    "        if (n > 4):\n",
    "            p1=comb[n-1][0]\n",
    "            p2=comb[n-1][1]\n",
    "            region[p1]=region[p1]+region[p2]\n",
    "            region.pop(p2)\n",
    "            selected_region.pop(-1)  # remove last region index\n",
    "        #original\n",
    "        else:  #n=0\n",
    "            p1=0\n",
    "            p2=0\n",
    "\n",
    "        print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "        # ===== one CNN =============\n",
    "        NUM_CLASSES = len(selected_region)  #NUM_CLASSES should be here to update for each loop\n",
    "        \n",
    "        # Clip the numeber of class. The \"test_label_answer\" is just for the verification. The changed \"test_label_answer\"\n",
    "        # doesn't affact the CNN predictions.\n",
    "        print(\"len(np.unique(test_label_answer1))\",len(np.unique(test_label_answer)), list(set(test_label_answer)))\n",
    "        if NUM_CLASSES < len(np.unique(test_label_answer)):\n",
    "            test_label_answer=np.clip(test_label_answer, 0, NUM_CLASSES-1)\n",
    "        print(\"len(np.unique(test_label_answer2))\",len(np.unique(test_label_answer)), list(set(test_label_answer)))\n",
    "\n",
    "        # input image and label\n",
    "        Input_img     = []\n",
    "        Input_img_len = []\n",
    "        for c,sel in enumerate(selected_region, start=0):\n",
    "            Input_img = Input_img + list(region[sel])\n",
    "            Input_img_len.append(len(region[sel])) #can only concatenate list (not \"int\") to list    \n",
    "            \n",
    "        # 20240319\n",
    "        if RAW_2D_DATA: # 2D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            H           = np.shape(test_array[0])[1]\n",
    "            train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "        else: # 1D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "                  \n",
    "        train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "        # fill up the training label to each training image\n",
    "        current_train_label = np.zeros(len(train_array), dtype=int)  # Assign 0 to the label\n",
    "        accum_base=0  #accumulate\n",
    "        for label in range(1,NUM_CLASSES):\n",
    "            sector = Input_img_len[label-1]\n",
    "            accum_base = accum_base + sector  # sector is the sector length\n",
    "            current_train_label[accum_base:] = label  # fill the label\n",
    "\n",
    "\n",
    "        # CNN\n",
    "        #===============================================\n",
    "        one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "        one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "        model_history = np.zeros(TRIALS, dtype=list)\n",
    "        print(\"NUM_CLASSES\",NUM_CLASSES)\n",
    "        print(\"current_train_label: \",list(set(current_train_label)))\n",
    "        print(\"test_label_answer: \",list(set(test_label_answer)))\n",
    "        for r in range(TRIALS):  #10\n",
    "            one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                    x_train     = train_array,\n",
    "                    train_label = current_train_label,\n",
    "                    test_array  = test_array,\n",
    "                    true_answer = test_label_answer,\n",
    "                    Num_Classes = NUM_CLASSES\n",
    "                    )\n",
    "            print(type(model_history))\n",
    "\n",
    "\n",
    "            # ===== delete CNN tensors =====\n",
    "            from keras import backend as K\n",
    "            K.clear_session()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"One CNN, r: \",r)\n",
    "            ROUND_duration = time.time() - ROUND_start\n",
    "            print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "\n",
    "        # === save to file ===\n",
    "        #This is useless in phase 5. Prepare for further checking in the future.\n",
    "        savefile_path = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'_'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path, 'wb') as f:\n",
    "            pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "        savefile_path2 = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_5_tests_simple_ITE'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path2, 'wb') as f:\n",
    "            pickle.dump([one_predicted_results, one_predict_percentage], f)\n",
    "\n",
    "        # === save to log ===    \n",
    "        savelog = open(savelog_path, 'a+')\n",
    "        print(\"\\n\", savefile_path, file = savelog)\n",
    "        print(\"Saved parameters: Input_img, Input_img_len, one_predicted_results, one_predict_percentage\", file = savelog) #0722\n",
    "\n",
    "        # total time\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Completion time: \", datetime.datetime.now(), file = savelog)\n",
    "        print(\"Total Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)), file = savelog)\n",
    "\n",
    "        savelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_method(PATH5,NUM_region,region_label,table_1D):\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "    \n",
    "    dist_table_truth=np.zeros((NUM_region, len(np.unique(test_label_answer))),dtype=int)   # [6 x 9] matrix. The size of \"NUM_region\" may be less than answer.\n",
    "    region_correct=[]\n",
    "    region_amount=[]\n",
    "    overall_correct=0\n",
    "    overall_amount=0\n",
    "    for i in range(NUM_region):\n",
    "        #(1) input\n",
    "        region_image=np.where(table_1D==i)[0]\n",
    "        #region_image=merged_region_image[i].copy()\n",
    "        \n",
    "        #(2) establish confusion matrix\n",
    "        for j in range(len(np.unique(test_label_answer))):\n",
    "            dist_table_truth[i][j]=len(np.where(test_label_answer[region_image]==j)[0]) #the number of images which equals to true answer \n",
    "        \n",
    "        #(3) statisitc\n",
    "        region_correct.append(dist_table_truth[i][region_label[i]])\n",
    "        region_amount.append(len(region_image))\n",
    "      \n",
    "    #(4) statistic for overall\n",
    "    overall_correct=sum(region_correct)\n",
    "    overall_amount=sum(region_amount)\n",
    "\n",
    "    return region_correct, region_amount, overall_correct, overall_amount, dist_table_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(PATH5,ITE):\n",
    "    # input 1:\n",
    "    # (1)merged_region_image_(ITE)\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    del merged_reg_and_nei\n",
    "    NUM_region=len(merged_region_image)\n",
    "    \n",
    "    # (2)test_label_answer\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "    # (3)get consistent result table\n",
    "    with open(newpath+'/(classes=' + str(NUM_region) + ')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "    LENGTH=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    " \n",
    "    # (4) Obtain the true label (answer) in our estimated region_image\n",
    "    region_label=[] #true label by selecting dominate ones\n",
    "    for i in range(NUM_region):\n",
    "        region_image=merged_region_image[i].copy()\n",
    "        region_label.append(collections.Counter(test_label_answer[region_image]).most_common()[0][0])  #images --> true label --> most_common label\n",
    "    print(\"true region_label=\", region_label)\n",
    "\n",
    "\n",
    "   #========================================     \n",
    "   # (1)train + test\n",
    "    a2,b2,c2,d2,e2=statistic_method(PATH5,NUM_region,region_label,Original_result)\n",
    "    print(\"dist_table_truth\\n\",e2)\n",
    "    na2=np.asarray(a2)\n",
    "    nb2=np.asarray(b2)\n",
    "    nc2=np.asarray(c2)\n",
    "    nd2=np.asarray(d2)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c2, d2,      round(nc2/nd2    ,3), \"5con over all, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c2, all_num, round(nc2/all_num,3), \"5con over all\"], csv_path1)\n",
    " \n",
    "\n",
    "        \n",
    "    # (2)train\n",
    "    train_results=-1*np.ones(LENGTH,dtype=int)\n",
    "    for i in range(NUM_region):\n",
    "        images=merged_region_image[i]\n",
    "        train_results[images]=i\n",
    "        print(\"num of merged_region_image\",i,len(merged_region_image[i]))\n",
    "    print(collections.Counter(train_results))\n",
    "        \n",
    "    a1,b1,c1,d1,e1=statistic_method(PATH5,NUM_region,region_label,train_results)\n",
    "    na1=np.asarray(a1)  #region_correct\n",
    "    nb1=np.asarray(b1)  #region_amount\n",
    "    nc1=np.asarray(c1)  #overall_correct\n",
    "    nd1=np.asarray(d1)  #overall_amount\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c1, d1, round(nc1/nd1,3), \"5con over trained\"], csv_path1)\n",
    "    append_csv([ITE, c1, all_num, round(nc1/all_num,3), \"5con over all\"], csv_path1)\n",
    "    \n",
    "        \n",
    "    # (3)test\n",
    "    # remove training data(good images), only check test data(bad images)\n",
    "    from itertools import chain\n",
    "    used_image=merged_region_image.copy()\n",
    "    used_image=list(chain.from_iterable(used_image))\n",
    "    Original_result2=Original_result.copy()\n",
    "    Original_result2[used_image]=-2\n",
    "\n",
    "        \n",
    "    a4,b4,c4,d4,e4=statistic_method(PATH5,NUM_region,region_label, Original_result2)\n",
    "    na4=np.asarray(a4)\n",
    "    nb4=np.asarray(b4)\n",
    "    nc4=np.asarray(c4)\n",
    "    nd4=np.asarray(d4) #this is len(Original_result)-len(used_image)-len(unconsistent)\n",
    "    untrain=len(Original_result)-len(used_image)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c4, untrain, round(nc4/untrain, 3), \"5con over untrained, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c4, all_num, round(nc4/all_num, 3), \"5con over untrained (unclean)\"], csv_path1)\n",
    "\n",
    "        \n",
    "    # (4)set majority as label for each image        \n",
    "    # set majority from 5 trias as label for each image\n",
    "    predicted_results_major=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        predicted_results_major[i]=collections.Counter(one_predicted_results.T[i]).most_common()[0][0]\n",
    "    \n",
    "\n",
    "    a3,b3,c3,d3,e3=statistic_method(PATH5,NUM_region,region_label,predicted_results_major)\n",
    "    na3=np.asarray(a3)\n",
    "    nb3=np.asarray(b3)\n",
    "    nc3=np.asarray(c3)\n",
    "    nd3=np.asarray(d3) #this is all in majority criterion\n",
    "    append_csv([ITE, c3, d3, round(nc3/nd3    ,3), \"majo over all\"], csv_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_and_expand(PATH5,ITE):\n",
    "# all4.\n",
    "    print(\"\\n\\n==== merged_and_expand(PATH5,ITE) ====\")\n",
    "    # load\n",
    "    with open('./region_initials.pickle', 'rb') as f:\n",
    "        all_region_index, all_region_image = pickle.load(f)\n",
    "    MAX_region = max(all_region_index)\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    NUM_region = len(merged_reg_and_nei) # NUM_region is the number of clusters\n",
    "\n",
    "    with open(newpath+'/(classes='+str(NUM_region)+')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "\n",
    "    # choose absolutely consistent images\n",
    "    NUM_test=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(NUM_test,dtype=int)\n",
    "\n",
    "    # (***)\n",
    "    # As set contains only unique elements, so convert the list to set.\n",
    "    # If set size is 1 then it means all elements in given list are same\n",
    "    for i in range(NUM_test):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    "    \n",
    "    used_img=list(chain.from_iterable(merged_region_image))\n",
    "    used_img=np.sort(used_img)\n",
    "    working_img = np.asarray(list(  set(range(NUM_test))-set(used_img)  ))  #working_img means the unclean ones for working on the further adding process\n",
    "    print(\"===========  ITE =\",ITE, \"  ===========\")    \n",
    "    print(\"used_img\",len(used_img), len(set(used_img)))    \n",
    "    print(\"working_img(=other images=unclean images)\",len(working_img), len(set(working_img)))\n",
    "\n",
    "    # save clean and unclean images\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/clean_and_unclean_image_ITE='+str(ITE)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([used_img, working_img], f) #used_img is clean, working_img is others\n",
    "\n",
    "    # other_regions\n",
    "    # ==== Process of other regions. Generate \"other_regions\" ====\n",
    "    merged_reg_and_nei_flatten=list(chain.from_iterable(merged_reg_and_nei))\n",
    "    print(\"merged regions\", len(merged_reg_and_nei_flatten), len(set(merged_reg_and_nei_flatten)))\n",
    "    other_regions       = list(  set(range(1,MAX_region+1))-set(merged_reg_and_nei_flatten)  ) #region index exclude used regions. 1 to 200.\n",
    "    print(\"other_regions\",len(other_regions), len(set(other_regions)))\n",
    "    \n",
    "    dmn_img             = [] # Index of dmn_img is consistent with other_regions\n",
    "    NUM_other_regions   = len(other_regions) # number of clusters in other regions\n",
    "    dist_table_truth    = np.zeros((NUM_other_regions,NUM_region),dtype=int)\n",
    "    p_reg_label_dmn     = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in predicted lagels.\n",
    "    grd_reg_answer_dmn  = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in true answers.\n",
    "    p_reg_dmn_rate      = np.zeros(NUM_other_regions,dtype=float) #one value. dominate ratio in a region\n",
    "\n",
    "    #(1) other_regions      --> establish all other region table \n",
    "    for i,region_name in enumerate(other_regions): #check all other regions\n",
    "\n",
    "        #(a)===== predicted images (multiple values) =====\n",
    "        p_img        = all_region_image[region_name-1] # In this region, get their images. \"region_name-1\" is due to region index starts from 1 to 200\n",
    "        p_img_label  = Original_result[p_img]   # Predicted labels in the region.\n",
    "        p_img_total  = len(p_img)\n",
    "        # the value of predicted labels is the index of trainning region. These indices are the labels\n",
    "        # but these p_img_answer are predicted, may not always be the truth.\n",
    "        if not p_img: # if p_img is empty, skip this loop\n",
    "            dmn_img.append([])\n",
    "            continue\n",
    "\n",
    "        #(b)===== region dominate; one value =====\n",
    "        #region label\n",
    "        p_reg_label_dmn[i] = collections.Counter(p_img_label).most_common()[0][0] # one value\n",
    "        # region dominate rate\n",
    "        if(p_reg_label_dmn[i]>=0):\n",
    "            p_reg_dmn_rate[i] = collections.Counter(p_img_label).most_common()[0][1]/p_img_total\n",
    "        else:              # means invalid label\n",
    "            p_reg_dmn_rate[i] = 0\n",
    "\n",
    "\n",
    "        #(c)==== ground truth =====\n",
    "        grd_label                 = test_label_answer[p_img]  #multiple values\n",
    "        grd_reg_answer_dmn[i]     = collections.Counter(grd_label).most_common()[0][0] #one value\n",
    "\n",
    "\n",
    "        #(d)==== establish confusion table=====\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(grd_label==j)[0])\n",
    "\n",
    "\n",
    "        #(e)=== collect dominated images =============\n",
    "        addr2=np.where( (p_img_label==p_reg_label_dmn[i]) & (p_img_label>=0) )[0] # ignore -1 which are non-consistency\n",
    "        #         the labels which  == 7               the labels which >= 0\n",
    "        temp=[]\n",
    "        for k in range(len(addr2)):\n",
    "            temp.append(p_img[addr2[k]])\n",
    "        dmn_img.append(temp)\n",
    "        #=============================================\n",
    "\n",
    "\n",
    "    df1 = pandas.DataFrame({\"other index\":other_regions}) # 1 to 200   other region index\n",
    "    df2 = pandas.DataFrame({\"pred label\":p_reg_label_dmn})      \n",
    "    df4 = pandas.DataFrame({\"truth\":grd_reg_answer_dmn})\n",
    "    df6 = pandas.DataFrame({\"rate\":np.round(p_reg_dmn_rate,2)})\n",
    "    df7 = pandas.DataFrame(dist_table_truth)\n",
    "    entire_table=pandas.concat([df1, df2, df4, df6, df7], axis=1)\n",
    "    print(\"All other regions\")\n",
    "    display(entire_table)\n",
    "\n",
    "\n",
    "\n",
    "    #(2)get regions according to conditions\n",
    "    NN=5 #choose top 5 regions\n",
    "    RATE=0.7\n",
    "    candidate_reg_by_top_NN=[]\n",
    "\n",
    "    # === get candidate regions by the order of dmn label 0 to 9 ====\n",
    "    for i in range(NUM_region):\n",
    "        # (2-1) ==== select region by rate > 0.7 and top 5 ====\n",
    "        index     = np.where(p_reg_label_dmn==i)[0] # index is the index of other_regions(0~183), rather than original entire region index 1 to 200        \n",
    "        working_table = entire_table.iloc[index]\n",
    "        working_table = working_table.sort_values(by=['rate'], ascending=False)\n",
    "        working_table = working_table.loc[working_table['rate'] > RATE]  #rate > 0.7\n",
    "        NUM_region_in_one_class = len(working_table.iloc[:NN])  #top 5\n",
    "               \n",
    "        # (2-2) ==== get candidate regions ====\n",
    "        # get top N records; save only the column 'other_reg', and transfer it to list from DataFrame by \"tolist()\"\n",
    "        candidate_reg_by_top_NN.append(working_table[:NN]['other index'].tolist())\n",
    "         \n",
    "    #(3) add regions and images\n",
    "    for i in range(NUM_region):\n",
    "        added_img=[]\n",
    "        if (len(candidate_reg_by_top_NN[i])>0):\n",
    "            for j in range(len(candidate_reg_by_top_NN[i])):\n",
    "                reg_addr  = np.where( np.array(other_regions)==candidate_reg_by_top_NN[i][j] )[0][0].tolist()\n",
    "                added_img = added_img + dmn_img[reg_addr]\n",
    "            # (3-1) add image\n",
    "            temp=len(merged_region_image[i])\n",
    "            merged_region_image[i] = merged_region_image[i] + added_img\n",
    "            merged_region_image[i] = list(set(merged_region_image[i]))\n",
    "            img_amount=len(merged_region_image[i])-temp\n",
    "            \n",
    "            # (3-2) add region\n",
    "            merged_reg_and_nei[i]  = merged_reg_and_nei[i] + candidate_reg_by_top_NN[i]\n",
    "            \n",
    "            # (3-3) print out\n",
    "            print(\"added label, regions, img amount:\", set(Original_result[added_img]), candidate_reg_by_top_NN[i], img_amount)\n",
    "\n",
    "            \n",
    "    # (4) collect residual images\n",
    "    # This works only for CIFAR10. All images in the MNIST and MNIST-TRAN are clean. No this issue.        \n",
    "    #20240105\n",
    "    if (not MNIST):\n",
    "    #if ((DATASET==2) or (DATASET==4)):\n",
    "        if (len(list(chain.from_iterable(candidate_reg_by_top_NN))) == 0):  #if no extra regions\n",
    "            #20240105\n",
    "            if (True):\n",
    "            #if(DATASET==4):\n",
    "                df = pandas.read_csv(PATH4)\n",
    "                tSNE_table = df.to_numpy()[:,:3]\n",
    "            else:\n",
    "                df = pandas.read_csv(PATH8)\n",
    "                tSNE_table = df.to_numpy()\n",
    "            print(\"tSNE_table\",np.shape(tSNE_table))\n",
    "\n",
    "            working_table=tSNE_table[working_img]\n",
    "            pairwise_dist=squareform(pdist(working_table, 'euclidean'))\n",
    "            print(\"pairwise_dist\",np.shape(pairwise_dist)) #value of data point, rather than image index\n",
    "\n",
    "            TopN=10\n",
    "            M=len(working_img)\n",
    "            nei_table_images  = np.zeros((M,TopN),dtype=int)  #contain top 10 images\n",
    "            nei_table_label   = np.zeros((M,TopN),dtype=int)\n",
    "            working_img_label = np.zeros(M,dtype=int)\n",
    "            for i in range(M):   \n",
    "                # fill up top 10 \n",
    "                addr=np.argsort(pairwise_dist[i])\n",
    "                for j in range(TopN):\n",
    "                    nei_table_images[i][j]=working_img[ addr[j+1] ] #Ignore first one. First one is itself\n",
    "                    nei_table_label[i][j] =Original_result[nei_table_images[i][j]]\n",
    "                # consistent\n",
    "                if (len(set(nei_table_label[i])) == 1): #only get the one which is entire consistent\n",
    "                    working_img_label[i]=nei_table_label[i][0]\n",
    "                else:\n",
    "                    working_img_label[i]=-1\n",
    "\n",
    "            print(\"nei_table_images\",np.shape(nei_table_images))\n",
    "            print(\"working_img_label\",working_img_label)\n",
    "\n",
    "            new_img=[] # just  for monitoring\n",
    "            for i in range(NUM_region):\n",
    "                addr=np.where(working_img_label==i)[0].tolist()\n",
    "                new_img.append(working_img[addr])\n",
    "                merged_region_image[i].extend(working_img[addr])\n",
    "            print(\"add residuals \",len(list(chain.from_iterable(new_img))))\n",
    "            print(\"number of next merged_region_image\", len(list(chain.from_iterable(merged_region_image))))\n",
    "        else:\n",
    "            print(\"Not getting into residuals\")\n",
    "\n",
    "    #save\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/merged_region_image_'+str(ITE+1)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([merged_reg_and_nei, merged_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup region_initials.pickle\n",
    "#### For both single network and integrate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>Spec200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.230895</td>\n",
       "      <td>-11.513212</td>\n",
       "      <td>18.781408</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.190353</td>\n",
       "      <td>-14.243707</td>\n",
       "      <td>14.359673</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.832576</td>\n",
       "      <td>-12.109021</td>\n",
       "      <td>15.785449</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.880559</td>\n",
       "      <td>-12.113366</td>\n",
       "      <td>15.720321</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.759654</td>\n",
       "      <td>-13.821998</td>\n",
       "      <td>15.943708</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1         X2         X3   Class  Label  Spec200\n",
       "0  1.230895 -11.513212  18.781408  Cherry      2      177\n",
       "1  3.190353 -14.243707  14.359673  Cherry      2      177\n",
       "2  1.832576 -12.109021  15.785449  Cherry      2      177\n",
       "3  1.880559 -12.113366  15.720321  Cherry      2      177\n",
       "4  5.759654 -13.821998  15.943708  Cherry      2      177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13076\n",
      "all_region_index\n",
      " [177 177 177 177 177]\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv(PATH4)\n",
    "display(df.head())\n",
    "#all_region_index = df.to_numpy().T[REGION_INDEX_LOC].astype(int)\n",
    "#print(len(all_region_index))\n",
    "all_region_index  = df[REG_COLUMN].to_numpy().astype(int)\n",
    "print(len(all_region_index))\n",
    "print(\"all_region_index\\n\",all_region_index[:5])\n",
    "\n",
    "all_region_image=[]\n",
    "MAX_region=max(all_region_index)\n",
    "for i in range(MAX_region):\n",
    "    addr=list(np.where(all_region_index==i+1)[0])\n",
    "    all_region_image.append(addr)    \n",
    "\n",
    "#save\n",
    "if (SAVE_bool):\n",
    "    with open('./region_initials.pickle', 'wb') as f:\n",
    "        pickle.dump([all_region_index, all_region_image], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_accu_results(path, AMOUNT_ITE):\n",
    "    df = pandas.read_csv(path)\n",
    "    label_table = df.to_numpy()\n",
    "    NUM_CRI=7  #number of our accuracy criteriors, now is 7\n",
    "    \n",
    "    criterion_string=\\\n",
    "    [ \"correct in 5-consensus\\n------------------------------------\\n5-consensus\\n\",\n",
    "     \"correct in 5-consensus\\n------------------------------------\\nall\\n\",\n",
    "     \"correct in train in 5-consensus\\n------------------------------------\\ntrain in 5-consensus\\n\",\n",
    "     \"correct in train in 5-consensus \\n------------------------------------\\nall\\n\",\n",
    "     \"correct in test in 5-consensus\\n------------------------------------\\ntest in 5-consensus\\n\",\n",
    "     \"correct in test in 5-consensus\\n------------------------------------\\nall\\n\",\n",
    "      \"correct\\n------------------\\nall\\n\",\n",
    "    ]\n",
    "    for SHIFT in range (NUM_CRI):\n",
    "        if (SHIFT+1==1):\n",
    "            print(\"(overall 5-consensus)\")\n",
    "        elif (SHIFT+1==3):\n",
    "            print(\"(clean)\")\n",
    "        elif (SHIFT+1==5):\n",
    "            print(\"(unclean)\")\n",
    "        elif (SHIFT+1==7):\n",
    "            print(\"(majority)\")\n",
    "        print(\"criterion\", SHIFT+1)\n",
    "        print(criterion_string[SHIFT])\n",
    "        for i in range(AMOUNT_ITE):\n",
    "            print(\"ITE\",label_table[NUM_CRI*i+SHIFT].T[0], \"   \",label_table[NUM_CRI*i+SHIFT].T[1],\"/\", label_table[NUM_CRI*i+SHIFT].T[2], \"=\",label_table[NUM_CRI*i+SHIFT].T[3])\n",
    "        print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_i in range(NUM_CASE):\n",
    "\n",
    "    #===== create folder case1, case2, case3...\n",
    "    print(\"case=\",case_i+1)\n",
    "    newpath = './case' + str(case_i+1)\n",
    "    if (not INTE_bool):\n",
    "        if not os.path.exists(newpath):   #No necessary in Integration\n",
    "            os.makedirs(newpath)\n",
    "    \n",
    "    #==== open csv 1\n",
    "    csv_path1 = newpath+'/' + 'accu_history.csv'\n",
    "    with open(csv_path1, 'a', newline='') as f:\n",
    "        csv_file = csv.writer(f)\n",
    "        csv_file.writerow(['ITE', 'correct', 'denominator', 'accu', 'description'])\n",
    "\n",
    "# 1.\n",
    "    if (not INTE_bool):\n",
    "        create_image_0(PATH6, case_i)   #No necessary in Integration\n",
    "\n",
    "\n",
    "    for ITE in range(ITE_START, ITE_END):\n",
    "# 2. CNN\n",
    "        CNN_part(PATH5,ITE)\n",
    "\n",
    "# 3. statistic\n",
    "        statistic(PATH5,ITE)\n",
    "\n",
    "# 4. merged_and_expand \n",
    "        merged_and_expand(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(overall 5-consensus)\n",
      "criterion 1\n",
      "correct in 5-consensus\n",
      "------------------------------------\n",
      "5-consensus\n",
      "\n",
      "ITE ITE     correct / denominator = accu\n",
      "ITE 0     4059 / 13076 = 0.31\n",
      "ITE 1     2397 / 13076 = 0.183\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "criterion 2\n",
      "correct in 5-consensus\n",
      "------------------------------------\n",
      "all\n",
      "\n",
      "ITE ITE     correct / denominator = accu\n",
      "ITE 0     7754 / 13076 = 0.593\n",
      "ITE 1     9519 / 13076 = 0.728\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(clean)\n",
      "criterion 3\n",
      "correct in train in 5-consensus\n",
      "------------------------------------\n",
      "train in 5-consensus\n",
      "\n",
      "ITE 0     7130 / 11393 = 0.626\n",
      "ITE 1     9116 / 11694 = 0.78\n",
      "ITE 2     9046 / 11748 = 0.77\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "criterion 4\n",
      "correct in train in 5-consensus \n",
      "------------------------------------\n",
      "all\n",
      "\n",
      "ITE 0     7130 / 13076 = 0.545\n",
      "ITE 1     9116 / 13076 = 0.697\n",
      "ITE 2     9046 / 13076 = 0.692\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(unclean)\n",
      "criterion 5\n",
      "correct in test in 5-consensus\n",
      "------------------------------------\n",
      "test in 5-consensus\n",
      "\n",
      "ITE 0     3072 / 3424 = 0.897\n",
      "ITE 1     6719 / 7785 = 0.863\n",
      "ITE 2     7822 / 8956 = 0.873\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "criterion 6\n",
      "correct in test in 5-consensus\n",
      "------------------------------------\n",
      "all\n",
      "\n",
      "ITE 0     3072 / 13076 = 0.235\n",
      "ITE 1     6719 / 13076 = 0.514\n",
      "ITE 2     7822 / 13076 = 0.598\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(majority)\n",
      "criterion 7\n",
      "correct\n",
      "------------------\n",
      "all\n",
      "\n",
      "ITE 0     4059 / 9652 = 0.421\n",
      "ITE 1     2397 / 5291 = 0.453\n",
      "ITE 2     1225 / 4120 = 0.297\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_accu_results(interpret_path, AMOUNT_ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
