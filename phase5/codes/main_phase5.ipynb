{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; May, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 5 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@ntu.edu.tw </p>\n",
    "### <p> Master Program in Statistics, National Taiwan University, Taipei, Taiwan.</p>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "from itertools import chain\n",
    "from scipy.spatial.distance import squareform, pdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "PATH4='../../phase3/data/ResNet18_PlantDisease_45K_Spec200.csv'\n",
    "PATH5='../../phase3/data/embedded_data.pickle'\n",
    "PATH6='../data/mergedseedclasslabels.txt'\n",
    "PATH7='../../phase3/data/region_for_phase5.pickle'\n",
    "\n",
    "\n",
    "# === parameters ===================================================\n",
    "MNIST     = False\n",
    "NUM_CASE  = 1\n",
    "INTE_bool = False  #True: Integrate two networks VGG+ResNet    False: single network\n",
    "SAVE_bool = True\n",
    "ITE_FROM  = 5 # This setting is ONLY for Integration\n",
    "REG_COLUMN = \"Spec200\"\n",
    "RAW_2D_DATA = False\n",
    "interpret_path='./case1/accu_history.csv'  #interprete the accuracy results\n",
    "AMOUNT_ITE=3\n",
    "\n",
    "\n",
    "if RAW_2D_DATA: # 2D\n",
    "    from CNN_Modules import ME_CNN\n",
    "else: # 1D\n",
    "    from CNN_Modules_1D import ME_CNN\n",
    "    \n",
    "\n",
    "if (INTE_bool):\n",
    "    ITE_START=ITE_FROM\n",
    "    ITE_END=ITE_FROM+4\n",
    "else:\n",
    "    ITE_START=0\n",
    "    ITE_END=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_csv(x, path):\n",
    "    with open(path,'a+', newline='') as f:\n",
    "        csv_file = csv.writer(f)#   = f.write()\n",
    "        csv_file.writerow(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for single network. No necessary in Integrated networks\n",
    "if (not INTE_bool):\n",
    "    def create_image_0(PATH6, case_i):\n",
    "        # ===================\n",
    "        #\n",
    "        #  prepare  merged_region_image_0\n",
    "        #\n",
    "        #====================\n",
    "        # (A)\n",
    "        #get \"(1)merged_region\"      only seed regions, no neighboring regions\n",
    "        df = pandas.read_csv(PATH6, delim_whitespace=' ', header=0,  index_col=None)\n",
    "        table = df.to_numpy()\n",
    "        print(\"mergedseedclasslabels table\")\n",
    "        display(table)\n",
    "\n",
    "        merged_region=[]\n",
    "        for i in range(min(table.T[case_i+1]), max(table.T[case_i+1])+1):  #18 ---merge to --> 10\n",
    "            addr=np.where(table.T[case_i+1]==i)[0] # 2nd column equal to 0(min),1,2,3...10(max); DO NOT consider 3rd column, which is hidden\n",
    "            if(len(addr) and i>0): #if not empty and i=0 is the invalid seed region.\n",
    "                merged_region.append(table[addr][:,0].tolist())\n",
    "        print(\"merged_region\")\n",
    "        display(merged_region)\n",
    "\n",
    "\n",
    "        # (B)\n",
    "        #get \"merged_reg_and_nei\"\n",
    "        #get \"merged_reg_and_nei_image\"\n",
    "        #generate \"merged_region_image_0.pickle\"\n",
    "\n",
    "        # (B_a)=== without neighbors ====\n",
    "        #if ((DATASET == 2) or (DATASET == 4)): \n",
    "        ##20240105\n",
    "        if (not True): \n",
    "            # ==== collect regions. No neighbors, just use merged regions ====\n",
    "            merged_reg_and_nei=merged_region.copy()\n",
    "\n",
    "            # ==== collect images ====\n",
    "            img_temp=[]\n",
    "            for i in range(len(merged_region)):\n",
    "                addr=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    temp=np.where(all_region_index==merged_region[i][j])[0].tolist()   #tolist(): convert temp into list\n",
    "                    addr=addr+temp\n",
    "                    print(len(temp),end=' ')\n",
    "                img_temp.append(addr)\n",
    "                print(\"=\",len(img_temp[i]))\n",
    "            merged_reg_and_nei_image = img_temp.copy()\n",
    "\n",
    "\n",
    "        # (B_b)=== with neighbors ==== \n",
    "        else: \n",
    "            with open(PATH7, 'rb') as f:\n",
    "                pre_region, pre_reg_nei, pre_region_image_pure, pre_region_image= pickle.load(f)\n",
    "            #    1reg         2reg+nei        1's img            2's img\n",
    "\n",
    "            # ==== collect regions with neighbors====\n",
    "            # remove duplicate  -->  https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them\n",
    "            merged_reg_and_nei=[]\n",
    "            NUM_region=len(merged_region)\n",
    "            for i in range(NUM_region):\n",
    "                temp=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0] \n",
    "                    temp=temp+pre_reg_nei[idx]\n",
    "                    print(idx,pre_region[idx])\n",
    "                merged_reg_and_nei.append(temp)\n",
    "\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(merged_reg_and_nei[i]) != len(set(merged_reg_and_nei[i]))):\n",
    "                    a=merged_reg_and_nei[i].copy()\n",
    "\n",
    "                    # find the duplicate.\n",
    "                    seen = set()\n",
    "                    dupli                 = [x for x in a if (x in seen or seen.add(x))]\n",
    "                    print(\"***duplicates:\",dupli)\n",
    "\n",
    "                    # keep fisrt one, remove succeeding duplicates.\n",
    "                    seen = set()\n",
    "                    merged_reg_and_nei[i] = [x for x in a if not (x in seen or seen.add(x))]  # a is the data to process; x is a working varialbe\n",
    "                    print(\"unique:\",merged_reg_and_nei[i])\n",
    "\n",
    "                print(\"total\",len(merged_reg_and_nei[i]),end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei\")\n",
    "            for i in range(len(merged_reg_and_nei)):\n",
    "                print(merged_reg_and_nei[i])\n",
    "\n",
    "\n",
    "            # Collect images\n",
    "            merged_reg_and_nei_image=[]\n",
    "            for i in range(NUM_region):\n",
    "                #search and add\n",
    "                img=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0]\n",
    "                    print(len(pre_region_image[idx]),\"(\",idx,\")\",end=' ')\n",
    "                    img=img+pre_region_image[idx] \n",
    "                print(\"=\",len(img),end=\" \")\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(img) != len(set(img))):\n",
    "                    img=list(set(img)) #remove duplicates\n",
    "                    print(\"     **duplicate, shrink to\",len(img),end=\"\\n\")  \n",
    "                else:\n",
    "                    print(end=\"\\n\")\n",
    "\n",
    "                #append\n",
    "                merged_reg_and_nei_image.append(img)\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei_image\")\n",
    "            for i in range(len(merged_reg_and_nei_image)):\n",
    "                print(len(merged_reg_and_nei_image[i]),merged_reg_and_nei_image[i][:5],\"...\")\n",
    "\n",
    "        # save\n",
    "        if (SAVE_bool):\n",
    "            with open(newpath+'/merged_region_image_0.pickle', 'wb') as f:\n",
    "                pickle.dump([merged_reg_and_nei, merged_reg_and_nei_image], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_part(PATH5,ITE):\n",
    "    TRIALS          = 5\n",
    "\n",
    "    savelog_path = newpath+'/' + 'log.txt'\n",
    "\n",
    "    # ==== test_array ====\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "        \n",
    "    if RAW_2D_DATA: # 2D\n",
    "        print(\"\")\n",
    "    else: # 1D\n",
    "        test_array = np.expand_dims(test_array, axis = -1)\n",
    "\n",
    "    \n",
    "    #if((DATASET==2) or (DATASET==4)):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #elif(DATASET==1):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #    test_array /= 255\n",
    "    #elif(DATASET==0):\n",
    "    #    test_array /= 255\n",
    "    #display(np.shape(test_array))\n",
    "\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    region_image=merged_region_image.copy()\n",
    "    del merged_reg_and_nei\n",
    "\n",
    "\n",
    "    NUM_region=len(region_image)\n",
    "    print(\"NUM_region\",NUM_region)\n",
    "\n",
    "\n",
    "    from itertools import chain\n",
    "    region_image_flatten=list(chain.from_iterable(region_image))\n",
    "    print(\"number of clean images\",len(region_image_flatten))\n",
    "\n",
    "\n",
    "    ROUND_start = time.time()\n",
    "    #========  merge ==========\n",
    "    #prepare selected_region, region\n",
    "    for n in range(1): #extra_original\n",
    "    #   #reset\n",
    "        region=region_image.copy()\n",
    "        region=list(region)\n",
    "        selected_region = list(range(NUM_region))  #[0,1,2, ... ,29]\n",
    "\n",
    "        #merge\n",
    "        if (n > 4):\n",
    "            p1=comb[n-1][0]\n",
    "            p2=comb[n-1][1]\n",
    "            region[p1]=region[p1]+region[p2]\n",
    "            region.pop(p2)\n",
    "            selected_region.pop(-1)  # remove last region index\n",
    "        #original\n",
    "        else:  #n=0\n",
    "            p1=0\n",
    "            p2=0\n",
    "\n",
    "        print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "        # ===== one CNN =============\n",
    "        NUM_CLASSES = len(selected_region)  #NUM_CLASSES should be here to update for each loop\n",
    "        \n",
    "        # Clip the numeber of class. The \"test_label_answer\" is just for the verification. The changed \"test_label_answer\"\n",
    "        # doesn't affact the CNN predictions.\n",
    "        if NUM_CLASSES < len(np.unique(test_label_answer)):\n",
    "            test_label_answer=np.clip(test_label_answer, 0, NUM_CLASSES-1)\n",
    "\n",
    "        # input image and label\n",
    "        Input_img     = []\n",
    "        Input_img_len = []\n",
    "        for c,sel in enumerate(selected_region, start=0):\n",
    "            Input_img = Input_img + list(region[sel])\n",
    "            Input_img_len.append(len(region[sel])) #can only concatenate list (not \"int\") to list    \n",
    "            \n",
    "        # 20240319\n",
    "        if RAW_2D_DATA: # 2D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            H           = np.shape(test_array[0])[1]\n",
    "            train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "        else: # 1D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "                  \n",
    "        train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "        # fill up the training label to each training image\n",
    "        current_train_label = np.zeros(len(train_array), dtype=int)  # Assign 0 to the label\n",
    "        accum_base=0  #accumulate\n",
    "        for label in range(1,NUM_CLASSES):\n",
    "            sector = Input_img_len[label-1]\n",
    "            accum_base = accum_base + sector  # sector is the sector length\n",
    "            current_train_label[accum_base:] = label  # fill the label\n",
    "\n",
    "\n",
    "        # CNN\n",
    "        #===============================================\n",
    "        one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "        one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "        model_history = np.zeros(TRIALS, dtype=list)\n",
    "        print(\"NUM_CLASSES\",NUM_CLASSES)\n",
    "        print(\"current_train_label: \",list(set(current_train_label)))\n",
    "        for r in range(TRIALS):  #10\n",
    "            one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                    x_train     = train_array,\n",
    "                    train_label = current_train_label,\n",
    "                    test_array  = test_array,\n",
    "                    true_answer = test_label_answer,\n",
    "                    Num_Classes = NUM_CLASSES\n",
    "                    )\n",
    "            print(type(model_history))\n",
    "\n",
    "\n",
    "            # ===== delete CNN tensors =====\n",
    "            from keras import backend as K\n",
    "            K.clear_session()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"One CNN, r: \",r)\n",
    "            ROUND_duration = time.time() - ROUND_start\n",
    "            print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "\n",
    "        # === save to file ===\n",
    "        #This is useless in phase IV. Prepare for further checking in the future.\n",
    "        savefile_path = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'_'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path, 'wb') as f:\n",
    "            pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "        savefile_path2 = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_5_tests_simple_ITE'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path2, 'wb') as f:\n",
    "            pickle.dump([one_predicted_results, one_predict_percentage], f)\n",
    "\n",
    "        # === save to log ===    \n",
    "        savelog = open(savelog_path, 'a+')\n",
    "        print(\"\\n\", savefile_path, file = savelog)\n",
    "        print(\"Saved parameters: Input_img, Input_img_len, one_predicted_results, one_predict_percentage\", file = savelog) #0722\n",
    "\n",
    "        # total time\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Completion time: \", datetime.datetime.now(), file = savelog)\n",
    "        print(\"Total Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)), file = savelog)\n",
    "\n",
    "        savelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_method(PATH5,NUM_region,region_label,table_1D):\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "    \n",
    "    dist_table_truth=np.zeros((NUM_region, len(np.unique(test_label_answer))),dtype=int)   # [6 x 9] matrix. The size of \"NUM_region\" may be less than answer.\n",
    "    region_correct=[]\n",
    "    region_amount=[]\n",
    "    overall_correct=0\n",
    "    overall_amount=0\n",
    "    for i in range(NUM_region):\n",
    "        #(1) input\n",
    "        region_image=np.where(table_1D==i)[0]\n",
    "        #region_image=merged_region_image[i].copy()\n",
    "        \n",
    "        #(2) establish confusion matrix\n",
    "        for j in range(len(np.unique(test_label_answer))):\n",
    "            dist_table_truth[i][j]=len(np.where(test_label_answer[region_image]==j)[0]) #the number of images which equals to true answer \n",
    "        \n",
    "        #(3) statisitc\n",
    "        region_correct.append(dist_table_truth[i][region_label[i]])\n",
    "        region_amount.append(len(region_image))\n",
    "      \n",
    "    #(4) statistic for overall\n",
    "    overall_correct=sum(region_correct)\n",
    "    overall_amount=sum(region_amount)\n",
    "\n",
    "    return region_correct, region_amount, overall_correct, overall_amount, dist_table_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(PATH5,ITE):\n",
    "    # input 1:\n",
    "    # (1)merged_region_image_(ITE)\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    del merged_reg_and_nei\n",
    "    NUM_region=len(merged_region_image)\n",
    "    \n",
    "    # (2)test_label_answer\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "    # (3)get consistent result table\n",
    "    with open(newpath+'/(classes=' + str(NUM_region) + ')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "    LENGTH=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    " \n",
    "    # (4) Obtain the true label (answer) in our estimated region_image\n",
    "    region_label=[] #true label by selecting dominate ones\n",
    "    for i in range(NUM_region):\n",
    "        region_image=merged_region_image[i].copy()\n",
    "        region_label.append(collections.Counter(test_label_answer[region_image]).most_common()[0][0])  #images --> true label --> most_common label\n",
    "    print(\"true region_label=\", region_label)\n",
    "\n",
    "\n",
    "   #========================================     \n",
    "   # (1)train + test\n",
    "    a2,b2,c2,d2,e2=statistic_method(PATH5,NUM_region,region_label,Original_result)\n",
    "    print(\"dist_table_truth\\n\",e2)\n",
    "    na2=np.asarray(a2)\n",
    "    nb2=np.asarray(b2)\n",
    "    nc2=np.asarray(c2)\n",
    "    nd2=np.asarray(d2)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c2, d2,      round(nc2/nd2    ,3), \"5con over all, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c2, all_num, round(nc2/all_num,3), \"5con over all\"], csv_path1)\n",
    " \n",
    "\n",
    "        \n",
    "    # (2)train\n",
    "    train_results=-1*np.ones(LENGTH,dtype=int)\n",
    "    for i in range(NUM_region):\n",
    "        images=merged_region_image[i]\n",
    "        train_results[images]=i\n",
    "        print(\"num of merged_region_image\",i,len(merged_region_image[i]))\n",
    "    print(collections.Counter(train_results))\n",
    "        \n",
    "    a1,b1,c1,d1,e1=statistic_method(PATH5,NUM_region,region_label,train_results)\n",
    "    na1=np.asarray(a1)  #region_correct\n",
    "    nb1=np.asarray(b1)  #region_amount\n",
    "    nc1=np.asarray(c1)  #overall_correct\n",
    "    nd1=np.asarray(d1)  #overall_amount\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c1, d1, round(nc1/nd1,3), \"5con over trained\"], csv_path1)\n",
    "    append_csv([ITE, c1, all_num, round(nc1/all_num,3), \"5con over all\"], csv_path1)\n",
    "    \n",
    "        \n",
    "    # (3)test\n",
    "    # remove training data(good images), only check test data(bad images)\n",
    "    from itertools import chain\n",
    "    used_image=merged_region_image.copy()\n",
    "    used_image=list(chain.from_iterable(used_image))\n",
    "    Original_result2=Original_result.copy()\n",
    "    Original_result2[used_image]=-2\n",
    "\n",
    "        \n",
    "    a4,b4,c4,d4,e4=statistic_method(PATH5,NUM_region,region_label, Original_result2)\n",
    "    na4=np.asarray(a4)\n",
    "    nb4=np.asarray(b4)\n",
    "    nc4=np.asarray(c4)\n",
    "    nd4=np.asarray(d4) #this is len(Original_result)-len(used_image)-len(unconsistent)\n",
    "    untrain=len(Original_result)-len(used_image)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c4, untrain, round(nc4/untrain, 3), \"5con over untrained, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c4, all_num, round(nc4/all_num, 3), \"5con over untrained (unclean)\"], csv_path1)\n",
    "\n",
    "        \n",
    "    # (4)set majority as label for each image        \n",
    "    # set majority from 5 trias as label for each image\n",
    "    predicted_results_major=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        predicted_results_major[i]=collections.Counter(one_predicted_results.T[i]).most_common()[0][0]\n",
    "    \n",
    "\n",
    "    a3,b3,c3,d3,e3=statistic_method(PATH5,NUM_region,region_label,predicted_results_major)\n",
    "    na3=np.asarray(a3)\n",
    "    nb3=np.asarray(b3)\n",
    "    nc3=np.asarray(c3)\n",
    "    nd3=np.asarray(d3) #this is all in majority criterion\n",
    "    append_csv([ITE, c3, d3, round(nc3/nd3    ,3), \"majo over all\"], csv_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_and_expand(PATH5,ITE):\n",
    "# all4.\n",
    "    print(\"\\n\\n==== merged_and_expand(PATH5,ITE) ====\")\n",
    "    # load\n",
    "    with open('./region_initials.pickle', 'rb') as f:\n",
    "        all_region_index, all_region_image = pickle.load(f)\n",
    "    MAX_region = max(all_region_index)\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    NUM_region = len(merged_reg_and_nei) # NUM_region is the number of clusters\n",
    "\n",
    "    with open(newpath+'/(classes='+str(NUM_region)+')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "\n",
    "    # choose absolutely consistent images\n",
    "    NUM_test=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(NUM_test,dtype=int)\n",
    "\n",
    "    # (***)\n",
    "    # As set contains only unique elements, so convert the list to set.\n",
    "    # If set size is 1 then it means all elements in given list are same\n",
    "    for i in range(NUM_test):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    "    \n",
    "    used_img=list(chain.from_iterable(merged_region_image))\n",
    "    used_img=np.sort(used_img)\n",
    "    working_img = np.asarray(list(  set(range(NUM_test))-set(used_img)  ))  #working_img means the unclean ones for working on the further adding process\n",
    "    print(\"===========  ITE =\",ITE, \"  ===========\")    \n",
    "    print(\"used_img\",len(used_img), len(set(used_img)))    \n",
    "    print(\"working_img(=other images=unclean images)\",len(working_img), len(set(working_img)))\n",
    "\n",
    "    # save clean and unclean images\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/clean_and_unclean_image_ITE='+str(ITE)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([used_img, working_img], f) #used_img is clean, working_img is others\n",
    "\n",
    "    # other_regions\n",
    "    # ==== Process of other regions. Generate \"other_regions\" ====\n",
    "    merged_reg_and_nei_flatten=list(chain.from_iterable(merged_reg_and_nei))\n",
    "    print(\"merged regions\", len(merged_reg_and_nei_flatten), len(set(merged_reg_and_nei_flatten)))\n",
    "    other_regions       = list(  set(range(1,MAX_region+1))-set(merged_reg_and_nei_flatten)  ) #region index exclude used regions. 1 to 200.\n",
    "    print(\"other_regions\",len(other_regions), len(set(other_regions)))\n",
    "    \n",
    "    dmn_img             = [] # Index of dmn_img is consistent with other_regions\n",
    "    NUM_other_regions   = len(other_regions) # number of clusters in other regions\n",
    "    dist_table_truth    = np.zeros((NUM_other_regions,NUM_region),dtype=int)\n",
    "    p_reg_label_dmn     = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in predicted lagels.\n",
    "    grd_reg_answer_dmn  = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in true answers.\n",
    "    p_reg_dmn_rate      = np.zeros(NUM_other_regions,dtype=float) #one value. dominate ratio in a region\n",
    "\n",
    "    #(1) other_regions      --> establish all other region table \n",
    "    for i,region_name in enumerate(other_regions): #check all other regions\n",
    "\n",
    "        #(a)===== predicted images (multiple values) =====\n",
    "        p_img        = all_region_image[region_name-1] # In this region, get their images. \"region_name-1\" is due to region index starts from 1 to 200\n",
    "        p_img_label  = Original_result[p_img]   # Predicted labels in the region.\n",
    "        p_img_total  = len(p_img)\n",
    "        # the value of predicted labels is the index of trainning region. These indices are the labels\n",
    "        # but these p_img_answer are predicted, may not always be the truth.\n",
    "        if not p_img: # if p_img is empty, skip this loop\n",
    "            dmn_img.append([])\n",
    "            continue\n",
    "\n",
    "        #(b)===== region dominate; one value =====\n",
    "        #region label\n",
    "        p_reg_label_dmn[i] = collections.Counter(p_img_label).most_common()[0][0] # one value\n",
    "        # region dominate rate\n",
    "        if(p_reg_label_dmn[i]>=0):\n",
    "            p_reg_dmn_rate[i] = collections.Counter(p_img_label).most_common()[0][1]/p_img_total\n",
    "        else:              # means invalid label\n",
    "            p_reg_dmn_rate[i] = 0\n",
    "\n",
    "\n",
    "        #(c)==== ground truth =====\n",
    "        grd_label                 = test_label_answer[p_img]  #multiple values\n",
    "        grd_reg_answer_dmn[i]     = collections.Counter(grd_label).most_common()[0][0] #one value\n",
    "\n",
    "\n",
    "        #(d)==== establish confusion table=====\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(grd_label==j)[0])\n",
    "\n",
    "\n",
    "        #(e)=== collect dominated images =============\n",
    "        addr2=np.where( (p_img_label==p_reg_label_dmn[i]) & (p_img_label>=0) )[0] # ignore -1 which are non-consistency\n",
    "        #         the labels which  == 7               the labels which >= 0\n",
    "        temp=[]\n",
    "        for k in range(len(addr2)):\n",
    "            temp.append(p_img[addr2[k]])\n",
    "        dmn_img.append(temp)\n",
    "        #=============================================\n",
    "\n",
    "\n",
    "    df1 = pandas.DataFrame({\"other index\":other_regions}) # 1 to 200   other region index\n",
    "    df2 = pandas.DataFrame({\"pred label\":p_reg_label_dmn})      \n",
    "    df4 = pandas.DataFrame({\"truth\":grd_reg_answer_dmn})\n",
    "    df6 = pandas.DataFrame({\"rate\":np.round(p_reg_dmn_rate,2)})\n",
    "    df7 = pandas.DataFrame(dist_table_truth)\n",
    "    entire_table=pandas.concat([df1, df2, df4, df6, df7], axis=1)\n",
    "    print(\"All other regions\")\n",
    "    display(entire_table)\n",
    "\n",
    "\n",
    "\n",
    "    #(2)get regions according to conditions\n",
    "    NN=5 #choose top 5 regions\n",
    "    RATE=0.7\n",
    "    candidate_reg_by_top_NN=[]\n",
    "\n",
    "    # === get candidate regions by the order of dmn label 0 to 9 ====\n",
    "    for i in range(NUM_region):\n",
    "        # (2-1) ==== select region by rate > 0.7 and top 5 ====\n",
    "        index     = np.where(p_reg_label_dmn==i)[0] # index is the index of other_regions(0~183), rather than original entire region index 1 to 200        \n",
    "        working_table = entire_table.iloc[index]\n",
    "        working_table = working_table.sort_values(by=['rate'], ascending=False)\n",
    "        working_table = working_table.loc[working_table['rate'] > RATE]  #rate > 0.7\n",
    "        NUM_region_in_one_class = len(working_table.iloc[:NN])  #top 5\n",
    "               \n",
    "        # (2-2) ==== get candidate regions ====\n",
    "        # get top N records; save only the column 'other_reg', and transfer it to list from DataFrame by \"tolist()\"\n",
    "        candidate_reg_by_top_NN.append(working_table[:NN]['other index'].tolist())\n",
    "         \n",
    "    #(3) add regions and images\n",
    "    for i in range(NUM_region):\n",
    "        added_img=[]\n",
    "        if (len(candidate_reg_by_top_NN[i])>0):\n",
    "            for j in range(len(candidate_reg_by_top_NN[i])):\n",
    "                reg_addr  = np.where( np.array(other_regions)==candidate_reg_by_top_NN[i][j] )[0][0].tolist()\n",
    "                added_img = added_img + dmn_img[reg_addr]\n",
    "            # (3-1) add image\n",
    "            temp=len(merged_region_image[i])\n",
    "            merged_region_image[i] = merged_region_image[i] + added_img\n",
    "            merged_region_image[i] = list(set(merged_region_image[i]))\n",
    "            img_amount=len(merged_region_image[i])-temp\n",
    "            \n",
    "            # (3-2) add region\n",
    "            merged_reg_and_nei[i]  = merged_reg_and_nei[i] + candidate_reg_by_top_NN[i]\n",
    "            \n",
    "            # (3-3) print out\n",
    "            print(\"added label, regions, img amount:\", set(Original_result[added_img]), candidate_reg_by_top_NN[i], img_amount)\n",
    "\n",
    "            \n",
    "    # (4) collect residual images\n",
    "    # This works only for CIFAR10. All images in the MNIST and MNIST-TRAN are clean. No this issue.        \n",
    "    #20240105\n",
    "    if (not MNIST):\n",
    "    #if ((DATASET==2) or (DATASET==4)):\n",
    "        if (len(list(chain.from_iterable(candidate_reg_by_top_NN))) == 0):  #if no extra regions\n",
    "            #20240105\n",
    "            if (True):\n",
    "            #if(DATASET==4):\n",
    "                df = pandas.read_csv(PATH4)\n",
    "                tSNE_table = df.to_numpy()[:,:3]\n",
    "            else:\n",
    "                df = pandas.read_csv(PATH8)\n",
    "                tSNE_table = df.to_numpy()\n",
    "            print(\"tSNE_table\",np.shape(tSNE_table))\n",
    "\n",
    "            working_table=tSNE_table[working_img]\n",
    "            pairwise_dist=squareform(pdist(working_table, 'euclidean'))\n",
    "            print(\"pairwise_dist\",np.shape(pairwise_dist)) #value of data point, rather than image index\n",
    "\n",
    "            TopN=10\n",
    "            M=len(working_img)\n",
    "            nei_table_images  = np.zeros((M,TopN),dtype=int)  #contain top 10 images\n",
    "            nei_table_label   = np.zeros((M,TopN),dtype=int)\n",
    "            working_img_label = np.zeros(M,dtype=int)\n",
    "            for i in range(M):   \n",
    "                # fill up top 10 \n",
    "                addr=np.argsort(pairwise_dist[i])\n",
    "                for j in range(TopN):\n",
    "                    nei_table_images[i][j]=working_img[ addr[j+1] ] #Ignore first one. First one is itself\n",
    "                    nei_table_label[i][j] =Original_result[nei_table_images[i][j]]\n",
    "                # consistent\n",
    "                if (len(set(nei_table_label[i])) == 1): #only get the one which is entire consistent\n",
    "                    working_img_label[i]=nei_table_label[i][0]\n",
    "                else:\n",
    "                    working_img_label[i]=-1\n",
    "\n",
    "            print(\"nei_table_images\",np.shape(nei_table_images))\n",
    "            print(\"working_img_label\",working_img_label)\n",
    "\n",
    "            new_img=[] # just  for monitoring\n",
    "            for i in range(NUM_region):\n",
    "                addr=np.where(working_img_label==i)[0].tolist()\n",
    "                new_img.append(working_img[addr])\n",
    "                merged_region_image[i].extend(working_img[addr])\n",
    "            print(\"add residuals \",len(list(chain.from_iterable(new_img))))\n",
    "            print(\"number of next merged_region_image\", len(list(chain.from_iterable(merged_region_image))))\n",
    "        else:\n",
    "            print(\"Not getting into residuals\")\n",
    "\n",
    "    #save\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/merged_region_image_'+str(ITE+1)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([merged_reg_and_nei, merged_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup region_initials.pickle\n",
    "#### For both single network and integrate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>Spec200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.131136</td>\n",
       "      <td>-11.714952</td>\n",
       "      <td>14.559869</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.653914</td>\n",
       "      <td>-15.913769</td>\n",
       "      <td>8.271565</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.666949</td>\n",
       "      <td>-15.911179</td>\n",
       "      <td>8.316633</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.160013</td>\n",
       "      <td>-8.344077</td>\n",
       "      <td>19.694381</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.430728</td>\n",
       "      <td>-5.483346</td>\n",
       "      <td>17.380125</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1         X2         X3   Class  Label  Spec200\n",
       "0 -6.131136 -11.714952  14.559869  Cherry      2      100\n",
       "1  4.653914 -15.913769   8.271565  Cherry      2        6\n",
       "2  4.666949 -15.911179   8.316633  Cherry      2        6\n",
       "3 -1.160013  -8.344077  19.694381  Cherry      2      166\n",
       "4 -0.430728  -5.483346  17.380125  Cherry      2      170"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43217\n",
      "all_region_index\n",
      " [100   6   6 166 170]\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv(PATH4)\n",
    "display(df.head())\n",
    "#all_region_index = df.to_numpy().T[REGION_INDEX_LOC].astype(int)\n",
    "#print(len(all_region_index))\n",
    "all_region_index  = df[REG_COLUMN].to_numpy().astype(int)\n",
    "print(len(all_region_index))\n",
    "print(\"all_region_index\\n\",all_region_index[:5])\n",
    "\n",
    "all_region_image=[]\n",
    "MAX_region=max(all_region_index)\n",
    "for i in range(MAX_region):\n",
    "    addr=list(np.where(all_region_index==i+1)[0])\n",
    "    all_region_image.append(addr)    \n",
    "\n",
    "#save\n",
    "if (SAVE_bool):\n",
    "    with open('./region_initials.pickle', 'wb') as f:\n",
    "        pickle.dump([all_region_index, all_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_accu_results(path, AMOUNT_ITE):\n",
    "    df = pandas.read_csv(path)\n",
    "    label_table = df.to_numpy()\n",
    "    NUM_CRI=7  #number of our accuracy criteriors, now is 7\n",
    "    \n",
    "    criterion_string=\\\n",
    "    [ \"correct in 5-consensus\\n------------------------------------\\n5-consensus\\n\",\n",
    "     \"correct in 5-consensus\\n------------------------------------\\nall\\n\",\n",
    "     \"correct in train in 5-consensus\\n------------------------------------\\ntrain in 5-consensus\\n\",\n",
    "     \"correct in train in 5-consensus \\n------------------------------------\\nall\\n\",\n",
    "     \"correct in test in 5-consensus\\n------------------------------------\\ntest in 5-consensus\\n\",\n",
    "     \"correct in test in 5-consensus\\n------------------------------------\\nall\\n\",\n",
    "      \"correct\\n------------------\\nall\\n\",\n",
    "    ]\n",
    "    for SHIFT in range (NUM_CRI):\n",
    "        if (SHIFT+1==1):\n",
    "            print(\"(overall 5-consensus)\")\n",
    "        elif (SHIFT+1==3):\n",
    "            print(\"(clean)\")\n",
    "        elif (SHIFT+1==5):\n",
    "            print(\"(unclean)\")\n",
    "        elif (SHIFT+1==7):\n",
    "            print(\"(majority)\")\n",
    "        print(\"criterion\", SHIFT+1)\n",
    "        print(criterion_string[SHIFT])\n",
    "        for i in range(AMOUNT_ITE):\n",
    "            print(\"ITE\",label_table[NUM_CRI*i+SHIFT].T[0], \"   \",label_table[NUM_CRI*i+SHIFT].T[1],\"/\", label_table[NUM_CRI*i+SHIFT].T[2], \"=\",label_table[NUM_CRI*i+SHIFT].T[3])\n",
    "        print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case= 1\n",
      "mergedseedclasslabels table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 33,   3],\n",
       "       [ 95,   0],\n",
       "       [131,   0],\n",
       "       [ 24,   1],\n",
       "       [125,   2],\n",
       "       [ 10,   4],\n",
       "       [ 94,   0],\n",
       "       [192,   2],\n",
       "       [135,   0],\n",
       "       [ 25,   3],\n",
       "       [139,   0],\n",
       "       [168,   0],\n",
       "       [110,   0],\n",
       "       [ 60,   1],\n",
       "       [134,   4],\n",
       "       [ 27,   5],\n",
       "       [ 58,   6],\n",
       "       [ 28,   0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_region\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[24, 60], [125, 192], [33, 25], [10, 134], [27], [58]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 24\n",
      "13 60\n",
      "total 11\n",
      "\n",
      "4 125\n",
      "7 192\n",
      "total 12\n",
      "\n",
      "0 33\n",
      "9 25\n",
      "total 12\n",
      "\n",
      "5 10\n",
      "14 134\n",
      "total 12\n",
      "\n",
      "15 27\n",
      "total 6\n",
      "\n",
      "16 58\n",
      "total 5\n",
      "\n",
      "\n",
      "merged_reg_and_nei\n",
      "[24, 15, 63, 146, 115, 109, 60, 61, 160, 121, 26]\n",
      "[125, 68, 127, 36, 67, 174, 192, 116, 147, 91, 64, 198]\n",
      "[33, 138, 55, 48, 87, 1, 25, 17, 156, 175, 88, 66]\n",
      "[10, 197, 120, 96, 130, 50, 134, 155, 161, 41, 86, 157]\n",
      "[27, 178, 56, 5, 42, 75]\n",
      "[58, 126, 180, 89, 179]\n",
      "1148 ( 3 ) 1068 ( 13 ) = 2216 \n",
      "842 ( 4 ) 1730 ( 7 ) = 2572 \n",
      "2038 ( 0 ) 1195 ( 9 ) = 3233 \n",
      "1324 ( 5 ) 1285 ( 14 ) = 2609 \n",
      "1344 ( 15 ) = 1344 \n",
      "1904 ( 16 ) = 1904 \n",
      "\n",
      "merged_reg_and_nei_image\n",
      "2216 [18223, 18299, 18329, 18370, 18382] ...\n",
      "2572 [8854, 8856, 8885, 8915, 8925] ...\n",
      "3233 [28228, 28235, 28239, 28279, 28281] ...\n",
      "2609 [4427, 4444, 4450, 4459, 4477] ...\n",
      "1344 [23220, 23228, 23232, 23236, 23239] ...\n",
      "1904 [13347, 13348, 13349, 13352, 13353] ...\n",
      "NUM_region 6\n",
      "number of clean images 13878\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 6\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12490 samples, validate on 1388 samples\n",
      "Epoch 1/80\n",
      "12490/12490 [==============================] - 2s 175us/step - loss: 0.8242 - accuracy: 0.7002 - val_loss: 0.5491 - val_accuracy: 0.7637\n",
      "Epoch 2/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.4641 - accuracy: 0.8038 - val_loss: 0.4426 - val_accuracy: 0.7875\n",
      "Epoch 3/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.3984 - accuracy: 0.8282 - val_loss: 0.4142 - val_accuracy: 0.8184\n",
      "Epoch 4/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.3425 - accuracy: 0.8565 - val_loss: 0.3362 - val_accuracy: 0.8595\n",
      "Epoch 5/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2921 - accuracy: 0.8825 - val_loss: 0.2949 - val_accuracy: 0.8797\n",
      "Epoch 6/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2596 - accuracy: 0.8934 - val_loss: 0.2638 - val_accuracy: 0.8927\n",
      "Epoch 7/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2412 - accuracy: 0.9012 - val_loss: 0.2456 - val_accuracy: 0.8977\n",
      "Epoch 8/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2238 - accuracy: 0.9076 - val_loss: 0.2506 - val_accuracy: 0.8934\n",
      "Epoch 9/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.2107 - accuracy: 0.9089 - val_loss: 0.2287 - val_accuracy: 0.8999\n",
      "Epoch 10/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.2007 - accuracy: 0.9112 - val_loss: 0.2292 - val_accuracy: 0.8999\n",
      "Epoch 11/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2040 - accuracy: 0.9097 - val_loss: 0.2141 - val_accuracy: 0.8984\n",
      "Epoch 12/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.1923 - accuracy: 0.9159 - val_loss: 0.2438 - val_accuracy: 0.9006\n",
      "Epoch 13/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1893 - accuracy: 0.9165 - val_loss: 0.2281 - val_accuracy: 0.9042\n",
      "Epoch 14/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1864 - accuracy: 0.9164 - val_loss: 0.2281 - val_accuracy: 0.9035\n",
      "Epoch 15/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1868 - accuracy: 0.9153 - val_loss: 0.2387 - val_accuracy: 0.9006\n",
      "Epoch 16/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1854 - accuracy: 0.9169 - val_loss: 0.2106 - val_accuracy: 0.8984\n",
      "Epoch 17/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1769 - accuracy: 0.9195 - val_loss: 0.1987 - val_accuracy: 0.9121\n",
      "Epoch 18/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1757 - accuracy: 0.9202 - val_loss: 0.1994 - val_accuracy: 0.9099\n",
      "Epoch 19/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1719 - accuracy: 0.9237 - val_loss: 0.2108 - val_accuracy: 0.9092\n",
      "Epoch 20/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1698 - accuracy: 0.9248 - val_loss: 0.2178 - val_accuracy: 0.9092\n",
      "Epoch 21/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1722 - accuracy: 0.9241 - val_loss: 0.1942 - val_accuracy: 0.9164\n",
      "Epoch 22/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1697 - accuracy: 0.9251 - val_loss: 0.1974 - val_accuracy: 0.9157\n",
      "Epoch 23/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.1676 - accuracy: 0.9272 - val_loss: 0.1923 - val_accuracy: 0.9099\n",
      "Epoch 24/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1694 - accuracy: 0.9261 - val_loss: 0.1848 - val_accuracy: 0.9164\n",
      "Epoch 25/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1652 - accuracy: 0.9261 - val_loss: 0.2028 - val_accuracy: 0.9143\n",
      "Epoch 26/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1661 - accuracy: 0.9267 - val_loss: 0.2051 - val_accuracy: 0.9128\n",
      "Epoch 27/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1651 - accuracy: 0.9266 - val_loss: 0.1857 - val_accuracy: 0.9164\n",
      "Epoch 28/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1657 - accuracy: 0.9241 - val_loss: 0.1954 - val_accuracy: 0.9143\n",
      "Epoch 29/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1655 - accuracy: 0.9242 - val_loss: 0.2023 - val_accuracy: 0.9164\n",
      "Epoch 30/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1666 - accuracy: 0.9240 - val_loss: 0.1837 - val_accuracy: 0.9179\n",
      "Epoch 31/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1621 - accuracy: 0.9263 - val_loss: 0.1905 - val_accuracy: 0.9143\n",
      "Epoch 32/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1622 - accuracy: 0.9257 - val_loss: 0.1781 - val_accuracy: 0.9150\n",
      "Epoch 33/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1616 - accuracy: 0.9270 - val_loss: 0.1820 - val_accuracy: 0.9171\n",
      "Epoch 34/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1608 - accuracy: 0.9278 - val_loss: 0.1827 - val_accuracy: 0.9186\n",
      "Epoch 35/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1611 - accuracy: 0.9303 - val_loss: 0.1885 - val_accuracy: 0.9150\n",
      "Epoch 36/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1583 - accuracy: 0.9280 - val_loss: 0.1974 - val_accuracy: 0.9114\n",
      "Epoch 37/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1576 - accuracy: 0.9281 - val_loss: 0.1896 - val_accuracy: 0.9150\n",
      "Epoch 38/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1575 - accuracy: 0.9284 - val_loss: 0.1818 - val_accuracy: 0.9121\n",
      "Epoch 39/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1537 - accuracy: 0.9299 - val_loss: 0.1761 - val_accuracy: 0.9171\n",
      "Epoch 40/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1562 - accuracy: 0.9274 - val_loss: 0.1844 - val_accuracy: 0.9143\n",
      "Epoch 41/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1550 - accuracy: 0.9299 - val_loss: 0.1886 - val_accuracy: 0.9164\n",
      "Epoch 42/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1608 - accuracy: 0.9256 - val_loss: 0.1858 - val_accuracy: 0.9128\n",
      "Epoch 43/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1588 - accuracy: 0.9280 - val_loss: 0.2041 - val_accuracy: 0.9092\n",
      "Epoch 44/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1540 - accuracy: 0.9283 - val_loss: 0.1965 - val_accuracy: 0.9143\n",
      "Epoch 45/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.1582 - accuracy: 0.9272 - val_loss: 0.2068 - val_accuracy: 0.9121\n",
      "Epoch 46/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1537 - accuracy: 0.9307 - val_loss: 0.1915 - val_accuracy: 0.9114\n",
      "Epoch 47/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1528 - accuracy: 0.9283 - val_loss: 0.1783 - val_accuracy: 0.9200\n",
      "Epoch 48/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1527 - accuracy: 0.9280 - val_loss: 0.1729 - val_accuracy: 0.9143\n",
      "Epoch 49/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1514 - accuracy: 0.9318 - val_loss: 0.1840 - val_accuracy: 0.9150\n",
      "Epoch 50/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1504 - accuracy: 0.9291 - val_loss: 0.1733 - val_accuracy: 0.9186\n",
      "Epoch 51/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1531 - accuracy: 0.9263 - val_loss: 0.1758 - val_accuracy: 0.9193\n",
      "Epoch 52/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1558 - accuracy: 0.9256 - val_loss: 0.1690 - val_accuracy: 0.9229\n",
      "Epoch 53/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1535 - accuracy: 0.9271 - val_loss: 0.1703 - val_accuracy: 0.9193\n",
      "Epoch 54/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1513 - accuracy: 0.9310 - val_loss: 0.1833 - val_accuracy: 0.9164\n",
      "Epoch 55/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1481 - accuracy: 0.9313 - val_loss: 0.1836 - val_accuracy: 0.9164\n",
      "Epoch 56/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1513 - accuracy: 0.9266 - val_loss: 0.1810 - val_accuracy: 0.9143\n",
      "Epoch 57/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1517 - accuracy: 0.9273 - val_loss: 0.2061 - val_accuracy: 0.9143\n",
      "Epoch 58/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1501 - accuracy: 0.9283 - val_loss: 0.1785 - val_accuracy: 0.9150\n",
      "Epoch 59/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1515 - accuracy: 0.9291 - val_loss: 0.1859 - val_accuracy: 0.9171\n",
      "Epoch 60/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1471 - accuracy: 0.9307 - val_loss: 0.1655 - val_accuracy: 0.9193\n",
      "Epoch 61/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1482 - accuracy: 0.9303 - val_loss: 0.1711 - val_accuracy: 0.9164\n",
      "Epoch 62/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.1473 - accuracy: 0.9299 - val_loss: 0.1777 - val_accuracy: 0.9150\n",
      "Epoch 63/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1488 - accuracy: 0.9286 - val_loss: 0.1720 - val_accuracy: 0.9186\n",
      "Epoch 64/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1451 - accuracy: 0.9313 - val_loss: 0.1719 - val_accuracy: 0.9186\n",
      "Epoch 65/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1472 - accuracy: 0.9272 - val_loss: 0.1660 - val_accuracy: 0.9193\n",
      "Epoch 66/80\n",
      "12490/12490 [==============================] - 1s 43us/step - loss: 0.1476 - accuracy: 0.9298 - val_loss: 0.1679 - val_accuracy: 0.9171\n",
      "Epoch 67/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1452 - accuracy: 0.9320 - val_loss: 0.1628 - val_accuracy: 0.9186\n",
      "Epoch 68/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1464 - accuracy: 0.9305 - val_loss: 0.1754 - val_accuracy: 0.9143\n",
      "Epoch 69/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1437 - accuracy: 0.9303 - val_loss: 0.1647 - val_accuracy: 0.9236\n",
      "Epoch 70/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1448 - accuracy: 0.9311 - val_loss: 0.1611 - val_accuracy: 0.9179\n",
      "Epoch 71/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1438 - accuracy: 0.9313 - val_loss: 0.1639 - val_accuracy: 0.9222\n",
      "Epoch 72/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1419 - accuracy: 0.9321 - val_loss: 0.1651 - val_accuracy: 0.9179\n",
      "Epoch 73/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.1459 - accuracy: 0.9305 - val_loss: 0.1811 - val_accuracy: 0.9193\n",
      "Epoch 74/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1445 - accuracy: 0.9303 - val_loss: 0.1718 - val_accuracy: 0.9186\n",
      "Epoch 75/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1447 - accuracy: 0.9315 - val_loss: 0.1789 - val_accuracy: 0.9193\n",
      "Epoch 76/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1459 - accuracy: 0.9310 - val_loss: 0.1694 - val_accuracy: 0.9186\n",
      "Epoch 77/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1406 - accuracy: 0.9312 - val_loss: 0.1714 - val_accuracy: 0.9121\n",
      "Epoch 78/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1433 - accuracy: 0.9317 - val_loss: 0.1610 - val_accuracy: 0.9207\n",
      "Epoch 79/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1407 - accuracy: 0.9324 - val_loss: 0.1580 - val_accuracy: 0.9186\n",
      "Epoch 80/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1410 - accuracy: 0.9318 - val_loss: 0.1736 - val_accuracy: 0.9193\n",
      "[[9.9999714e-01 2.8823804e-06 5.3921454e-11 4.2130168e-17 2.0575005e-11\n",
      "  1.7709518e-11]\n",
      " [5.3159840e-04 5.4968726e-03 5.8089172e-05 3.0523790e-13 4.0673872e-04\n",
      "  9.9350679e-01]\n",
      " [5.2101305e-04 5.4296702e-03 5.7601701e-05 3.0176190e-13 4.0300100e-04\n",
      "  9.9358881e-01]\n",
      " ...\n",
      " [7.4886599e-12 1.7220884e-15 9.9991667e-01 7.8555262e-05 4.8413025e-14\n",
      "  4.7821186e-06]\n",
      " [1.7764109e-05 2.9947381e-12 9.0787286e-01 3.6935233e-02 5.3356446e-02\n",
      "  1.8176835e-03]\n",
      " [1.1447115e-04 1.8774948e-09 9.8231712e-06 2.0075541e-08 9.9975675e-01\n",
      "  1.1897328e-04]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:00:58.789733\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12490 samples, validate on 1388 samples\n",
      "Epoch 1/80\n",
      "12490/12490 [==============================] - 1s 56us/step - loss: 0.8891 - accuracy: 0.6976 - val_loss: 0.5568 - val_accuracy: 0.7601\n",
      "Epoch 2/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.4606 - accuracy: 0.8038 - val_loss: 0.4799 - val_accuracy: 0.7896\n",
      "Epoch 3/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.4089 - accuracy: 0.8235 - val_loss: 0.4313 - val_accuracy: 0.8091\n",
      "Epoch 4/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.3638 - accuracy: 0.8448 - val_loss: 0.3741 - val_accuracy: 0.8429\n",
      "Epoch 5/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.3235 - accuracy: 0.8672 - val_loss: 0.3195 - val_accuracy: 0.8739\n",
      "Epoch 6/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2960 - accuracy: 0.8857 - val_loss: 0.2876 - val_accuracy: 0.8833\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2663 - accuracy: 0.8929 - val_loss: 0.2719 - val_accuracy: 0.8804\n",
      "Epoch 8/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2440 - accuracy: 0.8983 - val_loss: 0.2452 - val_accuracy: 0.8854\n",
      "Epoch 9/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.2327 - accuracy: 0.9024 - val_loss: 0.2447 - val_accuracy: 0.8898\n",
      "Epoch 10/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2211 - accuracy: 0.9062 - val_loss: 0.2238 - val_accuracy: 0.8883\n",
      "Epoch 11/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2140 - accuracy: 0.9054 - val_loss: 0.2088 - val_accuracy: 0.9006\n",
      "Epoch 12/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.2091 - accuracy: 0.9076 - val_loss: 0.2238 - val_accuracy: 0.8905\n",
      "Epoch 13/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2033 - accuracy: 0.9073 - val_loss: 0.1995 - val_accuracy: 0.8984\n",
      "Epoch 14/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2009 - accuracy: 0.9108 - val_loss: 0.2660 - val_accuracy: 0.8912\n",
      "Epoch 15/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2018 - accuracy: 0.9089 - val_loss: 0.1949 - val_accuracy: 0.9020\n",
      "Epoch 16/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2015 - accuracy: 0.9098 - val_loss: 0.1927 - val_accuracy: 0.9063\n",
      "Epoch 17/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1914 - accuracy: 0.9123 - val_loss: 0.1938 - val_accuracy: 0.9020\n",
      "Epoch 18/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1946 - accuracy: 0.9104 - val_loss: 0.1929 - val_accuracy: 0.9020\n",
      "Epoch 19/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1924 - accuracy: 0.9127 - val_loss: 0.1899 - val_accuracy: 0.9035\n",
      "Epoch 20/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1882 - accuracy: 0.9102 - val_loss: 0.1938 - val_accuracy: 0.9049\n",
      "Epoch 21/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1864 - accuracy: 0.9131 - val_loss: 0.1899 - val_accuracy: 0.9020\n",
      "Epoch 22/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1856 - accuracy: 0.9121 - val_loss: 0.1882 - val_accuracy: 0.9006\n",
      "Epoch 23/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1813 - accuracy: 0.9151 - val_loss: 0.1889 - val_accuracy: 0.8963\n",
      "Epoch 24/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1795 - accuracy: 0.9142 - val_loss: 0.2100 - val_accuracy: 0.9035\n",
      "Epoch 25/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1788 - accuracy: 0.9159 - val_loss: 0.1810 - val_accuracy: 0.9049\n",
      "Epoch 26/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1743 - accuracy: 0.9138 - val_loss: 0.1819 - val_accuracy: 0.9049\n",
      "Epoch 27/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1804 - accuracy: 0.9118 - val_loss: 0.1816 - val_accuracy: 0.9006\n",
      "Epoch 28/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1742 - accuracy: 0.9167 - val_loss: 0.1974 - val_accuracy: 0.9056\n",
      "Epoch 29/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1777 - accuracy: 0.9151 - val_loss: 0.1874 - val_accuracy: 0.9027\n",
      "Epoch 30/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1729 - accuracy: 0.9138 - val_loss: 0.2118 - val_accuracy: 0.9020\n",
      "Epoch 31/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1744 - accuracy: 0.9159 - val_loss: 0.1954 - val_accuracy: 0.8984\n",
      "Epoch 32/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1724 - accuracy: 0.9187 - val_loss: 0.1945 - val_accuracy: 0.9027\n",
      "Epoch 33/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1723 - accuracy: 0.9172 - val_loss: 0.1804 - val_accuracy: 0.9035\n",
      "Epoch 34/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1670 - accuracy: 0.9165 - val_loss: 0.1828 - val_accuracy: 0.9085\n",
      "Epoch 35/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1689 - accuracy: 0.9194 - val_loss: 0.1727 - val_accuracy: 0.9099\n",
      "Epoch 36/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1703 - accuracy: 0.9172 - val_loss: 0.1756 - val_accuracy: 0.9035\n",
      "Epoch 37/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1682 - accuracy: 0.9199 - val_loss: 0.1716 - val_accuracy: 0.9092\n",
      "Epoch 38/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1651 - accuracy: 0.9211 - val_loss: 0.1842 - val_accuracy: 0.9099\n",
      "Epoch 39/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1706 - accuracy: 0.9199 - val_loss: 0.1811 - val_accuracy: 0.9056\n",
      "Epoch 40/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1689 - accuracy: 0.9175 - val_loss: 0.1873 - val_accuracy: 0.9092\n",
      "Epoch 41/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1684 - accuracy: 0.9191 - val_loss: 0.1810 - val_accuracy: 0.9135\n",
      "Epoch 42/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1637 - accuracy: 0.9219 - val_loss: 0.1774 - val_accuracy: 0.9128\n",
      "Epoch 43/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1623 - accuracy: 0.9223 - val_loss: 0.1745 - val_accuracy: 0.9128\n",
      "Epoch 44/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1605 - accuracy: 0.9222 - val_loss: 0.1720 - val_accuracy: 0.9135\n",
      "Epoch 45/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1646 - accuracy: 0.9223 - val_loss: 0.1688 - val_accuracy: 0.9150\n",
      "Epoch 46/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1596 - accuracy: 0.9230 - val_loss: 0.1758 - val_accuracy: 0.9099\n",
      "Epoch 47/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1642 - accuracy: 0.9216 - val_loss: 0.1693 - val_accuracy: 0.9121\n",
      "Epoch 48/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1642 - accuracy: 0.9222 - val_loss: 0.1684 - val_accuracy: 0.9150\n",
      "Epoch 49/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1593 - accuracy: 0.9247 - val_loss: 0.1713 - val_accuracy: 0.9121\n",
      "Epoch 50/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1596 - accuracy: 0.9255 - val_loss: 0.1749 - val_accuracy: 0.9143\n",
      "Epoch 51/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1598 - accuracy: 0.9249 - val_loss: 0.1796 - val_accuracy: 0.9020\n",
      "Epoch 52/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1575 - accuracy: 0.9264 - val_loss: 0.1608 - val_accuracy: 0.9179\n",
      "Epoch 53/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1576 - accuracy: 0.9265 - val_loss: 0.1630 - val_accuracy: 0.9200\n",
      "Epoch 54/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1591 - accuracy: 0.9278 - val_loss: 0.1765 - val_accuracy: 0.9128\n",
      "Epoch 55/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1579 - accuracy: 0.9281 - val_loss: 0.1762 - val_accuracy: 0.9179\n",
      "Epoch 56/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1581 - accuracy: 0.9272 - val_loss: 0.1717 - val_accuracy: 0.9143\n",
      "Epoch 57/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1586 - accuracy: 0.9263 - val_loss: 0.1686 - val_accuracy: 0.9179\n",
      "Epoch 58/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1556 - accuracy: 0.9279 - val_loss: 0.1637 - val_accuracy: 0.9164\n",
      "Epoch 59/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1562 - accuracy: 0.9263 - val_loss: 0.1616 - val_accuracy: 0.9179\n",
      "Epoch 60/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1563 - accuracy: 0.9284 - val_loss: 0.1587 - val_accuracy: 0.9179\n",
      "Epoch 61/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1561 - accuracy: 0.9281 - val_loss: 0.1684 - val_accuracy: 0.9157\n",
      "Epoch 62/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1538 - accuracy: 0.9300 - val_loss: 0.1683 - val_accuracy: 0.9179\n",
      "Epoch 63/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1544 - accuracy: 0.9267 - val_loss: 0.1653 - val_accuracy: 0.9193\n",
      "Epoch 64/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.1597 - accuracy: 0.9290 - val_loss: 0.1714 - val_accuracy: 0.9114\n",
      "Epoch 65/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1527 - accuracy: 0.9295 - val_loss: 0.1704 - val_accuracy: 0.9135\n",
      "Epoch 66/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1542 - accuracy: 0.9291 - val_loss: 0.1693 - val_accuracy: 0.9207\n",
      "Epoch 67/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1531 - accuracy: 0.9304 - val_loss: 0.1665 - val_accuracy: 0.9222\n",
      "Epoch 68/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1509 - accuracy: 0.9285 - val_loss: 0.1596 - val_accuracy: 0.9229\n",
      "Epoch 69/80\n",
      "12490/12490 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.93 - 0s 40us/step - loss: 0.1527 - accuracy: 0.9312 - val_loss: 0.1562 - val_accuracy: 0.9272\n",
      "Epoch 70/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1560 - accuracy: 0.9279 - val_loss: 0.1570 - val_accuracy: 0.9207\n",
      "Epoch 71/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1512 - accuracy: 0.9295 - val_loss: 0.1565 - val_accuracy: 0.9236\n",
      "Epoch 72/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1514 - accuracy: 0.9299 - val_loss: 0.1553 - val_accuracy: 0.9244\n",
      "Epoch 73/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1498 - accuracy: 0.9294 - val_loss: 0.1507 - val_accuracy: 0.9244\n",
      "Epoch 74/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1468 - accuracy: 0.9319 - val_loss: 0.1655 - val_accuracy: 0.9157\n",
      "Epoch 75/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1504 - accuracy: 0.9302 - val_loss: 0.1544 - val_accuracy: 0.9215\n",
      "Epoch 76/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1472 - accuracy: 0.9317 - val_loss: 0.1528 - val_accuracy: 0.9236\n",
      "Epoch 77/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1480 - accuracy: 0.9315 - val_loss: 0.1544 - val_accuracy: 0.9207\n",
      "Epoch 78/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1460 - accuracy: 0.9301 - val_loss: 0.1759 - val_accuracy: 0.9121\n",
      "Epoch 79/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1470 - accuracy: 0.9308 - val_loss: 0.1656 - val_accuracy: 0.9207\n",
      "Epoch 80/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1486 - accuracy: 0.9307 - val_loss: 0.1740 - val_accuracy: 0.9092\n",
      "[[1.0000000e+00 3.3677343e-09 4.2253916e-16 2.8072630e-10 5.5274050e-08\n",
      "  8.7756293e-14]\n",
      " [6.5397966e-04 4.0196148e-03 6.6488128e-11 2.3930639e-07 7.4138260e-04\n",
      "  9.9458486e-01]\n",
      " [6.4550078e-04 3.9698039e-03 6.6655056e-11 2.3702131e-07 7.3572632e-04\n",
      "  9.9464875e-01]\n",
      " ...\n",
      " [3.0142025e-10 3.6302390e-09 9.9953496e-01 4.6506608e-04 3.7735644e-12\n",
      "  4.9087934e-08]\n",
      " [1.5344511e-06 1.3929733e-08 9.9713242e-01 6.0621329e-04 1.5591796e-03\n",
      "  7.0065854e-04]\n",
      " [5.2377538e-05 7.6923632e-09 1.1936616e-08 1.8075488e-07 9.9910873e-01\n",
      "  8.3872420e-04]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:01:42.539756\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12490 samples, validate on 1388 samples\n",
      "Epoch 1/80\n",
      "12490/12490 [==============================] - 1s 68us/step - loss: 0.6833 - accuracy: 0.7428 - val_loss: 0.4238 - val_accuracy: 0.8228\n",
      "Epoch 2/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.4247 - accuracy: 0.8141 - val_loss: 0.3506 - val_accuracy: 0.8631\n",
      "Epoch 3/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.3584 - accuracy: 0.8488 - val_loss: 0.3008 - val_accuracy: 0.8912\n",
      "Epoch 4/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.2952 - accuracy: 0.8819 - val_loss: 0.2465 - val_accuracy: 0.9092\n",
      "Epoch 5/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2602 - accuracy: 0.8933 - val_loss: 0.2373 - val_accuracy: 0.9099\n",
      "Epoch 6/80\n",
      "12490/12490 [==============================] - 1s 45us/step - loss: 0.2430 - accuracy: 0.9006 - val_loss: 0.2032 - val_accuracy: 0.9222\n",
      "Epoch 7/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2223 - accuracy: 0.9067 - val_loss: 0.2090 - val_accuracy: 0.9056\n",
      "Epoch 8/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.2127 - accuracy: 0.9071 - val_loss: 0.1941 - val_accuracy: 0.9150\n",
      "Epoch 9/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.2079 - accuracy: 0.9083 - val_loss: 0.1930 - val_accuracy: 0.9107\n",
      "Epoch 10/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1995 - accuracy: 0.9083 - val_loss: 0.1851 - val_accuracy: 0.9121\n",
      "Epoch 11/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1961 - accuracy: 0.9078 - val_loss: 0.1979 - val_accuracy: 0.8948\n",
      "Epoch 12/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1897 - accuracy: 0.9128 - val_loss: 0.1834 - val_accuracy: 0.9099\n",
      "Epoch 13/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1894 - accuracy: 0.9124 - val_loss: 0.1751 - val_accuracy: 0.9157\n",
      "Epoch 14/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1862 - accuracy: 0.9118 - val_loss: 0.1761 - val_accuracy: 0.9171\n",
      "Epoch 15/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1824 - accuracy: 0.9112 - val_loss: 0.1687 - val_accuracy: 0.9171\n",
      "Epoch 16/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1859 - accuracy: 0.9101 - val_loss: 0.1850 - val_accuracy: 0.9280\n",
      "Epoch 17/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1805 - accuracy: 0.9151 - val_loss: 0.1695 - val_accuracy: 0.9222\n",
      "Epoch 18/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1767 - accuracy: 0.9142 - val_loss: 0.1644 - val_accuracy: 0.9280\n",
      "Epoch 19/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1820 - accuracy: 0.9139 - val_loss: 0.1631 - val_accuracy: 0.9272\n",
      "Epoch 20/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1673 - accuracy: 0.9197 - val_loss: 0.1635 - val_accuracy: 0.9229\n",
      "Epoch 21/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1718 - accuracy: 0.9211 - val_loss: 0.1580 - val_accuracy: 0.9287\n",
      "Epoch 22/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1686 - accuracy: 0.9209 - val_loss: 0.1551 - val_accuracy: 0.9287\n",
      "Epoch 23/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1644 - accuracy: 0.9227 - val_loss: 0.1692 - val_accuracy: 0.9171\n",
      "Epoch 24/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1693 - accuracy: 0.9212 - val_loss: 0.1681 - val_accuracy: 0.9193\n",
      "Epoch 25/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1659 - accuracy: 0.9239 - val_loss: 0.1571 - val_accuracy: 0.9236\n",
      "Epoch 26/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1638 - accuracy: 0.9237 - val_loss: 0.1624 - val_accuracy: 0.9215\n",
      "Epoch 27/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1636 - accuracy: 0.9241 - val_loss: 0.1636 - val_accuracy: 0.9272\n",
      "Epoch 28/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1626 - accuracy: 0.9239 - val_loss: 0.1486 - val_accuracy: 0.9337\n",
      "Epoch 29/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1642 - accuracy: 0.9246 - val_loss: 0.1579 - val_accuracy: 0.9337\n",
      "Epoch 30/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1660 - accuracy: 0.9232 - val_loss: 0.1496 - val_accuracy: 0.9337\n",
      "Epoch 31/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1629 - accuracy: 0.9255 - val_loss: 0.1642 - val_accuracy: 0.9193\n",
      "Epoch 32/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1639 - accuracy: 0.9265 - val_loss: 0.1576 - val_accuracy: 0.9280\n",
      "Epoch 33/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1605 - accuracy: 0.9258 - val_loss: 0.1496 - val_accuracy: 0.9323\n",
      "Epoch 34/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1642 - accuracy: 0.9244 - val_loss: 0.1552 - val_accuracy: 0.9323\n",
      "Epoch 35/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1623 - accuracy: 0.9257 - val_loss: 0.1564 - val_accuracy: 0.9265\n",
      "Epoch 36/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1591 - accuracy: 0.9265 - val_loss: 0.1544 - val_accuracy: 0.9301\n",
      "Epoch 37/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1561 - accuracy: 0.9295 - val_loss: 0.1501 - val_accuracy: 0.9337\n",
      "Epoch 38/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1591 - accuracy: 0.9277 - val_loss: 0.1526 - val_accuracy: 0.9337\n",
      "Epoch 00038: early stopping\n",
      "[[9.9999964e-01 1.3392291e-07 5.7531636e-11 1.4514627e-07 5.7547748e-08\n",
      "  3.2893495e-09]\n",
      " [8.7204209e-04 4.6216762e-03 5.1189668e-06 2.3756127e-04 2.9081060e-03\n",
      "  9.9135548e-01]\n",
      " [8.5380458e-04 4.5358604e-03 5.0871627e-06 2.3463307e-04 2.8847621e-03\n",
      "  9.9148589e-01]\n",
      " ...\n",
      " [7.0284480e-11 1.4145384e-07 9.9538475e-01 4.6103946e-03 3.6348569e-09\n",
      "  4.7730459e-06]\n",
      " [2.4697051e-06 1.0845871e-07 9.8538637e-01 2.8473586e-03 1.0296378e-02\n",
      "  1.4672953e-03]\n",
      " [2.8131704e-04 4.8047522e-08 3.0991446e-06 2.8968173e-06 9.9856949e-01\n",
      "  1.1431575e-03]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:02:05.899139\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 12490 samples, validate on 1388 samples\n",
      "Epoch 1/80\n",
      "12490/12490 [==============================] - 1s 56us/step - loss: 0.8485 - accuracy: 0.6792 - val_loss: 0.5330 - val_accuracy: 0.7702\n",
      "Epoch 2/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.4458 - accuracy: 0.8118 - val_loss: 0.4074 - val_accuracy: 0.8148\n",
      "Epoch 3/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.3594 - accuracy: 0.8496 - val_loss: 0.3327 - val_accuracy: 0.8667\n",
      "Epoch 4/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2894 - accuracy: 0.8839 - val_loss: 0.2693 - val_accuracy: 0.8970\n",
      "Epoch 5/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2471 - accuracy: 0.9001 - val_loss: 0.2416 - val_accuracy: 0.8991\n",
      "Epoch 6/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2235 - accuracy: 0.9050 - val_loss: 0.2242 - val_accuracy: 0.8984\n",
      "Epoch 7/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.2113 - accuracy: 0.9050 - val_loss: 0.2232 - val_accuracy: 0.9013\n",
      "Epoch 8/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2015 - accuracy: 0.9106 - val_loss: 0.2060 - val_accuracy: 0.9006\n",
      "Epoch 9/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2004 - accuracy: 0.9091 - val_loss: 0.2237 - val_accuracy: 0.8941\n",
      "Epoch 10/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1927 - accuracy: 0.9108 - val_loss: 0.2106 - val_accuracy: 0.8999\n",
      "Epoch 11/80\n",
      "12490/12490 [==============================] - 0s 34us/step - loss: 0.1912 - accuracy: 0.9129 - val_loss: 0.2050 - val_accuracy: 0.9006\n",
      "Epoch 12/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1879 - accuracy: 0.9112 - val_loss: 0.2015 - val_accuracy: 0.9042\n",
      "Epoch 13/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1888 - accuracy: 0.9112 - val_loss: 0.2069 - val_accuracy: 0.9107\n",
      "Epoch 14/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1845 - accuracy: 0.9143 - val_loss: 0.2170 - val_accuracy: 0.8919\n",
      "Epoch 15/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1845 - accuracy: 0.9131 - val_loss: 0.1961 - val_accuracy: 0.9056\n",
      "Epoch 16/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1816 - accuracy: 0.9141 - val_loss: 0.2018 - val_accuracy: 0.8970\n",
      "Epoch 17/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1808 - accuracy: 0.9135 - val_loss: 0.2030 - val_accuracy: 0.9071\n",
      "Epoch 18/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1850 - accuracy: 0.9113 - val_loss: 0.1960 - val_accuracy: 0.9042\n",
      "Epoch 19/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1798 - accuracy: 0.9139 - val_loss: 0.1877 - val_accuracy: 0.9099\n",
      "Epoch 20/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1773 - accuracy: 0.9163 - val_loss: 0.1931 - val_accuracy: 0.9042\n",
      "Epoch 21/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1755 - accuracy: 0.9165 - val_loss: 0.1846 - val_accuracy: 0.9085\n",
      "Epoch 22/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1790 - accuracy: 0.9167 - val_loss: 0.1901 - val_accuracy: 0.8991\n",
      "Epoch 23/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1715 - accuracy: 0.9188 - val_loss: 0.1931 - val_accuracy: 0.9114\n",
      "Epoch 24/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1729 - accuracy: 0.9184 - val_loss: 0.1800 - val_accuracy: 0.9114\n",
      "Epoch 25/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1706 - accuracy: 0.9199 - val_loss: 0.1777 - val_accuracy: 0.9128\n",
      "Epoch 26/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1696 - accuracy: 0.9184 - val_loss: 0.1815 - val_accuracy: 0.9186\n",
      "Epoch 27/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1662 - accuracy: 0.9227 - val_loss: 0.1746 - val_accuracy: 0.9193\n",
      "Epoch 28/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1670 - accuracy: 0.9228 - val_loss: 0.1713 - val_accuracy: 0.9207\n",
      "Epoch 29/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1669 - accuracy: 0.9239 - val_loss: 0.1869 - val_accuracy: 0.8963\n",
      "Epoch 30/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1662 - accuracy: 0.9238 - val_loss: 0.1744 - val_accuracy: 0.9157\n",
      "Epoch 31/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1679 - accuracy: 0.9231 - val_loss: 0.1690 - val_accuracy: 0.9236\n",
      "Epoch 32/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1657 - accuracy: 0.9229 - val_loss: 0.1691 - val_accuracy: 0.9150\n",
      "Epoch 33/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1613 - accuracy: 0.9250 - val_loss: 0.1746 - val_accuracy: 0.9207\n",
      "Epoch 34/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1625 - accuracy: 0.9259 - val_loss: 0.1692 - val_accuracy: 0.9215\n",
      "Epoch 35/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1616 - accuracy: 0.9259 - val_loss: 0.1814 - val_accuracy: 0.9150\n",
      "Epoch 36/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1614 - accuracy: 0.9254 - val_loss: 0.1785 - val_accuracy: 0.9222\n",
      "Epoch 37/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1593 - accuracy: 0.9261 - val_loss: 0.1902 - val_accuracy: 0.9215\n",
      "Epoch 38/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1616 - accuracy: 0.9255 - val_loss: 0.1707 - val_accuracy: 0.9186\n",
      "Epoch 39/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1627 - accuracy: 0.9271 - val_loss: 0.1686 - val_accuracy: 0.9229\n",
      "Epoch 40/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1582 - accuracy: 0.9259 - val_loss: 0.1667 - val_accuracy: 0.9186\n",
      "Epoch 41/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1557 - accuracy: 0.9286 - val_loss: 0.1771 - val_accuracy: 0.9229\n",
      "Epoch 42/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1591 - accuracy: 0.9285 - val_loss: 0.1644 - val_accuracy: 0.9272\n",
      "Epoch 43/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1555 - accuracy: 0.9275 - val_loss: 0.1670 - val_accuracy: 0.9272\n",
      "Epoch 44/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1569 - accuracy: 0.9286 - val_loss: 0.1664 - val_accuracy: 0.9236\n",
      "Epoch 45/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1561 - accuracy: 0.9285 - val_loss: 0.1803 - val_accuracy: 0.9171\n",
      "Epoch 46/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1570 - accuracy: 0.9302 - val_loss: 0.1738 - val_accuracy: 0.9251\n",
      "Epoch 47/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1525 - accuracy: 0.9293 - val_loss: 0.1682 - val_accuracy: 0.9236\n",
      "Epoch 48/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1566 - accuracy: 0.9271 - val_loss: 0.1591 - val_accuracy: 0.9337\n",
      "Epoch 49/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1518 - accuracy: 0.9309 - val_loss: 0.1724 - val_accuracy: 0.9193\n",
      "Epoch 50/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1527 - accuracy: 0.9289 - val_loss: 0.1624 - val_accuracy: 0.9280\n",
      "Epoch 51/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1525 - accuracy: 0.9294 - val_loss: 0.1703 - val_accuracy: 0.9171\n",
      "Epoch 52/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1504 - accuracy: 0.9312 - val_loss: 0.1696 - val_accuracy: 0.9272\n",
      "Epoch 53/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1508 - accuracy: 0.9300 - val_loss: 0.1581 - val_accuracy: 0.9244\n",
      "Epoch 54/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1572 - accuracy: 0.9284 - val_loss: 0.1672 - val_accuracy: 0.9244\n",
      "Epoch 55/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1492 - accuracy: 0.9309 - val_loss: 0.1692 - val_accuracy: 0.9244\n",
      "Epoch 56/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1536 - accuracy: 0.9299 - val_loss: 0.1567 - val_accuracy: 0.9207\n",
      "Epoch 57/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1507 - accuracy: 0.9309 - val_loss: 0.1555 - val_accuracy: 0.9352\n",
      "Epoch 58/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1480 - accuracy: 0.9320 - val_loss: 0.1611 - val_accuracy: 0.9236\n",
      "Epoch 59/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1492 - accuracy: 0.9292 - val_loss: 0.1625 - val_accuracy: 0.9258\n",
      "Epoch 60/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1478 - accuracy: 0.9316 - val_loss: 0.1552 - val_accuracy: 0.9330\n",
      "Epoch 61/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1502 - accuracy: 0.9308 - val_loss: 0.1575 - val_accuracy: 0.9215\n",
      "Epoch 62/80\n",
      "12490/12490 [==============================] - 0s 34us/step - loss: 0.1483 - accuracy: 0.9323 - val_loss: 0.1643 - val_accuracy: 0.9236\n",
      "Epoch 63/80\n",
      "12490/12490 [==============================] - 0s 32us/step - loss: 0.1446 - accuracy: 0.9318 - val_loss: 0.1549 - val_accuracy: 0.9294\n",
      "Epoch 64/80\n",
      "12490/12490 [==============================] - 0s 34us/step - loss: 0.1445 - accuracy: 0.9315 - val_loss: 0.1516 - val_accuracy: 0.9236\n",
      "Epoch 65/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1444 - accuracy: 0.9320 - val_loss: 0.1708 - val_accuracy: 0.9229\n",
      "Epoch 66/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1459 - accuracy: 0.9322 - val_loss: 0.1504 - val_accuracy: 0.9236\n",
      "Epoch 67/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1483 - accuracy: 0.9300 - val_loss: 0.1773 - val_accuracy: 0.9179\n",
      "Epoch 68/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1477 - accuracy: 0.9300 - val_loss: 0.1565 - val_accuracy: 0.9179\n",
      "Epoch 69/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1460 - accuracy: 0.9323 - val_loss: 0.1686 - val_accuracy: 0.9287\n",
      "Epoch 70/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1430 - accuracy: 0.9329 - val_loss: 0.1579 - val_accuracy: 0.9215\n",
      "Epoch 71/80\n",
      "12490/12490 [==============================] - 0s 34us/step - loss: 0.1427 - accuracy: 0.9330 - val_loss: 0.1598 - val_accuracy: 0.9186\n",
      "Epoch 72/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1447 - accuracy: 0.9301 - val_loss: 0.1637 - val_accuracy: 0.9171\n",
      "Epoch 73/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1414 - accuracy: 0.9335 - val_loss: 0.1645 - val_accuracy: 0.9236\n",
      "Epoch 74/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1433 - accuracy: 0.9325 - val_loss: 0.1647 - val_accuracy: 0.9186\n",
      "Epoch 75/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1423 - accuracy: 0.9323 - val_loss: 0.1604 - val_accuracy: 0.9244\n",
      "Epoch 76/80\n",
      "12490/12490 [==============================] - 0s 37us/step - loss: 0.1440 - accuracy: 0.9311 - val_loss: 0.1517 - val_accuracy: 0.9330\n",
      "Epoch 00076: early stopping\n",
      "[[9.99999881e-01 1.08704164e-07 1.01842524e-14 5.01831302e-12\n",
      "  1.24812098e-08 7.09054093e-10]\n",
      " [1.89147177e-05 4.68900253e-04 1.77451316e-08 2.15727297e-07\n",
      "  3.43149790e-04 9.99168873e-01]\n",
      " [1.81947362e-05 4.59076313e-04 1.77540684e-08 2.12970519e-07\n",
      "  3.40421218e-04 9.99182045e-01]\n",
      " ...\n",
      " [1.01674311e-12 3.22776278e-10 9.99113023e-01 8.83396133e-04\n",
      "  1.02591942e-10 3.53886367e-06]\n",
      " [1.40158775e-07 1.40941947e-09 9.63613927e-01 4.06678952e-03\n",
      "  3.08853220e-02 1.43382081e-03]\n",
      " [5.59303517e-05 7.14464807e-11 5.68753592e-07 1.73502972e-08\n",
      "  9.99524713e-01 4.18827141e-04]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:02:46.430440\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12490 samples, validate on 1388 samples\n",
      "Epoch 1/80\n",
      "12490/12490 [==============================] - 1s 58us/step - loss: 0.8507 - accuracy: 0.6906 - val_loss: 0.4959 - val_accuracy: 0.7983\n",
      "Epoch 2/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.4517 - accuracy: 0.8050 - val_loss: 0.4385 - val_accuracy: 0.8300\n",
      "Epoch 3/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.3976 - accuracy: 0.8299 - val_loss: 0.3472 - val_accuracy: 0.8566\n",
      "Epoch 4/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.3517 - accuracy: 0.8535 - val_loss: 0.3081 - val_accuracy: 0.8746\n",
      "Epoch 5/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.3101 - accuracy: 0.8740 - val_loss: 0.2868 - val_accuracy: 0.8927\n",
      "Epoch 6/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.2766 - accuracy: 0.8903 - val_loss: 0.2494 - val_accuracy: 0.9006\n",
      "Epoch 7/80\n",
      "12490/12490 [==============================] - 1s 42us/step - loss: 0.2525 - accuracy: 0.8960 - val_loss: 0.2555 - val_accuracy: 0.8927\n",
      "Epoch 8/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2419 - accuracy: 0.9003 - val_loss: 0.2112 - val_accuracy: 0.9121\n",
      "Epoch 9/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.2313 - accuracy: 0.8995 - val_loss: 0.2201 - val_accuracy: 0.9049\n",
      "Epoch 10/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.2161 - accuracy: 0.9041 - val_loss: 0.2062 - val_accuracy: 0.9092\n",
      "Epoch 11/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2145 - accuracy: 0.9065 - val_loss: 0.1977 - val_accuracy: 0.9049\n",
      "Epoch 12/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.2084 - accuracy: 0.9085 - val_loss: 0.1995 - val_accuracy: 0.9143\n",
      "Epoch 13/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.2055 - accuracy: 0.9072 - val_loss: 0.1981 - val_accuracy: 0.9164\n",
      "Epoch 14/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2022 - accuracy: 0.9086 - val_loss: 0.2091 - val_accuracy: 0.9107\n",
      "Epoch 15/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.2020 - accuracy: 0.9088 - val_loss: 0.1912 - val_accuracy: 0.9027\n",
      "Epoch 16/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1980 - accuracy: 0.9093 - val_loss: 0.1817 - val_accuracy: 0.9121\n",
      "Epoch 17/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1964 - accuracy: 0.9092 - val_loss: 0.1905 - val_accuracy: 0.9078\n",
      "Epoch 18/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1935 - accuracy: 0.9099 - val_loss: 0.1923 - val_accuracy: 0.9171\n",
      "Epoch 19/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1909 - accuracy: 0.9110 - val_loss: 0.1895 - val_accuracy: 0.9042\n",
      "Epoch 20/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1974 - accuracy: 0.9095 - val_loss: 0.1841 - val_accuracy: 0.9071\n",
      "Epoch 21/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1927 - accuracy: 0.9091 - val_loss: 0.1874 - val_accuracy: 0.9164\n",
      "Epoch 22/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1902 - accuracy: 0.9099 - val_loss: 0.1812 - val_accuracy: 0.9121\n",
      "Epoch 23/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1864 - accuracy: 0.9118 - val_loss: 0.1877 - val_accuracy: 0.9121\n",
      "Epoch 24/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1874 - accuracy: 0.9114 - val_loss: 0.1794 - val_accuracy: 0.9107\n",
      "Epoch 25/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1865 - accuracy: 0.9095 - val_loss: 0.1768 - val_accuracy: 0.9071\n",
      "Epoch 26/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1841 - accuracy: 0.9143 - val_loss: 0.1819 - val_accuracy: 0.9200\n",
      "Epoch 27/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1789 - accuracy: 0.9144 - val_loss: 0.1847 - val_accuracy: 0.9063\n",
      "Epoch 28/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1853 - accuracy: 0.9106 - val_loss: 0.1766 - val_accuracy: 0.9157\n",
      "Epoch 29/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1851 - accuracy: 0.9102 - val_loss: 0.2092 - val_accuracy: 0.8984\n",
      "Epoch 30/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1866 - accuracy: 0.9111 - val_loss: 0.1999 - val_accuracy: 0.9020\n",
      "Epoch 31/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1788 - accuracy: 0.9126 - val_loss: 0.1717 - val_accuracy: 0.9164\n",
      "Epoch 32/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1766 - accuracy: 0.9149 - val_loss: 0.1736 - val_accuracy: 0.9186\n",
      "Epoch 33/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1788 - accuracy: 0.9130 - val_loss: 0.1989 - val_accuracy: 0.9056\n",
      "Epoch 34/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1770 - accuracy: 0.9138 - val_loss: 0.1714 - val_accuracy: 0.9222\n",
      "Epoch 35/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1722 - accuracy: 0.9141 - val_loss: 0.1902 - val_accuracy: 0.9042\n",
      "Epoch 36/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1740 - accuracy: 0.9140 - val_loss: 0.1760 - val_accuracy: 0.9179\n",
      "Epoch 37/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1722 - accuracy: 0.9163 - val_loss: 0.1696 - val_accuracy: 0.9128\n",
      "Epoch 38/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1695 - accuracy: 0.9174 - val_loss: 0.1697 - val_accuracy: 0.9179\n",
      "Epoch 39/80\n",
      "12490/12490 [==============================] - 0s 36us/step - loss: 0.1704 - accuracy: 0.9173 - val_loss: 0.1773 - val_accuracy: 0.9200\n",
      "Epoch 40/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1737 - accuracy: 0.9161 - val_loss: 0.1692 - val_accuracy: 0.9200\n",
      "Epoch 41/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1690 - accuracy: 0.9170 - val_loss: 0.1815 - val_accuracy: 0.9049\n",
      "Epoch 42/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1740 - accuracy: 0.9175 - val_loss: 0.1746 - val_accuracy: 0.9244\n",
      "Epoch 43/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1680 - accuracy: 0.9203 - val_loss: 0.1721 - val_accuracy: 0.9143\n",
      "Epoch 44/80\n",
      "12490/12490 [==============================] - 0s 35us/step - loss: 0.1652 - accuracy: 0.9208 - val_loss: 0.1705 - val_accuracy: 0.9186\n",
      "Epoch 45/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1689 - accuracy: 0.9199 - val_loss: 0.1675 - val_accuracy: 0.9128\n",
      "Epoch 46/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1667 - accuracy: 0.9213 - val_loss: 0.1668 - val_accuracy: 0.9157\n",
      "Epoch 47/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1641 - accuracy: 0.9248 - val_loss: 0.1657 - val_accuracy: 0.9193\n",
      "Epoch 48/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1624 - accuracy: 0.9234 - val_loss: 0.1652 - val_accuracy: 0.9200\n",
      "Epoch 49/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1613 - accuracy: 0.9253 - val_loss: 0.1688 - val_accuracy: 0.9193\n",
      "Epoch 50/80\n",
      "12490/12490 [==============================] - 1s 40us/step - loss: 0.1634 - accuracy: 0.9251 - val_loss: 0.1638 - val_accuracy: 0.9207\n",
      "Epoch 51/80\n",
      "12490/12490 [==============================] - 1s 43us/step - loss: 0.1645 - accuracy: 0.9235 - val_loss: 0.1592 - val_accuracy: 0.9222\n",
      "Epoch 52/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1644 - accuracy: 0.9219 - val_loss: 0.1632 - val_accuracy: 0.9229\n",
      "Epoch 53/80\n",
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1613 - accuracy: 0.9241 - val_loss: 0.1676 - val_accuracy: 0.9193\n",
      "Epoch 54/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1614 - accuracy: 0.9250 - val_loss: 0.1609 - val_accuracy: 0.9229\n",
      "Epoch 55/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1609 - accuracy: 0.9263 - val_loss: 0.1636 - val_accuracy: 0.9222\n",
      "Epoch 56/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12490/12490 [==============================] - 0s 38us/step - loss: 0.1601 - accuracy: 0.9250 - val_loss: 0.1638 - val_accuracy: 0.9207\n",
      "Epoch 57/80\n",
      "12490/12490 [==============================] - 1s 41us/step - loss: 0.1617 - accuracy: 0.9247 - val_loss: 0.1718 - val_accuracy: 0.9287\n",
      "Epoch 58/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1634 - accuracy: 0.9263 - val_loss: 0.1703 - val_accuracy: 0.9135\n",
      "Epoch 59/80\n",
      "12490/12490 [==============================] - 0s 40us/step - loss: 0.1596 - accuracy: 0.9253 - val_loss: 0.1635 - val_accuracy: 0.9200\n",
      "Epoch 60/80\n",
      "12490/12490 [==============================] - 0s 39us/step - loss: 0.1670 - accuracy: 0.9251 - val_loss: 0.1687 - val_accuracy: 0.9207\n",
      "Epoch 61/80\n",
      "12490/12490 [==============================] - 1s 45us/step - loss: 0.1598 - accuracy: 0.9276 - val_loss: 0.1709 - val_accuracy: 0.9294\n",
      "Epoch 00061: early stopping\n",
      "[[9.9999905e-01 6.7247606e-07 6.0313162e-08 1.4248860e-07 1.2187783e-10\n",
      "  6.1978338e-09]\n",
      " [3.3648263e-05 4.6599102e-03 3.3945013e-05 2.7611079e-05 1.0519527e-03\n",
      "  9.9419290e-01]\n",
      " [3.2905176e-05 4.6035023e-03 3.3664463e-05 2.7266056e-05 1.0475754e-03\n",
      "  9.9425513e-01]\n",
      " ...\n",
      " [1.0384459e-06 3.2653400e-08 9.8962969e-01 1.0362544e-02 2.2552520e-12\n",
      "  6.7649548e-06]\n",
      " [2.6439497e-05 2.1812116e-07 9.8860210e-01 7.6776505e-03 3.1887821e-03\n",
      "  5.0493644e-04]\n",
      " [1.3979972e-06 1.9095001e-08 1.8879551e-05 1.1577811e-07 9.9869508e-01\n",
      "  1.2845842e-03]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:03:20.852257\n",
      "region_label= [6, 6, 2, 4, 3, 5]\n",
      "dist_table_truth\n",
      " [[1320  845    0 1474   29  306    0    0    0]\n",
      " [ 618   44    3  126  270    2    0    0    0]\n",
      " [ 166  523 2197   25 1285  573    0    0    0]\n",
      " [ 164 1271 2233    0 1425  237    0    0    0]\n",
      " [2289  557   12 2469   38 1471    0    0    0]\n",
      " [ 135  588   13   82  227 1718    0    0    0]]\n",
      "num of merged_region_image 0 2216\n",
      "num of merged_region_image 1 2572\n",
      "num of merged_region_image 2 3233\n",
      "num of merged_region_image 3 2609\n",
      "num of merged_region_image 4 1344\n",
      "num of merged_region_image 5 1904\n",
      "Counter({-1: 29339, 2: 3233, 3: 2609, 1: 2572, 0: 2216, 5: 1904, 4: 1344})\n",
      "===========  ITE = 0   ===========\n",
      "used_img 13878 13878\n",
      "working_img(=other images=unclean images) 29339 29339\n",
      "merged regions 58 58\n",
      "other_regions 142 142\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>195</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.57</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>142 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     other index  pred label  truth  rate    0    1  2    3  4    5\n",
       "0              2           5      6  0.90    0    0  0    0  0    0\n",
       "1              3           4      3  1.00    1    0  0  183  2    0\n",
       "2              4           1      0  1.00  161    0  2    3  0    2\n",
       "3              6           5      1  0.87    4   62  0    0  1    2\n",
       "4              7           3      5  0.93    1    0  0    0  3  239\n",
       "..           ...         ...    ...   ...  ...  ... ..  ... ..  ...\n",
       "137          194           0      1  1.00    1  111  0    0  0    2\n",
       "138          195           4      1  0.57    2   21  0    0  0   17\n",
       "139          196           4      0  0.70  372    2  1    1  4    1\n",
       "140          199           2      8  0.89    0    0  2    0  0    0\n",
       "141          200           4      0  1.00  260    0  0    0  1    0\n",
       "\n",
       "[142 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [129, 100, 185, 162, 159] 923\n",
      "added label, regions, img amount: {1} [4, 74, 119, 23, 103] 628\n",
      "added label, regions, img amount: {2} [124, 181, 47, 168, 65] 564\n",
      "added label, regions, img amount: {3} [110, 54, 148, 114, 108] 821\n",
      "added label, regions, img amount: {4} [3, 98, 167, 165, 151] 875\n",
      "added label, regions, img amount: {5} [107, 140, 62, 2, 150] 722\n",
      "Not getting into residuals\n",
      "NUM_region 6\n",
      "number of clean images 18411\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 6\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16569 samples, validate on 1842 samples\n",
      "Epoch 1/80\n",
      "16569/16569 [==============================] - 1s 71us/step - loss: 0.6907 - accuracy: 0.7309 - val_loss: 0.4207 - val_accuracy: 0.8198\n",
      "Epoch 2/80\n",
      "16569/16569 [==============================] - 1s 43us/step - loss: 0.3289 - accuracy: 0.8733 - val_loss: 0.2616 - val_accuracy: 0.9104\n",
      "Epoch 3/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.2255 - accuracy: 0.9158 - val_loss: 0.2089 - val_accuracy: 0.9159\n",
      "Epoch 4/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1892 - accuracy: 0.9236 - val_loss: 0.1956 - val_accuracy: 0.9202\n",
      "Epoch 5/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1756 - accuracy: 0.9268 - val_loss: 0.1747 - val_accuracy: 0.9343\n",
      "Epoch 6/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1668 - accuracy: 0.9316 - val_loss: 0.1573 - val_accuracy: 0.9289\n",
      "Epoch 7/80\n",
      "16569/16569 [==============================] - 1s 42us/step - loss: 0.1596 - accuracy: 0.9298 - val_loss: 0.1548 - val_accuracy: 0.9289\n",
      "Epoch 8/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1594 - accuracy: 0.9293 - val_loss: 0.1469 - val_accuracy: 0.9414\n",
      "Epoch 9/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1483 - accuracy: 0.9326 - val_loss: 0.1485 - val_accuracy: 0.9370\n",
      "Epoch 10/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1506 - accuracy: 0.9334 - val_loss: 0.1445 - val_accuracy: 0.9370\n",
      "Epoch 11/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1525 - accuracy: 0.9343 - val_loss: 0.1486 - val_accuracy: 0.9343\n",
      "Epoch 12/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1444 - accuracy: 0.9345 - val_loss: 0.1530 - val_accuracy: 0.9311\n",
      "Epoch 13/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1438 - accuracy: 0.9366 - val_loss: 0.1378 - val_accuracy: 0.9376\n",
      "Epoch 14/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1395 - accuracy: 0.9351 - val_loss: 0.1345 - val_accuracy: 0.9376\n",
      "Epoch 15/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1426 - accuracy: 0.9340 - val_loss: 0.1469 - val_accuracy: 0.9240\n",
      "Epoch 16/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1399 - accuracy: 0.9349 - val_loss: 0.1338 - val_accuracy: 0.9327\n",
      "Epoch 17/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1373 - accuracy: 0.9354 - val_loss: 0.1364 - val_accuracy: 0.9327\n",
      "Epoch 18/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1404 - accuracy: 0.9361 - val_loss: 0.1344 - val_accuracy: 0.9430\n",
      "Epoch 19/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1352 - accuracy: 0.9381 - val_loss: 0.1307 - val_accuracy: 0.9397\n",
      "Epoch 20/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1400 - accuracy: 0.9352 - val_loss: 0.1373 - val_accuracy: 0.9435\n",
      "Epoch 21/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1362 - accuracy: 0.9357 - val_loss: 0.1255 - val_accuracy: 0.9397\n",
      "Epoch 22/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1350 - accuracy: 0.9376 - val_loss: 0.1265 - val_accuracy: 0.9419\n",
      "Epoch 23/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1324 - accuracy: 0.9390 - val_loss: 0.1270 - val_accuracy: 0.9387\n",
      "Epoch 24/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1323 - accuracy: 0.9397 - val_loss: 0.1424 - val_accuracy: 0.9435\n",
      "Epoch 25/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1316 - accuracy: 0.9396 - val_loss: 0.1248 - val_accuracy: 0.9463\n",
      "Epoch 26/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1286 - accuracy: 0.9406 - val_loss: 0.1228 - val_accuracy: 0.9452\n",
      "Epoch 27/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1269 - accuracy: 0.9425 - val_loss: 0.1418 - val_accuracy: 0.9251\n",
      "Epoch 28/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1304 - accuracy: 0.9421 - val_loss: 0.1231 - val_accuracy: 0.9419\n",
      "Epoch 29/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1264 - accuracy: 0.9431 - val_loss: 0.1321 - val_accuracy: 0.9414\n",
      "Epoch 30/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1298 - accuracy: 0.9422 - val_loss: 0.1283 - val_accuracy: 0.9425\n",
      "Epoch 31/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1259 - accuracy: 0.9442 - val_loss: 0.1164 - val_accuracy: 0.9457\n",
      "Epoch 32/80\n",
      "16569/16569 [==============================] - 1s 34us/step - loss: 0.1243 - accuracy: 0.9442 - val_loss: 0.1161 - val_accuracy: 0.9495\n",
      "Epoch 33/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1288 - accuracy: 0.9432 - val_loss: 0.1274 - val_accuracy: 0.9435\n",
      "Epoch 34/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1264 - accuracy: 0.9438 - val_loss: 0.1170 - val_accuracy: 0.9473\n",
      "Epoch 35/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1256 - accuracy: 0.9442 - val_loss: 0.1287 - val_accuracy: 0.9495\n",
      "Epoch 36/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1256 - accuracy: 0.9436 - val_loss: 0.1266 - val_accuracy: 0.9495\n",
      "Epoch 37/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1255 - accuracy: 0.9456 - val_loss: 0.1309 - val_accuracy: 0.9403\n",
      "Epoch 38/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1249 - accuracy: 0.9448 - val_loss: 0.1149 - val_accuracy: 0.9511\n",
      "Epoch 39/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1272 - accuracy: 0.9448 - val_loss: 0.1187 - val_accuracy: 0.9479\n",
      "Epoch 40/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1243 - accuracy: 0.9454 - val_loss: 0.1159 - val_accuracy: 0.9495\n",
      "Epoch 41/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1242 - accuracy: 0.9454 - val_loss: 0.1172 - val_accuracy: 0.9501\n",
      "Epoch 42/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1213 - accuracy: 0.9481 - val_loss: 0.1128 - val_accuracy: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1216 - accuracy: 0.9465 - val_loss: 0.1141 - val_accuracy: 0.9517\n",
      "Epoch 44/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1219 - accuracy: 0.9457 - val_loss: 0.1247 - val_accuracy: 0.9463\n",
      "Epoch 45/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1218 - accuracy: 0.9476 - val_loss: 0.1202 - val_accuracy: 0.9435\n",
      "Epoch 46/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1227 - accuracy: 0.9453 - val_loss: 0.1116 - val_accuracy: 0.9490\n",
      "Epoch 47/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1201 - accuracy: 0.9466 - val_loss: 0.1182 - val_accuracy: 0.9490\n",
      "Epoch 48/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1204 - accuracy: 0.9457 - val_loss: 0.1161 - val_accuracy: 0.9452\n",
      "Epoch 49/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1194 - accuracy: 0.9463 - val_loss: 0.1174 - val_accuracy: 0.9419\n",
      "Epoch 50/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1197 - accuracy: 0.9483 - val_loss: 0.1113 - val_accuracy: 0.9501\n",
      "Epoch 51/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1182 - accuracy: 0.9466 - val_loss: 0.1093 - val_accuracy: 0.9517\n",
      "Epoch 52/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1197 - accuracy: 0.9460 - val_loss: 0.1149 - val_accuracy: 0.9446\n",
      "Epoch 53/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1174 - accuracy: 0.9469 - val_loss: 0.1170 - val_accuracy: 0.9452\n",
      "Epoch 54/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1192 - accuracy: 0.9468 - val_loss: 0.1150 - val_accuracy: 0.9484\n",
      "Epoch 55/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1167 - accuracy: 0.9472 - val_loss: 0.1148 - val_accuracy: 0.9463\n",
      "Epoch 56/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1163 - accuracy: 0.9469 - val_loss: 0.1106 - val_accuracy: 0.9490\n",
      "Epoch 57/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1182 - accuracy: 0.9470 - val_loss: 0.1187 - val_accuracy: 0.9495\n",
      "Epoch 58/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1172 - accuracy: 0.9470 - val_loss: 0.1122 - val_accuracy: 0.9495\n",
      "Epoch 59/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1158 - accuracy: 0.9472 - val_loss: 0.1110 - val_accuracy: 0.9511\n",
      "Epoch 60/80\n",
      "16569/16569 [==============================] - 1s 42us/step - loss: 0.1167 - accuracy: 0.9480 - val_loss: 0.1169 - val_accuracy: 0.9463\n",
      "Epoch 61/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1157 - accuracy: 0.9472 - val_loss: 0.1127 - val_accuracy: 0.9484\n",
      "Epoch 00061: early stopping\n",
      "[[1.0000000e+00 1.4320570e-11 5.9663496e-10 1.7656674e-12 7.5298291e-13\n",
      "  4.8704400e-13]\n",
      " [3.0361569e-05 1.9594371e-04 5.3977453e-07 1.8027780e-08 3.0211548e-05\n",
      "  9.9974293e-01]\n",
      " [2.9598681e-05 1.9307576e-04 5.3288750e-07 1.7817072e-08 3.0069066e-05\n",
      "  9.9974674e-01]\n",
      " ...\n",
      " [1.8534975e-09 5.3253761e-08 9.9594009e-01 4.0594456e-03 1.4344444e-14\n",
      "  4.2961130e-07]\n",
      " [1.7339557e-05 3.8578376e-09 9.9551117e-01 1.3200805e-03 1.3426666e-03\n",
      "  1.8087017e-03]\n",
      " [2.7539434e-03 2.4869898e-10 2.4261597e-05 5.6618097e-09 9.9709237e-01\n",
      "  1.2939142e-04]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:00:44.937443\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16569 samples, validate on 1842 samples\n",
      "Epoch 1/80\n",
      "16569/16569 [==============================] - 1s 52us/step - loss: 0.7127 - accuracy: 0.7350 - val_loss: 0.4291 - val_accuracy: 0.8426\n",
      "Epoch 2/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.3307 - accuracy: 0.8812 - val_loss: 0.2553 - val_accuracy: 0.9023\n",
      "Epoch 3/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.2233 - accuracy: 0.9136 - val_loss: 0.2209 - val_accuracy: 0.9148\n",
      "Epoch 4/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1870 - accuracy: 0.9249 - val_loss: 0.1828 - val_accuracy: 0.9300\n",
      "Epoch 5/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1753 - accuracy: 0.9281 - val_loss: 0.1758 - val_accuracy: 0.9311\n",
      "Epoch 6/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1709 - accuracy: 0.9285 - val_loss: 0.1760 - val_accuracy: 0.9175\n",
      "Epoch 7/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1597 - accuracy: 0.9317 - val_loss: 0.1651 - val_accuracy: 0.9240\n",
      "Epoch 8/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1585 - accuracy: 0.9317 - val_loss: 0.1591 - val_accuracy: 0.9273\n",
      "Epoch 9/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1592 - accuracy: 0.9304 - val_loss: 0.1575 - val_accuracy: 0.9262\n",
      "Epoch 10/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1502 - accuracy: 0.9326 - val_loss: 0.1545 - val_accuracy: 0.9278\n",
      "Epoch 11/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1534 - accuracy: 0.9338 - val_loss: 0.1583 - val_accuracy: 0.9338\n",
      "Epoch 12/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1464 - accuracy: 0.9341 - val_loss: 0.1493 - val_accuracy: 0.9316\n",
      "Epoch 13/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1451 - accuracy: 0.9350 - val_loss: 0.1517 - val_accuracy: 0.9267\n",
      "Epoch 14/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1457 - accuracy: 0.9346 - val_loss: 0.1464 - val_accuracy: 0.9359\n",
      "Epoch 15/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1425 - accuracy: 0.9351 - val_loss: 0.1433 - val_accuracy: 0.9365\n",
      "Epoch 16/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1432 - accuracy: 0.9354 - val_loss: 0.1408 - val_accuracy: 0.9321\n",
      "Epoch 17/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1389 - accuracy: 0.9355 - val_loss: 0.1453 - val_accuracy: 0.9267\n",
      "Epoch 18/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1424 - accuracy: 0.9332 - val_loss: 0.1487 - val_accuracy: 0.9262\n",
      "Epoch 19/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1396 - accuracy: 0.9361 - val_loss: 0.1394 - val_accuracy: 0.9370\n",
      "Epoch 20/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1381 - accuracy: 0.9358 - val_loss: 0.1379 - val_accuracy: 0.9289\n",
      "Epoch 21/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1364 - accuracy: 0.9367 - val_loss: 0.1617 - val_accuracy: 0.9343\n",
      "Epoch 22/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1356 - accuracy: 0.9354 - val_loss: 0.1415 - val_accuracy: 0.9311\n",
      "Epoch 23/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1361 - accuracy: 0.9342 - val_loss: 0.1376 - val_accuracy: 0.9349\n",
      "Epoch 24/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1352 - accuracy: 0.9367 - val_loss: 0.1506 - val_accuracy: 0.9327\n",
      "Epoch 25/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1373 - accuracy: 0.9364 - val_loss: 0.1320 - val_accuracy: 0.9354\n",
      "Epoch 26/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1338 - accuracy: 0.9380 - val_loss: 0.1437 - val_accuracy: 0.9365\n",
      "Epoch 27/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1320 - accuracy: 0.9371 - val_loss: 0.1340 - val_accuracy: 0.9343\n",
      "Epoch 28/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1322 - accuracy: 0.9388 - val_loss: 0.1482 - val_accuracy: 0.9365\n",
      "Epoch 29/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1318 - accuracy: 0.9366 - val_loss: 0.1370 - val_accuracy: 0.9365\n",
      "Epoch 30/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1318 - accuracy: 0.9392 - val_loss: 0.1352 - val_accuracy: 0.9419\n",
      "Epoch 31/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1296 - accuracy: 0.9397 - val_loss: 0.1280 - val_accuracy: 0.9397\n",
      "Epoch 32/80\n",
      "16569/16569 [==============================] - 1s 35us/step - loss: 0.1296 - accuracy: 0.9389 - val_loss: 0.1288 - val_accuracy: 0.9446\n",
      "Epoch 33/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1288 - accuracy: 0.9402 - val_loss: 0.1252 - val_accuracy: 0.9435\n",
      "Epoch 34/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1295 - accuracy: 0.9396 - val_loss: 0.1308 - val_accuracy: 0.9403\n",
      "Epoch 35/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1280 - accuracy: 0.9410 - val_loss: 0.1270 - val_accuracy: 0.9435\n",
      "Epoch 36/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1289 - accuracy: 0.9409 - val_loss: 0.1355 - val_accuracy: 0.9381\n",
      "Epoch 37/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1262 - accuracy: 0.9415 - val_loss: 0.1302 - val_accuracy: 0.9397\n",
      "Epoch 38/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1239 - accuracy: 0.9427 - val_loss: 0.1221 - val_accuracy: 0.9473\n",
      "Epoch 39/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1231 - accuracy: 0.9427 - val_loss: 0.1191 - val_accuracy: 0.9495\n",
      "Epoch 40/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1249 - accuracy: 0.9429 - val_loss: 0.1197 - val_accuracy: 0.9495\n",
      "Epoch 41/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1247 - accuracy: 0.9436 - val_loss: 0.1202 - val_accuracy: 0.9528\n",
      "Epoch 42/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1209 - accuracy: 0.9433 - val_loss: 0.1182 - val_accuracy: 0.9479\n",
      "Epoch 43/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1207 - accuracy: 0.9443 - val_loss: 0.1202 - val_accuracy: 0.9446\n",
      "Epoch 44/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1224 - accuracy: 0.9441 - val_loss: 0.1186 - val_accuracy: 0.9463\n",
      "Epoch 45/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1209 - accuracy: 0.9444 - val_loss: 0.1235 - val_accuracy: 0.9479\n",
      "Epoch 46/80\n",
      "16569/16569 [==============================] - 1s 35us/step - loss: 0.1219 - accuracy: 0.9448 - val_loss: 0.1246 - val_accuracy: 0.9484\n",
      "Epoch 47/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1225 - accuracy: 0.9450 - val_loss: 0.1179 - val_accuracy: 0.9490\n",
      "Epoch 48/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1210 - accuracy: 0.9461 - val_loss: 0.1207 - val_accuracy: 0.9457\n",
      "Epoch 49/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1196 - accuracy: 0.9455 - val_loss: 0.1221 - val_accuracy: 0.9441\n",
      "Epoch 50/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1240 - accuracy: 0.9441 - val_loss: 0.1193 - val_accuracy: 0.9517\n",
      "Epoch 51/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1225 - accuracy: 0.9451 - val_loss: 0.1179 - val_accuracy: 0.9511\n",
      "Epoch 52/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1210 - accuracy: 0.9445 - val_loss: 0.1214 - val_accuracy: 0.9457\n",
      "Epoch 53/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1201 - accuracy: 0.9459 - val_loss: 0.1138 - val_accuracy: 0.9511\n",
      "Epoch 54/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1194 - accuracy: 0.9457 - val_loss: 0.1167 - val_accuracy: 0.9495\n",
      "Epoch 55/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1178 - accuracy: 0.9468 - val_loss: 0.1175 - val_accuracy: 0.9484\n",
      "Epoch 56/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1183 - accuracy: 0.9459 - val_loss: 0.1244 - val_accuracy: 0.9419\n",
      "Epoch 57/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1184 - accuracy: 0.9468 - val_loss: 0.1235 - val_accuracy: 0.9430\n",
      "Epoch 58/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1200 - accuracy: 0.9457 - val_loss: 0.1151 - val_accuracy: 0.9463\n",
      "Epoch 59/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1171 - accuracy: 0.9472 - val_loss: 0.1174 - val_accuracy: 0.9528\n",
      "Epoch 60/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1185 - accuracy: 0.9454 - val_loss: 0.1163 - val_accuracy: 0.9517\n",
      "Epoch 61/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1161 - accuracy: 0.9465 - val_loss: 0.1216 - val_accuracy: 0.9463\n",
      "Epoch 62/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1203 - accuracy: 0.9459 - val_loss: 0.1154 - val_accuracy: 0.9522\n",
      "Epoch 63/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1166 - accuracy: 0.9471 - val_loss: 0.1159 - val_accuracy: 0.9528\n",
      "Epoch 00063: early stopping\n",
      "[[9.9999988e-01 9.5742813e-08 5.3429614e-17 9.8124391e-12 8.1970286e-10\n",
      "  7.5531956e-11]\n",
      " [5.8650410e-05 5.2771857e-04 1.1693423e-09 7.9219404e-09 1.8478966e-05\n",
      "  9.9939513e-01]\n",
      " [5.7600486e-05 5.1926391e-04 1.1733347e-09 7.8341431e-09 1.8424465e-05\n",
      "  9.9940467e-01]\n",
      " ...\n",
      " [6.9633792e-09 2.6622976e-07 9.9145877e-01 8.5409116e-03 4.4168275e-15\n",
      "  2.6895895e-08]\n",
      " [1.4013688e-05 1.1566553e-08 9.9715459e-01 4.1836288e-04 2.2907732e-03\n",
      "  1.2234708e-04]\n",
      " [1.7795904e-03 2.6641725e-11 9.9999786e-10 1.7400133e-11 9.9821883e-01\n",
      "  1.5833211e-06]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:01:28.735313\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16569 samples, validate on 1842 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16569/16569 [==============================] - 1s 52us/step - loss: 0.7012 - accuracy: 0.7156 - val_loss: 0.4181 - val_accuracy: 0.8469\n",
      "Epoch 2/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.3319 - accuracy: 0.8792 - val_loss: 0.2604 - val_accuracy: 0.9001\n",
      "Epoch 3/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.2310 - accuracy: 0.9139 - val_loss: 0.2222 - val_accuracy: 0.9137\n",
      "Epoch 4/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1941 - accuracy: 0.9232 - val_loss: 0.1818 - val_accuracy: 0.9267\n",
      "Epoch 5/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1764 - accuracy: 0.9288 - val_loss: 0.1701 - val_accuracy: 0.9289\n",
      "Epoch 6/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1677 - accuracy: 0.9325 - val_loss: 0.1932 - val_accuracy: 0.9240\n",
      "Epoch 7/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1605 - accuracy: 0.9347 - val_loss: 0.1877 - val_accuracy: 0.9256\n",
      "Epoch 8/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1577 - accuracy: 0.9361 - val_loss: 0.1529 - val_accuracy: 0.9408\n",
      "Epoch 9/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1477 - accuracy: 0.9374 - val_loss: 0.1490 - val_accuracy: 0.9381\n",
      "Epoch 10/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1424 - accuracy: 0.9383 - val_loss: 0.1415 - val_accuracy: 0.9370\n",
      "Epoch 11/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1404 - accuracy: 0.9387 - val_loss: 0.1492 - val_accuracy: 0.9321\n",
      "Epoch 12/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1391 - accuracy: 0.9395 - val_loss: 0.1382 - val_accuracy: 0.9392\n",
      "Epoch 13/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1398 - accuracy: 0.9396 - val_loss: 0.1452 - val_accuracy: 0.9359\n",
      "Epoch 14/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1347 - accuracy: 0.9401 - val_loss: 0.1388 - val_accuracy: 0.9349\n",
      "Epoch 15/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1321 - accuracy: 0.9411 - val_loss: 0.1301 - val_accuracy: 0.9359\n",
      "Epoch 16/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1283 - accuracy: 0.9416 - val_loss: 0.1448 - val_accuracy: 0.9327\n",
      "Epoch 17/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1295 - accuracy: 0.9407 - val_loss: 0.1420 - val_accuracy: 0.9343\n",
      "Epoch 18/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1289 - accuracy: 0.9414 - val_loss: 0.1289 - val_accuracy: 0.9381\n",
      "Epoch 19/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1259 - accuracy: 0.9418 - val_loss: 0.1294 - val_accuracy: 0.9430\n",
      "Epoch 20/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1252 - accuracy: 0.9424 - val_loss: 0.1241 - val_accuracy: 0.9425\n",
      "Epoch 21/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1232 - accuracy: 0.9422 - val_loss: 0.1268 - val_accuracy: 0.9441\n",
      "Epoch 22/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1219 - accuracy: 0.9436 - val_loss: 0.1291 - val_accuracy: 0.9403\n",
      "Epoch 23/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1221 - accuracy: 0.9421 - val_loss: 0.1231 - val_accuracy: 0.9457\n",
      "Epoch 24/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1207 - accuracy: 0.9445 - val_loss: 0.1320 - val_accuracy: 0.9425\n",
      "Epoch 25/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1209 - accuracy: 0.9433 - val_loss: 0.1305 - val_accuracy: 0.9359\n",
      "Epoch 26/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1229 - accuracy: 0.9416 - val_loss: 0.1267 - val_accuracy: 0.9435\n",
      "Epoch 27/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1189 - accuracy: 0.9439 - val_loss: 0.1248 - val_accuracy: 0.9425\n",
      "Epoch 28/80\n",
      "16569/16569 [==============================] - 1s 43us/step - loss: 0.1201 - accuracy: 0.9426 - val_loss: 0.1264 - val_accuracy: 0.9441\n",
      "Epoch 29/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1188 - accuracy: 0.9445 - val_loss: 0.1227 - val_accuracy: 0.9490\n",
      "Epoch 30/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1154 - accuracy: 0.9458 - val_loss: 0.1249 - val_accuracy: 0.9408\n",
      "Epoch 31/80\n",
      "16569/16569 [==============================] - 1s 35us/step - loss: 0.1198 - accuracy: 0.9427 - val_loss: 0.1299 - val_accuracy: 0.9387\n",
      "Epoch 32/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1182 - accuracy: 0.9434 - val_loss: 0.1187 - val_accuracy: 0.9490\n",
      "Epoch 33/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1156 - accuracy: 0.9448 - val_loss: 0.1184 - val_accuracy: 0.9457\n",
      "Epoch 34/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1174 - accuracy: 0.9439 - val_loss: 0.1194 - val_accuracy: 0.9419\n",
      "Epoch 35/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1145 - accuracy: 0.9444 - val_loss: 0.1223 - val_accuracy: 0.9463\n",
      "Epoch 36/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1149 - accuracy: 0.9461 - val_loss: 0.1151 - val_accuracy: 0.9479\n",
      "Epoch 37/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1166 - accuracy: 0.9448 - val_loss: 0.1177 - val_accuracy: 0.9484\n",
      "Epoch 38/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1140 - accuracy: 0.9455 - val_loss: 0.1198 - val_accuracy: 0.9414\n",
      "Epoch 39/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1146 - accuracy: 0.9460 - val_loss: 0.1215 - val_accuracy: 0.9457\n",
      "Epoch 40/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1147 - accuracy: 0.9443 - val_loss: 0.1311 - val_accuracy: 0.9349\n",
      "Epoch 41/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1138 - accuracy: 0.9464 - val_loss: 0.1224 - val_accuracy: 0.9430\n",
      "Epoch 42/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1147 - accuracy: 0.9448 - val_loss: 0.1184 - val_accuracy: 0.9414\n",
      "Epoch 43/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1096 - accuracy: 0.9480 - val_loss: 0.1141 - val_accuracy: 0.9501\n",
      "Epoch 44/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1125 - accuracy: 0.9459 - val_loss: 0.1277 - val_accuracy: 0.9321\n",
      "Epoch 45/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1110 - accuracy: 0.9476 - val_loss: 0.1134 - val_accuracy: 0.9501\n",
      "Epoch 46/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1110 - accuracy: 0.9459 - val_loss: 0.1307 - val_accuracy: 0.9343\n",
      "Epoch 47/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1147 - accuracy: 0.9460 - val_loss: 0.1144 - val_accuracy: 0.9468\n",
      "Epoch 48/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1094 - accuracy: 0.9477 - val_loss: 0.1183 - val_accuracy: 0.9452\n",
      "Epoch 49/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1099 - accuracy: 0.9483 - val_loss: 0.1155 - val_accuracy: 0.9473\n",
      "Epoch 50/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1105 - accuracy: 0.9459 - val_loss: 0.1148 - val_accuracy: 0.9501\n",
      "Epoch 51/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1122 - accuracy: 0.9471 - val_loss: 0.1282 - val_accuracy: 0.9343\n",
      "Epoch 52/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1111 - accuracy: 0.9462 - val_loss: 0.1163 - val_accuracy: 0.9414\n",
      "Epoch 53/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1103 - accuracy: 0.9473 - val_loss: 0.1119 - val_accuracy: 0.9528\n",
      "Epoch 54/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1087 - accuracy: 0.9477 - val_loss: 0.1322 - val_accuracy: 0.9403\n",
      "Epoch 55/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1103 - accuracy: 0.9468 - val_loss: 0.1268 - val_accuracy: 0.9332\n",
      "Epoch 56/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1091 - accuracy: 0.9491 - val_loss: 0.1127 - val_accuracy: 0.9490\n",
      "Epoch 57/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1090 - accuracy: 0.9485 - val_loss: 0.1144 - val_accuracy: 0.9414\n",
      "Epoch 58/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1099 - accuracy: 0.9468 - val_loss: 0.1198 - val_accuracy: 0.9425\n",
      "Epoch 59/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1099 - accuracy: 0.9460 - val_loss: 0.1211 - val_accuracy: 0.9392\n",
      "Epoch 60/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1085 - accuracy: 0.9482 - val_loss: 0.1121 - val_accuracy: 0.9501\n",
      "Epoch 61/80\n",
      "16569/16569 [==============================] - 1s 43us/step - loss: 0.1097 - accuracy: 0.9462 - val_loss: 0.1148 - val_accuracy: 0.9468\n",
      "Epoch 62/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1089 - accuracy: 0.9465 - val_loss: 0.1242 - val_accuracy: 0.9468\n",
      "Epoch 63/80\n",
      "16569/16569 [==============================] - 1s 42us/step - loss: 0.1110 - accuracy: 0.9460 - val_loss: 0.1128 - val_accuracy: 0.9446\n",
      "Epoch 00063: early stopping\n",
      "[[9.9999988e-01 9.6434256e-09 4.1099981e-09 6.9055339e-08 2.2764453e-10\n",
      "  6.1241651e-11]\n",
      " [4.0647628e-05 3.0433200e-04 8.7716927e-07 7.1188873e-07 4.1744028e-05\n",
      "  9.9961179e-01]\n",
      " [3.9967399e-05 2.9934070e-04 8.7063103e-07 7.0089266e-07 4.1674459e-05\n",
      "  9.9961734e-01]\n",
      " ...\n",
      " [1.0765481e-09 9.1991723e-08 9.9904627e-01 9.5229020e-04 4.0258436e-14\n",
      "  1.3566413e-06]\n",
      " [2.6094202e-05 5.1793521e-08 9.8615241e-01 5.7936425e-04 1.2762502e-02\n",
      "  4.7961043e-04]\n",
      " [1.5133227e-05 2.9259715e-09 1.9858862e-06 8.7454282e-08 9.9972516e-01\n",
      "  2.5761139e-04]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:02:13.124973\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16569 samples, validate on 1842 samples\n",
      "Epoch 1/80\n",
      "16569/16569 [==============================] - 1s 53us/step - loss: 0.7044 - accuracy: 0.7214 - val_loss: 0.4000 - val_accuracy: 0.8388\n",
      "Epoch 2/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.3590 - accuracy: 0.8662 - val_loss: 0.2892 - val_accuracy: 0.9045\n",
      "Epoch 3/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.2595 - accuracy: 0.9034 - val_loss: 0.2085 - val_accuracy: 0.9186\n",
      "Epoch 4/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.2119 - accuracy: 0.9173 - val_loss: 0.1785 - val_accuracy: 0.9305\n",
      "Epoch 5/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1889 - accuracy: 0.9246 - val_loss: 0.1675 - val_accuracy: 0.9354\n",
      "Epoch 6/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1733 - accuracy: 0.9273 - val_loss: 0.1603 - val_accuracy: 0.9370\n",
      "Epoch 7/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1677 - accuracy: 0.9293 - val_loss: 0.1413 - val_accuracy: 0.9419\n",
      "Epoch 8/80\n",
      "16569/16569 [==============================] - 1s 42us/step - loss: 0.1600 - accuracy: 0.9309 - val_loss: 0.1445 - val_accuracy: 0.9414\n",
      "Epoch 9/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1589 - accuracy: 0.9312 - val_loss: 0.1409 - val_accuracy: 0.9435\n",
      "Epoch 10/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1534 - accuracy: 0.9336 - val_loss: 0.1404 - val_accuracy: 0.9479\n",
      "Epoch 11/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1525 - accuracy: 0.9334 - val_loss: 0.1333 - val_accuracy: 0.9441\n",
      "Epoch 12/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1454 - accuracy: 0.9364 - val_loss: 0.1217 - val_accuracy: 0.9511\n",
      "Epoch 13/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1418 - accuracy: 0.9383 - val_loss: 0.1145 - val_accuracy: 0.9528\n",
      "Epoch 14/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1424 - accuracy: 0.9374 - val_loss: 0.1318 - val_accuracy: 0.9408\n",
      "Epoch 15/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1392 - accuracy: 0.9375 - val_loss: 0.1202 - val_accuracy: 0.9490\n",
      "Epoch 16/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1353 - accuracy: 0.9377 - val_loss: 0.1175 - val_accuracy: 0.9484\n",
      "Epoch 17/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1317 - accuracy: 0.9399 - val_loss: 0.1067 - val_accuracy: 0.9517\n",
      "Epoch 18/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1307 - accuracy: 0.9380 - val_loss: 0.1145 - val_accuracy: 0.9457\n",
      "Epoch 19/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1308 - accuracy: 0.9390 - val_loss: 0.1127 - val_accuracy: 0.9490\n",
      "Epoch 20/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1289 - accuracy: 0.9404 - val_loss: 0.1172 - val_accuracy: 0.9446\n",
      "Epoch 21/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1258 - accuracy: 0.9394 - val_loss: 0.1161 - val_accuracy: 0.9425\n",
      "Epoch 22/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1252 - accuracy: 0.9411 - val_loss: 0.1233 - val_accuracy: 0.9419\n",
      "Epoch 23/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1244 - accuracy: 0.9418 - val_loss: 0.1068 - val_accuracy: 0.9501\n",
      "Epoch 24/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1217 - accuracy: 0.9420 - val_loss: 0.1121 - val_accuracy: 0.9473\n",
      "Epoch 25/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1230 - accuracy: 0.9410 - val_loss: 0.1096 - val_accuracy: 0.9473\n",
      "Epoch 26/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1205 - accuracy: 0.9425 - val_loss: 0.1028 - val_accuracy: 0.9549\n",
      "Epoch 27/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1195 - accuracy: 0.9436 - val_loss: 0.1148 - val_accuracy: 0.9463\n",
      "Epoch 28/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1180 - accuracy: 0.9427 - val_loss: 0.1010 - val_accuracy: 0.9544\n",
      "Epoch 29/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1191 - accuracy: 0.9432 - val_loss: 0.1123 - val_accuracy: 0.9479\n",
      "Epoch 30/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1180 - accuracy: 0.9446 - val_loss: 0.1021 - val_accuracy: 0.9549\n",
      "Epoch 31/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1164 - accuracy: 0.9448 - val_loss: 0.1222 - val_accuracy: 0.9479\n",
      "Epoch 32/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1201 - accuracy: 0.9428 - val_loss: 0.1013 - val_accuracy: 0.9517\n",
      "Epoch 33/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1181 - accuracy: 0.9436 - val_loss: 0.1079 - val_accuracy: 0.9539\n",
      "Epoch 34/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1143 - accuracy: 0.9453 - val_loss: 0.1022 - val_accuracy: 0.9544\n",
      "Epoch 35/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1176 - accuracy: 0.9433 - val_loss: 0.1048 - val_accuracy: 0.9506\n",
      "Epoch 36/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1145 - accuracy: 0.9436 - val_loss: 0.0981 - val_accuracy: 0.9533\n",
      "Epoch 37/80\n",
      "16569/16569 [==============================] - 1s 42us/step - loss: 0.1154 - accuracy: 0.9447 - val_loss: 0.0963 - val_accuracy: 0.9555\n",
      "Epoch 38/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1148 - accuracy: 0.9460 - val_loss: 0.0991 - val_accuracy: 0.9587\n",
      "Epoch 39/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1118 - accuracy: 0.9454 - val_loss: 0.0983 - val_accuracy: 0.9544\n",
      "Epoch 40/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1119 - accuracy: 0.9457 - val_loss: 0.1011 - val_accuracy: 0.9528\n",
      "Epoch 41/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1144 - accuracy: 0.9446 - val_loss: 0.1003 - val_accuracy: 0.9577\n",
      "Epoch 42/80\n",
      "16569/16569 [==============================] - 1s 36us/step - loss: 0.1137 - accuracy: 0.9442 - val_loss: 0.0965 - val_accuracy: 0.9560\n",
      "Epoch 43/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1137 - accuracy: 0.9457 - val_loss: 0.0976 - val_accuracy: 0.9533\n",
      "Epoch 44/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1113 - accuracy: 0.9465 - val_loss: 0.0994 - val_accuracy: 0.9549\n",
      "Epoch 45/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1126 - accuracy: 0.9443 - val_loss: 0.0997 - val_accuracy: 0.9577\n",
      "Epoch 46/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1113 - accuracy: 0.9471 - val_loss: 0.1040 - val_accuracy: 0.9511\n",
      "Epoch 47/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1137 - accuracy: 0.9452 - val_loss: 0.1089 - val_accuracy: 0.9468\n",
      "Epoch 00047: early stopping\n",
      "[[9.9999905e-01 1.0824681e-11 1.7898835e-11 2.8977167e-09 9.7523196e-07\n",
      "  3.1647891e-09]\n",
      " [9.4303780e-04 4.8753431e-05 6.5902765e-07 5.2966129e-07 4.5355959e-03\n",
      "  9.9447143e-01]\n",
      " [9.2044519e-04 4.8228267e-05 6.5658281e-07 5.2371905e-07 4.5077628e-03\n",
      "  9.9452233e-01]\n",
      " ...\n",
      " [3.5433283e-09 3.6888033e-09 9.9904031e-01 9.5862907e-04 4.5900631e-13\n",
      "  1.0763308e-06]\n",
      " [5.7359917e-05 3.1783470e-10 9.9215358e-01 3.3631275e-04 5.7102796e-03\n",
      "  1.7423717e-03]\n",
      " [5.7583541e-04 1.2648488e-11 3.6104791e-06 1.5632294e-08 9.9880242e-01\n",
      "  6.1806350e-04]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:02:47.718722\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16569 samples, validate on 1842 samples\n",
      "Epoch 1/80\n",
      "16569/16569 [==============================] - 1s 54us/step - loss: 1.2845 - accuracy: 0.4446 - val_loss: 0.8842 - val_accuracy: 0.6498\n",
      "Epoch 2/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.7417 - accuracy: 0.6627 - val_loss: 0.6806 - val_accuracy: 0.6846\n",
      "Epoch 3/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.6593 - accuracy: 0.6724 - val_loss: 0.6440 - val_accuracy: 0.6835\n",
      "Epoch 4/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.6262 - accuracy: 0.6844 - val_loss: 0.6100 - val_accuracy: 0.6916\n",
      "Epoch 5/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.5129 - accuracy: 0.7824 - val_loss: 0.4014 - val_accuracy: 0.8442\n",
      "Epoch 6/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.3208 - accuracy: 0.8749 - val_loss: 0.3220 - val_accuracy: 0.8697\n",
      "Epoch 7/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.2319 - accuracy: 0.9152 - val_loss: 0.2162 - val_accuracy: 0.9218\n",
      "Epoch 8/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1962 - accuracy: 0.9276 - val_loss: 0.1755 - val_accuracy: 0.9300\n",
      "Epoch 9/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1783 - accuracy: 0.9285 - val_loss: 0.1652 - val_accuracy: 0.9305\n",
      "Epoch 10/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1708 - accuracy: 0.9299 - val_loss: 0.1537 - val_accuracy: 0.9283\n",
      "Epoch 11/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1654 - accuracy: 0.9305 - val_loss: 0.2491 - val_accuracy: 0.9169\n",
      "Epoch 12/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1658 - accuracy: 0.9302 - val_loss: 0.1609 - val_accuracy: 0.9316\n",
      "Epoch 13/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1654 - accuracy: 0.9296 - val_loss: 0.1672 - val_accuracy: 0.9197\n",
      "Epoch 14/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1548 - accuracy: 0.9315 - val_loss: 0.1461 - val_accuracy: 0.9365\n",
      "Epoch 15/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1532 - accuracy: 0.9322 - val_loss: 0.1398 - val_accuracy: 0.9349\n",
      "Epoch 16/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1635 - accuracy: 0.9279 - val_loss: 0.1565 - val_accuracy: 0.9245\n",
      "Epoch 17/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1576 - accuracy: 0.9312 - val_loss: 0.1417 - val_accuracy: 0.9349\n",
      "Epoch 18/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1516 - accuracy: 0.9324 - val_loss: 0.1628 - val_accuracy: 0.9278\n",
      "Epoch 19/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1611 - accuracy: 0.9284 - val_loss: 0.1377 - val_accuracy: 0.9365\n",
      "Epoch 20/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1520 - accuracy: 0.9326 - val_loss: 0.1344 - val_accuracy: 0.9343\n",
      "Epoch 21/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1527 - accuracy: 0.9307 - val_loss: 0.1339 - val_accuracy: 0.9349\n",
      "Epoch 22/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1460 - accuracy: 0.9341 - val_loss: 0.1521 - val_accuracy: 0.9305\n",
      "Epoch 23/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1519 - accuracy: 0.9321 - val_loss: 0.1462 - val_accuracy: 0.9349\n",
      "Epoch 24/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1483 - accuracy: 0.9314 - val_loss: 0.1397 - val_accuracy: 0.9321\n",
      "Epoch 25/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1480 - accuracy: 0.9322 - val_loss: 0.1368 - val_accuracy: 0.9338\n",
      "Epoch 26/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1485 - accuracy: 0.9313 - val_loss: 0.1429 - val_accuracy: 0.9365\n",
      "Epoch 27/80\n",
      "16569/16569 [==============================] - 1s 41us/step - loss: 0.1446 - accuracy: 0.9340 - val_loss: 0.1331 - val_accuracy: 0.9365\n",
      "Epoch 28/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1470 - accuracy: 0.9325 - val_loss: 0.1345 - val_accuracy: 0.9354\n",
      "Epoch 29/80\n",
      "16569/16569 [==============================] - 1s 42us/step - loss: 0.1541 - accuracy: 0.9305 - val_loss: 0.1338 - val_accuracy: 0.9359\n",
      "Epoch 30/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1448 - accuracy: 0.9338 - val_loss: 0.1347 - val_accuracy: 0.9343\n",
      "Epoch 31/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1434 - accuracy: 0.9338 - val_loss: 0.1457 - val_accuracy: 0.9387\n",
      "Epoch 32/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1435 - accuracy: 0.9343 - val_loss: 0.1387 - val_accuracy: 0.9278\n",
      "Epoch 33/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1448 - accuracy: 0.9332 - val_loss: 0.1308 - val_accuracy: 0.9349\n",
      "Epoch 34/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1481 - accuracy: 0.9331 - val_loss: 0.1338 - val_accuracy: 0.9349\n",
      "Epoch 35/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1414 - accuracy: 0.9355 - val_loss: 0.1365 - val_accuracy: 0.9327\n",
      "Epoch 36/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1434 - accuracy: 0.9327 - val_loss: 0.1351 - val_accuracy: 0.9365\n",
      "Epoch 37/80\n",
      "16569/16569 [==============================] - 1s 39us/step - loss: 0.1456 - accuracy: 0.9330 - val_loss: 0.1569 - val_accuracy: 0.9376\n",
      "Epoch 38/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1453 - accuracy: 0.9354 - val_loss: 0.1315 - val_accuracy: 0.9327\n",
      "Epoch 39/80\n",
      "16569/16569 [==============================] - 1s 37us/step - loss: 0.1397 - accuracy: 0.9347 - val_loss: 0.1678 - val_accuracy: 0.9207\n",
      "Epoch 40/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1407 - accuracy: 0.9332 - val_loss: 0.1453 - val_accuracy: 0.9273\n",
      "Epoch 41/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1474 - accuracy: 0.9306 - val_loss: 0.1333 - val_accuracy: 0.9321\n",
      "Epoch 42/80\n",
      "16569/16569 [==============================] - 1s 38us/step - loss: 0.1455 - accuracy: 0.9314 - val_loss: 0.1342 - val_accuracy: 0.9381\n",
      "Epoch 43/80\n",
      "16569/16569 [==============================] - 1s 40us/step - loss: 0.1429 - accuracy: 0.9351 - val_loss: 0.1316 - val_accuracy: 0.9359\n",
      "Epoch 00043: early stopping\n",
      "[[9.9999571e-01 7.4498064e-07 8.9245739e-10 6.7839789e-11 3.5587470e-06\n",
      "  6.6262061e-08]\n",
      " [2.3030069e-04 1.3722033e-05 1.2776832e-07 3.6442402e-09 1.3028412e-03\n",
      "  9.9845302e-01]\n",
      " [2.2818008e-04 1.3589562e-05 1.2779915e-07 3.6234471e-09 1.2975527e-03\n",
      "  9.9846065e-01]\n",
      " ...\n",
      " [1.7859508e-12 1.0686913e-21 9.9539977e-01 4.5742323e-03 1.1715239e-11\n",
      "  2.5987485e-05]\n",
      " [1.7561977e-07 9.9708444e-22 9.2262310e-01 5.8979909e-03 7.0683278e-02\n",
      "  7.9547474e-04]\n",
      " [8.1173202e-06 3.9044563e-16 1.7204977e-07 1.4848544e-07 9.9992502e-01\n",
      "  6.6473367e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:03:20.328152\n",
      "region_label= [6, 6, 2, 4, 3, 5]\n",
      "dist_table_truth\n",
      " [[1260  842    0 1422   15  306    0    0    0]\n",
      " [ 620    0    2   60  177    3    0    0    0]\n",
      " [  56  330 1948   22 1290  527    0    0    0]\n",
      " [ 165 1181 2063    0 1314  215    0    0    0]\n",
      " [2245  439   30 2456   44 1405    0    0    0]\n",
      " [ 138  641   30   87  234 1776    0    0    0]]\n",
      "num of merged_region_image 0 3139\n",
      "num of merged_region_image 1 3200\n",
      "num of merged_region_image 2 3797\n",
      "num of merged_region_image 3 3430\n",
      "num of merged_region_image 4 2219\n",
      "num of merged_region_image 5 2626\n",
      "Counter({-1: 24806, 2: 3797, 3: 3430, 1: 3200, 0: 3139, 5: 2626, 4: 2219})\n",
      "===========  ITE = 1   ===========\n",
      "used_img 18411 18411\n",
      "working_img(=other images=unclean images) 24806 24806\n",
      "merged regions 88 88\n",
      "other_regions 112 112\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.87</td>\n",
       "      <td>4</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>195</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     other index  pred label  truth  rate    0    1  2  3  4    5\n",
       "0              6           5      1  0.87    4   62  0  0  1    2\n",
       "1              7           3      5  0.85    1    0  0  0  3  239\n",
       "2              8          -1      5  0.00    0    3  0  0  0  361\n",
       "3              9          -1      0  0.00  245    0  1  0  1    0\n",
       "4             11           1      7  0.92    2    0  0  0  0    0\n",
       "..           ...         ...    ...   ...  ...  ... .. .. ..  ...\n",
       "107          194           0      1  1.00    1  111  0  0  0    2\n",
       "108          195           4      1  0.70    2   21  0  0  0   17\n",
       "109          196           4      0  0.67  372    2  1  1  4    1\n",
       "110          199           2      8  0.90    0    0  2  0  0    0\n",
       "111          200           4      0  1.00  260    0  0  0  1    0\n",
       "\n",
       "[112 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [35, 38, 132, 139, 158] 990\n",
      "added label, regions, img amount: {1} [104, 45, 11, 49, 193] 1308\n",
      "added label, regions, img amount: {2} [77, 92, 141, 163, 93] 431\n",
      "added label, regions, img amount: {3} [52, 43, 189, 7, 172] 776\n",
      "added label, regions, img amount: {4} [117, 76, 145, 135, 133] 1479\n",
      "added label, regions, img amount: {5} [14, 6, 143, 78, 90] 857\n",
      "Not getting into residuals\n",
      "NUM_region 6\n",
      "number of clean images 24252\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 6\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21826 samples, validate on 2426 samples\n",
      "Epoch 1/80\n",
      "21826/21826 [==============================] - 1s 64us/step - loss: 1.1842 - accuracy: 0.4814 - val_loss: 0.8376 - val_accuracy: 0.6270\n",
      "Epoch 2/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.7746 - accuracy: 0.6263 - val_loss: 0.7340 - val_accuracy: 0.6187\n",
      "Epoch 3/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.6745 - accuracy: 0.6817 - val_loss: 0.6138 - val_accuracy: 0.7185\n",
      "Epoch 4/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.5664 - accuracy: 0.7556 - val_loss: 0.5370 - val_accuracy: 0.7737\n",
      "Epoch 5/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.5097 - accuracy: 0.7798 - val_loss: 0.4881 - val_accuracy: 0.7877\n",
      "Epoch 6/80\n",
      "21826/21826 [==============================] - 1s 42us/step - loss: 0.4417 - accuracy: 0.8112 - val_loss: 0.4149 - val_accuracy: 0.8211\n",
      "Epoch 7/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.3657 - accuracy: 0.8438 - val_loss: 0.3458 - val_accuracy: 0.8450\n",
      "Epoch 8/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.3092 - accuracy: 0.8635 - val_loss: 0.2988 - val_accuracy: 0.8627\n",
      "Epoch 9/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.3006 - accuracy: 0.8660 - val_loss: 0.2986 - val_accuracy: 0.8652\n",
      "Epoch 10/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.2961 - accuracy: 0.8679 - val_loss: 0.2831 - val_accuracy: 0.8751\n",
      "Epoch 11/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.2827 - accuracy: 0.8708 - val_loss: 0.2960 - val_accuracy: 0.8590\n",
      "Epoch 12/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.2796 - accuracy: 0.8746 - val_loss: 0.2652 - val_accuracy: 0.8776\n",
      "Epoch 13/80\n",
      "21826/21826 [==============================] - 1s 34us/step - loss: 0.2137 - accuracy: 0.9183 - val_loss: 0.1391 - val_accuracy: 0.9505\n",
      "Epoch 14/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1408 - accuracy: 0.9499 - val_loss: 0.1191 - val_accuracy: 0.9567\n",
      "Epoch 15/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1224 - accuracy: 0.9540 - val_loss: 0.1249 - val_accuracy: 0.9555\n",
      "Epoch 16/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1150 - accuracy: 0.9546 - val_loss: 0.1113 - val_accuracy: 0.9575\n",
      "Epoch 17/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1147 - accuracy: 0.9523 - val_loss: 0.0969 - val_accuracy: 0.9592\n",
      "Epoch 18/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1116 - accuracy: 0.9537 - val_loss: 0.0972 - val_accuracy: 0.9563\n",
      "Epoch 19/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.1077 - accuracy: 0.9532 - val_loss: 0.1066 - val_accuracy: 0.9522\n",
      "Epoch 20/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1053 - accuracy: 0.9562 - val_loss: 0.1010 - val_accuracy: 0.9547\n",
      "Epoch 21/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1067 - accuracy: 0.9551 - val_loss: 0.0952 - val_accuracy: 0.9501\n",
      "Epoch 22/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1016 - accuracy: 0.9569 - val_loss: 0.1171 - val_accuracy: 0.9468\n",
      "Epoch 23/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0979 - accuracy: 0.9571 - val_loss: 0.0950 - val_accuracy: 0.9559\n",
      "Epoch 24/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1024 - accuracy: 0.9555 - val_loss: 0.0877 - val_accuracy: 0.9592\n",
      "Epoch 25/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1015 - accuracy: 0.9571 - val_loss: 0.1166 - val_accuracy: 0.9534\n",
      "Epoch 26/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1031 - accuracy: 0.9545 - val_loss: 0.0861 - val_accuracy: 0.9592\n",
      "Epoch 27/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0986 - accuracy: 0.9557 - val_loss: 0.1361 - val_accuracy: 0.9411\n",
      "Epoch 28/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1036 - accuracy: 0.9541 - val_loss: 0.0921 - val_accuracy: 0.9600\n",
      "Epoch 29/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0978 - accuracy: 0.9569 - val_loss: 0.1064 - val_accuracy: 0.9460\n",
      "Epoch 30/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0998 - accuracy: 0.9544 - val_loss: 0.0996 - val_accuracy: 0.9592\n",
      "Epoch 31/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1072 - accuracy: 0.9525 - val_loss: 0.1067 - val_accuracy: 0.9584\n",
      "Epoch 32/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1112 - accuracy: 0.9525 - val_loss: 0.0864 - val_accuracy: 0.9666\n",
      "Epoch 33/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0959 - accuracy: 0.9567 - val_loss: 0.1024 - val_accuracy: 0.9518\n",
      "Epoch 34/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0922 - accuracy: 0.9582 - val_loss: 0.0949 - val_accuracy: 0.9571\n",
      "Epoch 35/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0997 - accuracy: 0.9553 - val_loss: 0.1111 - val_accuracy: 0.9555\n",
      "Epoch 36/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0966 - accuracy: 0.9573 - val_loss: 0.0811 - val_accuracy: 0.9641\n",
      "Epoch 37/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0991 - accuracy: 0.9553 - val_loss: 0.0856 - val_accuracy: 0.9555\n",
      "Epoch 38/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0950 - accuracy: 0.9582 - val_loss: 0.0945 - val_accuracy: 0.9514\n",
      "Epoch 39/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0998 - accuracy: 0.9548 - val_loss: 0.0943 - val_accuracy: 0.9600\n",
      "Epoch 40/80\n",
      "21826/21826 [==============================] - 1s 34us/step - loss: 0.0947 - accuracy: 0.9571 - val_loss: 0.1307 - val_accuracy: 0.9382\n",
      "Epoch 41/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0948 - accuracy: 0.9565 - val_loss: 0.0835 - val_accuracy: 0.9637\n",
      "Epoch 42/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0951 - accuracy: 0.9569 - val_loss: 0.0949 - val_accuracy: 0.9604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0994 - accuracy: 0.9571 - val_loss: 0.0884 - val_accuracy: 0.9584\n",
      "Epoch 44/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0994 - accuracy: 0.9551 - val_loss: 0.1387 - val_accuracy: 0.9382\n",
      "Epoch 45/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0991 - accuracy: 0.9568 - val_loss: 0.0917 - val_accuracy: 0.9522\n",
      "Epoch 46/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0950 - accuracy: 0.9563 - val_loss: 0.0804 - val_accuracy: 0.9687\n",
      "Epoch 47/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0927 - accuracy: 0.9584 - val_loss: 0.0941 - val_accuracy: 0.9509\n",
      "Epoch 48/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0983 - accuracy: 0.9557 - val_loss: 0.0900 - val_accuracy: 0.9526\n",
      "Epoch 49/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0950 - accuracy: 0.9575 - val_loss: 0.0797 - val_accuracy: 0.9662\n",
      "Epoch 50/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0930 - accuracy: 0.9575 - val_loss: 0.0801 - val_accuracy: 0.9683\n",
      "Epoch 51/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0929 - accuracy: 0.9578 - val_loss: 0.0970 - val_accuracy: 0.9514\n",
      "Epoch 52/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0911 - accuracy: 0.9584 - val_loss: 0.0918 - val_accuracy: 0.9575\n",
      "Epoch 53/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0942 - accuracy: 0.9577 - val_loss: 0.0934 - val_accuracy: 0.9584\n",
      "Epoch 54/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0925 - accuracy: 0.9585 - val_loss: 0.0790 - val_accuracy: 0.9633\n",
      "Epoch 55/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0916 - accuracy: 0.9585 - val_loss: 0.0865 - val_accuracy: 0.9534\n",
      "Epoch 56/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.1011 - accuracy: 0.9551 - val_loss: 0.0903 - val_accuracy: 0.9542\n",
      "Epoch 57/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0945 - accuracy: 0.9581 - val_loss: 0.0985 - val_accuracy: 0.9526\n",
      "Epoch 58/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0949 - accuracy: 0.9568 - val_loss: 0.1002 - val_accuracy: 0.9505\n",
      "Epoch 59/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0964 - accuracy: 0.9570 - val_loss: 0.0812 - val_accuracy: 0.9637\n",
      "Epoch 60/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0965 - accuracy: 0.9569 - val_loss: 0.0808 - val_accuracy: 0.9613\n",
      "Epoch 61/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0917 - accuracy: 0.9583 - val_loss: 0.0802 - val_accuracy: 0.9654\n",
      "Epoch 62/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0925 - accuracy: 0.9576 - val_loss: 0.0928 - val_accuracy: 0.9596\n",
      "Epoch 63/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0919 - accuracy: 0.9574 - val_loss: 0.0779 - val_accuracy: 0.9662\n",
      "Epoch 64/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0918 - accuracy: 0.9570 - val_loss: 0.0801 - val_accuracy: 0.9654\n",
      "Epoch 65/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0976 - accuracy: 0.9562 - val_loss: 0.0906 - val_accuracy: 0.9658\n",
      "Epoch 66/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0940 - accuracy: 0.9567 - val_loss: 0.0866 - val_accuracy: 0.9580\n",
      "Epoch 67/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0905 - accuracy: 0.9588 - val_loss: 0.1139 - val_accuracy: 0.9452\n",
      "Epoch 68/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0923 - accuracy: 0.9585 - val_loss: 0.0835 - val_accuracy: 0.9654\n",
      "Epoch 69/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0930 - accuracy: 0.9573 - val_loss: 0.0845 - val_accuracy: 0.9654\n",
      "Epoch 70/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0914 - accuracy: 0.9572 - val_loss: 0.0844 - val_accuracy: 0.9658\n",
      "Epoch 71/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0940 - accuracy: 0.9577 - val_loss: 0.0778 - val_accuracy: 0.9637\n",
      "Epoch 72/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0904 - accuracy: 0.9582 - val_loss: 0.0827 - val_accuracy: 0.9584\n",
      "Epoch 73/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0960 - accuracy: 0.9567 - val_loss: 0.0841 - val_accuracy: 0.9604\n",
      "Epoch 74/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0898 - accuracy: 0.9585 - val_loss: 0.0844 - val_accuracy: 0.9654\n",
      "Epoch 75/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0902 - accuracy: 0.9586 - val_loss: 0.0835 - val_accuracy: 0.9592\n",
      "Epoch 76/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0942 - accuracy: 0.9560 - val_loss: 0.0830 - val_accuracy: 0.9658\n",
      "Epoch 77/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0901 - accuracy: 0.9578 - val_loss: 0.0798 - val_accuracy: 0.9658\n",
      "Epoch 78/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0877 - accuracy: 0.9612 - val_loss: 0.0917 - val_accuracy: 0.9666\n",
      "Epoch 79/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0968 - accuracy: 0.9569 - val_loss: 0.0824 - val_accuracy: 0.9604\n",
      "Epoch 80/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0928 - accuracy: 0.9564 - val_loss: 0.0812 - val_accuracy: 0.9604\n",
      "[[1.0000000e+00 2.7720339e-14 2.0218208e-15 4.0613687e-22 6.3935739e-11\n",
      "  1.4415163e-12]\n",
      " [6.7727419e-06 7.5075191e-09 1.2363791e-08 8.3403020e-16 9.7676828e-05\n",
      "  9.9989557e-01]\n",
      " [6.7295787e-06 7.4834441e-09 1.2455272e-08 8.4312145e-16 9.7639393e-05\n",
      "  9.9989557e-01]\n",
      " ...\n",
      " [1.2584839e-11 3.4201842e-33 9.9109215e-01 8.9069251e-03 4.9221846e-13\n",
      "  9.5335565e-07]\n",
      " [1.7593025e-07 2.4696614e-28 9.7535741e-01 7.6330133e-04 1.9915095e-02\n",
      "  3.9639827e-03]\n",
      " [3.1263942e-05 7.9087768e-26 4.3771053e-08 1.7727721e-12 9.9994361e-01\n",
      "  2.5020172e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:01:10.077491\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21826 samples, validate on 2426 samples\n",
      "Epoch 1/80\n",
      "21826/21826 [==============================] - 1s 50us/step - loss: 0.9808 - accuracy: 0.6068 - val_loss: 0.4483 - val_accuracy: 0.8549\n",
      "Epoch 2/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.3064 - accuracy: 0.8924 - val_loss: 0.2245 - val_accuracy: 0.9184\n",
      "Epoch 3/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1880 - accuracy: 0.9339 - val_loss: 0.1591 - val_accuracy: 0.9505\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1572 - accuracy: 0.9434 - val_loss: 0.1407 - val_accuracy: 0.9526\n",
      "Epoch 5/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1447 - accuracy: 0.9461 - val_loss: 0.1292 - val_accuracy: 0.9526\n",
      "Epoch 6/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1347 - accuracy: 0.9493 - val_loss: 0.1286 - val_accuracy: 0.9530\n",
      "Epoch 7/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1300 - accuracy: 0.9503 - val_loss: 0.1196 - val_accuracy: 0.9522\n",
      "Epoch 8/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1270 - accuracy: 0.9516 - val_loss: 0.1208 - val_accuracy: 0.9542\n",
      "Epoch 9/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1254 - accuracy: 0.9528 - val_loss: 0.1141 - val_accuracy: 0.9547\n",
      "Epoch 10/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1206 - accuracy: 0.9525 - val_loss: 0.1123 - val_accuracy: 0.9530\n",
      "Epoch 11/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1175 - accuracy: 0.9542 - val_loss: 0.1140 - val_accuracy: 0.9534\n",
      "Epoch 12/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1169 - accuracy: 0.9538 - val_loss: 0.1087 - val_accuracy: 0.9563\n",
      "Epoch 13/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1102 - accuracy: 0.9562 - val_loss: 0.1117 - val_accuracy: 0.9542\n",
      "Epoch 14/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1085 - accuracy: 0.9560 - val_loss: 0.1111 - val_accuracy: 0.9551\n",
      "Epoch 15/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1069 - accuracy: 0.9567 - val_loss: 0.1075 - val_accuracy: 0.9563\n",
      "Epoch 16/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1049 - accuracy: 0.9577 - val_loss: 0.1275 - val_accuracy: 0.9551\n",
      "Epoch 17/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1050 - accuracy: 0.9570 - val_loss: 0.1026 - val_accuracy: 0.9584\n",
      "Epoch 18/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1036 - accuracy: 0.9579 - val_loss: 0.0993 - val_accuracy: 0.9559\n",
      "Epoch 19/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1045 - accuracy: 0.9574 - val_loss: 0.1136 - val_accuracy: 0.9518\n",
      "Epoch 20/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1026 - accuracy: 0.9574 - val_loss: 0.1018 - val_accuracy: 0.9542\n",
      "Epoch 21/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1001 - accuracy: 0.9573 - val_loss: 0.1075 - val_accuracy: 0.9547\n",
      "Epoch 22/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0985 - accuracy: 0.9587 - val_loss: 0.0972 - val_accuracy: 0.9559\n",
      "Epoch 23/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0978 - accuracy: 0.9582 - val_loss: 0.0944 - val_accuracy: 0.9571\n",
      "Epoch 24/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0968 - accuracy: 0.9578 - val_loss: 0.0994 - val_accuracy: 0.9567\n",
      "Epoch 25/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0968 - accuracy: 0.9584 - val_loss: 0.1074 - val_accuracy: 0.9542\n",
      "Epoch 26/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0962 - accuracy: 0.9587 - val_loss: 0.0974 - val_accuracy: 0.9563\n",
      "Epoch 27/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0951 - accuracy: 0.9587 - val_loss: 0.0877 - val_accuracy: 0.9596\n",
      "Epoch 28/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0966 - accuracy: 0.9574 - val_loss: 0.1064 - val_accuracy: 0.9547\n",
      "Epoch 29/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0940 - accuracy: 0.9589 - val_loss: 0.1006 - val_accuracy: 0.9542\n",
      "Epoch 30/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0923 - accuracy: 0.9595 - val_loss: 0.0941 - val_accuracy: 0.9571\n",
      "Epoch 31/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0944 - accuracy: 0.9576 - val_loss: 0.0937 - val_accuracy: 0.9584\n",
      "Epoch 32/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0937 - accuracy: 0.9581 - val_loss: 0.0918 - val_accuracy: 0.9575\n",
      "Epoch 33/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0922 - accuracy: 0.9591 - val_loss: 0.0875 - val_accuracy: 0.9571\n",
      "Epoch 34/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0938 - accuracy: 0.9591 - val_loss: 0.0988 - val_accuracy: 0.9575\n",
      "Epoch 35/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0923 - accuracy: 0.9600 - val_loss: 0.0925 - val_accuracy: 0.9592\n",
      "Epoch 36/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0937 - accuracy: 0.9588 - val_loss: 0.0914 - val_accuracy: 0.9588\n",
      "Epoch 37/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0909 - accuracy: 0.9593 - val_loss: 0.0911 - val_accuracy: 0.9580\n",
      "Epoch 38/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0904 - accuracy: 0.9598 - val_loss: 0.0982 - val_accuracy: 0.9584\n",
      "Epoch 39/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0946 - accuracy: 0.9580 - val_loss: 0.0910 - val_accuracy: 0.9584\n",
      "Epoch 40/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0915 - accuracy: 0.9592 - val_loss: 0.0903 - val_accuracy: 0.9592\n",
      "Epoch 41/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0914 - accuracy: 0.9585 - val_loss: 0.0904 - val_accuracy: 0.9538\n",
      "Epoch 42/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0898 - accuracy: 0.9598 - val_loss: 0.0919 - val_accuracy: 0.9580\n",
      "Epoch 43/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0898 - accuracy: 0.9608 - val_loss: 0.0853 - val_accuracy: 0.9592\n",
      "Epoch 44/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0915 - accuracy: 0.9583 - val_loss: 0.1088 - val_accuracy: 0.9538\n",
      "Epoch 45/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0895 - accuracy: 0.9604 - val_loss: 0.0859 - val_accuracy: 0.9580\n",
      "Epoch 46/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0892 - accuracy: 0.9595 - val_loss: 0.1085 - val_accuracy: 0.9530\n",
      "Epoch 47/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0933 - accuracy: 0.9575 - val_loss: 0.0924 - val_accuracy: 0.9559\n",
      "Epoch 48/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0884 - accuracy: 0.9597 - val_loss: 0.0845 - val_accuracy: 0.9604\n",
      "Epoch 49/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0895 - accuracy: 0.9597 - val_loss: 0.1030 - val_accuracy: 0.9542\n",
      "Epoch 50/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0904 - accuracy: 0.9597 - val_loss: 0.0841 - val_accuracy: 0.9592\n",
      "Epoch 51/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0882 - accuracy: 0.9600 - val_loss: 0.0850 - val_accuracy: 0.9592\n",
      "Epoch 52/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0880 - accuracy: 0.9610 - val_loss: 0.0902 - val_accuracy: 0.9571\n",
      "Epoch 53/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0914 - accuracy: 0.9584 - val_loss: 0.0852 - val_accuracy: 0.9596\n",
      "Epoch 54/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0884 - accuracy: 0.9604 - val_loss: 0.0889 - val_accuracy: 0.9588\n",
      "Epoch 55/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0878 - accuracy: 0.9595 - val_loss: 0.0874 - val_accuracy: 0.9588\n",
      "Epoch 56/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0870 - accuracy: 0.9606 - val_loss: 0.0950 - val_accuracy: 0.9584\n",
      "Epoch 57/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0885 - accuracy: 0.9599 - val_loss: 0.0892 - val_accuracy: 0.9559\n",
      "Epoch 58/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0885 - accuracy: 0.9598 - val_loss: 0.0840 - val_accuracy: 0.9580\n",
      "Epoch 59/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0885 - accuracy: 0.9599 - val_loss: 0.0845 - val_accuracy: 0.9575\n",
      "Epoch 60/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0875 - accuracy: 0.9611 - val_loss: 0.0890 - val_accuracy: 0.9596\n",
      "Epoch 61/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0894 - accuracy: 0.9593 - val_loss: 0.0835 - val_accuracy: 0.9604\n",
      "Epoch 62/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0872 - accuracy: 0.9599 - val_loss: 0.0833 - val_accuracy: 0.9592\n",
      "Epoch 63/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0915 - accuracy: 0.9586 - val_loss: 0.0827 - val_accuracy: 0.9592\n",
      "Epoch 64/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0869 - accuracy: 0.9595 - val_loss: 0.0868 - val_accuracy: 0.9584\n",
      "Epoch 65/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0858 - accuracy: 0.9605 - val_loss: 0.0804 - val_accuracy: 0.9613\n",
      "Epoch 66/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0868 - accuracy: 0.9609 - val_loss: 0.0884 - val_accuracy: 0.9571\n",
      "Epoch 67/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0882 - accuracy: 0.9592 - val_loss: 0.0859 - val_accuracy: 0.9555\n",
      "Epoch 68/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0890 - accuracy: 0.9606 - val_loss: 0.0889 - val_accuracy: 0.9571\n",
      "Epoch 69/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0859 - accuracy: 0.9617 - val_loss: 0.0860 - val_accuracy: 0.9592\n",
      "Epoch 70/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0872 - accuracy: 0.9609 - val_loss: 0.1028 - val_accuracy: 0.9555\n",
      "Epoch 71/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0904 - accuracy: 0.9588 - val_loss: 0.0920 - val_accuracy: 0.9571\n",
      "Epoch 72/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0867 - accuracy: 0.9599 - val_loss: 0.0902 - val_accuracy: 0.9575\n",
      "Epoch 73/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0854 - accuracy: 0.9610 - val_loss: 0.0845 - val_accuracy: 0.9600\n",
      "Epoch 74/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0907 - accuracy: 0.9600 - val_loss: 0.0827 - val_accuracy: 0.9596\n",
      "Epoch 75/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0888 - accuracy: 0.9596 - val_loss: 0.1063 - val_accuracy: 0.9522\n",
      "Epoch 00075: early stopping\n",
      "[[1.0000000e+00 2.8471228e-18 6.8622968e-12 1.0586547e-23 1.7950109e-10\n",
      "  4.8311778e-19]\n",
      " [2.0959560e-05 7.5401505e-07 5.6291135e-08 7.0641315e-18 1.2789348e-03\n",
      "  9.9869931e-01]\n",
      " [2.0838872e-05 7.5260817e-07 5.6260095e-08 7.0801540e-18 1.2755323e-03\n",
      "  9.9870276e-01]\n",
      " ...\n",
      " [1.4324539e-13 8.3714757e-22 9.8417872e-01 1.5818344e-02 4.1514779e-13\n",
      "  2.9721718e-06]\n",
      " [3.6741117e-08 2.0191571e-20 9.2435527e-01 3.5830766e-03 6.9905400e-02\n",
      "  2.1562965e-03]\n",
      " [1.6517854e-06 3.7512505e-17 9.7221438e-08 1.3706340e-15 9.9999630e-01\n",
      "  1.8861714e-06]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:02:17.015012\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21826 samples, validate on 2426 samples\n",
      "Epoch 1/80\n",
      "21826/21826 [==============================] - 1s 49us/step - loss: 0.5862 - accuracy: 0.7797 - val_loss: 0.3597 - val_accuracy: 0.8862\n",
      "Epoch 2/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.2613 - accuracy: 0.9037 - val_loss: 0.2225 - val_accuracy: 0.9180\n",
      "Epoch 3/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.1885 - accuracy: 0.9308 - val_loss: 0.1727 - val_accuracy: 0.9353\n",
      "Epoch 4/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.1583 - accuracy: 0.9395 - val_loss: 0.1470 - val_accuracy: 0.9452\n",
      "Epoch 5/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1372 - accuracy: 0.9449 - val_loss: 0.1304 - val_accuracy: 0.9481\n",
      "Epoch 6/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1242 - accuracy: 0.9495 - val_loss: 0.1132 - val_accuracy: 0.9584\n",
      "Epoch 7/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1161 - accuracy: 0.9528 - val_loss: 0.1103 - val_accuracy: 0.9505\n",
      "Epoch 8/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1176 - accuracy: 0.9520 - val_loss: 0.1049 - val_accuracy: 0.9547\n",
      "Epoch 9/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1080 - accuracy: 0.9549 - val_loss: 0.0998 - val_accuracy: 0.9588\n",
      "Epoch 10/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1053 - accuracy: 0.9554 - val_loss: 0.1074 - val_accuracy: 0.9547\n",
      "Epoch 11/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1026 - accuracy: 0.9547 - val_loss: 0.0965 - val_accuracy: 0.9563\n",
      "Epoch 12/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1047 - accuracy: 0.9549 - val_loss: 0.0922 - val_accuracy: 0.9580\n",
      "Epoch 13/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1005 - accuracy: 0.9552 - val_loss: 0.0923 - val_accuracy: 0.9621\n",
      "Epoch 14/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0994 - accuracy: 0.9560 - val_loss: 0.0914 - val_accuracy: 0.9580\n",
      "Epoch 15/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0993 - accuracy: 0.9551 - val_loss: 0.1046 - val_accuracy: 0.9551\n",
      "Epoch 16/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0999 - accuracy: 0.9559 - val_loss: 0.0876 - val_accuracy: 0.9617\n",
      "Epoch 17/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0975 - accuracy: 0.9569 - val_loss: 0.0908 - val_accuracy: 0.9542\n",
      "Epoch 18/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0961 - accuracy: 0.9578 - val_loss: 0.0903 - val_accuracy: 0.9559\n",
      "Epoch 19/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0940 - accuracy: 0.9573 - val_loss: 0.1088 - val_accuracy: 0.9530\n",
      "Epoch 20/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0933 - accuracy: 0.9575 - val_loss: 0.0883 - val_accuracy: 0.9563\n",
      "Epoch 21/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0930 - accuracy: 0.9571 - val_loss: 0.0949 - val_accuracy: 0.9600\n",
      "Epoch 22/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0920 - accuracy: 0.9583 - val_loss: 0.0833 - val_accuracy: 0.9596\n",
      "Epoch 23/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0892 - accuracy: 0.9594 - val_loss: 0.0855 - val_accuracy: 0.9641\n",
      "Epoch 24/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0905 - accuracy: 0.9584 - val_loss: 0.0972 - val_accuracy: 0.9567\n",
      "Epoch 25/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0908 - accuracy: 0.9589 - val_loss: 0.0874 - val_accuracy: 0.9621\n",
      "Epoch 26/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0912 - accuracy: 0.9582 - val_loss: 0.1102 - val_accuracy: 0.9567\n",
      "Epoch 27/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0896 - accuracy: 0.9589 - val_loss: 0.0860 - val_accuracy: 0.9588\n",
      "Epoch 28/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0905 - accuracy: 0.9580 - val_loss: 0.0845 - val_accuracy: 0.9625\n",
      "Epoch 29/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0897 - accuracy: 0.9577 - val_loss: 0.0947 - val_accuracy: 0.9571\n",
      "Epoch 30/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0894 - accuracy: 0.9586 - val_loss: 0.0854 - val_accuracy: 0.9613\n",
      "Epoch 31/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0903 - accuracy: 0.9595 - val_loss: 0.0849 - val_accuracy: 0.9596\n",
      "Epoch 32/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0878 - accuracy: 0.9588 - val_loss: 0.0820 - val_accuracy: 0.9600\n",
      "Epoch 33/80\n",
      "21826/21826 [==============================] - ETA: 0s - loss: 0.0852 - accuracy: 0.96 - 1s 39us/step - loss: 0.0862 - accuracy: 0.9600 - val_loss: 0.0918 - val_accuracy: 0.9650\n",
      "Epoch 34/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0936 - accuracy: 0.9573 - val_loss: 0.0816 - val_accuracy: 0.9596\n",
      "Epoch 35/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0872 - accuracy: 0.9595 - val_loss: 0.0928 - val_accuracy: 0.9621\n",
      "Epoch 36/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0865 - accuracy: 0.9591 - val_loss: 0.0976 - val_accuracy: 0.9555\n",
      "Epoch 37/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0884 - accuracy: 0.9588 - val_loss: 0.0817 - val_accuracy: 0.9588\n",
      "Epoch 38/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0861 - accuracy: 0.9604 - val_loss: 0.0823 - val_accuracy: 0.9617\n",
      "Epoch 39/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0897 - accuracy: 0.9579 - val_loss: 0.0884 - val_accuracy: 0.9613\n",
      "Epoch 40/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0867 - accuracy: 0.9599 - val_loss: 0.0844 - val_accuracy: 0.9621\n",
      "Epoch 41/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0838 - accuracy: 0.9599 - val_loss: 0.0868 - val_accuracy: 0.9588\n",
      "Epoch 42/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0859 - accuracy: 0.9612 - val_loss: 0.0801 - val_accuracy: 0.9617\n",
      "Epoch 43/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0830 - accuracy: 0.9601 - val_loss: 0.0885 - val_accuracy: 0.9600\n",
      "Epoch 44/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0846 - accuracy: 0.9603 - val_loss: 0.0819 - val_accuracy: 0.9596\n",
      "Epoch 45/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0845 - accuracy: 0.9597 - val_loss: 0.0842 - val_accuracy: 0.9592\n",
      "Epoch 46/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0840 - accuracy: 0.9612 - val_loss: 0.0843 - val_accuracy: 0.9641\n",
      "Epoch 47/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0848 - accuracy: 0.9608 - val_loss: 0.0874 - val_accuracy: 0.9613\n",
      "Epoch 48/80\n",
      "21826/21826 [==============================] - 1s 42us/step - loss: 0.0883 - accuracy: 0.9587 - val_loss: 0.0870 - val_accuracy: 0.9613\n",
      "Epoch 49/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0844 - accuracy: 0.9595 - val_loss: 0.0881 - val_accuracy: 0.9592\n",
      "Epoch 50/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0857 - accuracy: 0.9602 - val_loss: 0.0807 - val_accuracy: 0.9658\n",
      "Epoch 51/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0864 - accuracy: 0.9597 - val_loss: 0.0887 - val_accuracy: 0.9650\n",
      "Epoch 52/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0848 - accuracy: 0.9606 - val_loss: 0.0860 - val_accuracy: 0.9596\n",
      "Epoch 00052: early stopping\n",
      "[[1.0000000e+00 1.1340892e-11 1.8500656e-11 1.2249951e-10 1.2599362e-13\n",
      "  1.3591086e-12]\n",
      " [3.5269861e-06 6.5186178e-08 4.3213228e-07 2.9324872e-07 1.2296094e-06\n",
      "  9.9999452e-01]\n",
      " [3.4712127e-06 6.4171154e-08 4.3037244e-07 2.9059532e-07 1.2349400e-06\n",
      "  9.9999452e-01]\n",
      " ...\n",
      " [6.4790290e-10 6.9932721e-10 9.9950397e-01 4.9519842e-04 8.2411449e-13\n",
      "  8.3815581e-07]\n",
      " [6.9597030e-05 1.6671194e-10 9.9726629e-01 4.5099220e-04 2.1911983e-03\n",
      "  2.1901746e-05]\n",
      " [1.1452221e-06 3.0705532e-18 4.4832855e-06 2.4676726e-11 9.9999118e-01\n",
      "  3.2243154e-06]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:03:05.671209\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21826 samples, validate on 2426 samples\n",
      "Epoch 1/80\n",
      "21826/21826 [==============================] - 1s 48us/step - loss: 0.6656 - accuracy: 0.7445 - val_loss: 0.3751 - val_accuracy: 0.8838\n",
      "Epoch 2/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.2828 - accuracy: 0.8965 - val_loss: 0.2584 - val_accuracy: 0.9048\n",
      "Epoch 3/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.2103 - accuracy: 0.9255 - val_loss: 0.1878 - val_accuracy: 0.9378\n",
      "Epoch 4/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1793 - accuracy: 0.9365 - val_loss: 0.1774 - val_accuracy: 0.9398\n",
      "Epoch 5/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1584 - accuracy: 0.9429 - val_loss: 0.1473 - val_accuracy: 0.9485\n",
      "Epoch 6/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1472 - accuracy: 0.9445 - val_loss: 0.1449 - val_accuracy: 0.9472\n",
      "Epoch 7/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.1400 - accuracy: 0.9472 - val_loss: 0.1435 - val_accuracy: 0.9477\n",
      "Epoch 8/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1340 - accuracy: 0.9494 - val_loss: 0.1276 - val_accuracy: 0.9551\n",
      "Epoch 9/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1304 - accuracy: 0.9511 - val_loss: 0.1273 - val_accuracy: 0.9547\n",
      "Epoch 10/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1235 - accuracy: 0.9530 - val_loss: 0.1190 - val_accuracy: 0.9588\n",
      "Epoch 11/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1188 - accuracy: 0.9546 - val_loss: 0.1228 - val_accuracy: 0.9514\n",
      "Epoch 12/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1144 - accuracy: 0.9547 - val_loss: 0.1148 - val_accuracy: 0.9547\n",
      "Epoch 13/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1103 - accuracy: 0.9561 - val_loss: 0.1079 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1081 - accuracy: 0.9563 - val_loss: 0.1039 - val_accuracy: 0.9608\n",
      "Epoch 15/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1057 - accuracy: 0.9565 - val_loss: 0.1053 - val_accuracy: 0.9584\n",
      "Epoch 16/80\n",
      "21826/21826 [==============================] - 1s 42us/step - loss: 0.1034 - accuracy: 0.9559 - val_loss: 0.1059 - val_accuracy: 0.9559\n",
      "Epoch 17/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.1008 - accuracy: 0.9564 - val_loss: 0.0995 - val_accuracy: 0.9600\n",
      "Epoch 18/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0998 - accuracy: 0.9567 - val_loss: 0.1228 - val_accuracy: 0.9489\n",
      "Epoch 19/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1007 - accuracy: 0.9560 - val_loss: 0.1000 - val_accuracy: 0.9592\n",
      "Epoch 20/80\n",
      "21826/21826 [==============================] - 1s 42us/step - loss: 0.0981 - accuracy: 0.9565 - val_loss: 0.0974 - val_accuracy: 0.9584\n",
      "Epoch 21/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0976 - accuracy: 0.9573 - val_loss: 0.1060 - val_accuracy: 0.9551\n",
      "Epoch 22/80\n",
      "21826/21826 [==============================] - 1s 43us/step - loss: 0.0992 - accuracy: 0.9563 - val_loss: 0.0962 - val_accuracy: 0.9596\n",
      "Epoch 23/80\n",
      "21826/21826 [==============================] - 1s 42us/step - loss: 0.0969 - accuracy: 0.9566 - val_loss: 0.0999 - val_accuracy: 0.9551\n",
      "Epoch 24/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0971 - accuracy: 0.9556 - val_loss: 0.1052 - val_accuracy: 0.9559\n",
      "Epoch 25/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0921 - accuracy: 0.9584 - val_loss: 0.0949 - val_accuracy: 0.9600\n",
      "Epoch 26/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0929 - accuracy: 0.9568 - val_loss: 0.0980 - val_accuracy: 0.9571\n",
      "Epoch 27/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0960 - accuracy: 0.9572 - val_loss: 0.1045 - val_accuracy: 0.9584\n",
      "Epoch 28/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0904 - accuracy: 0.9588 - val_loss: 0.0931 - val_accuracy: 0.9600\n",
      "Epoch 29/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0922 - accuracy: 0.9577 - val_loss: 0.0907 - val_accuracy: 0.9613\n",
      "Epoch 30/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0915 - accuracy: 0.9585 - val_loss: 0.0943 - val_accuracy: 0.9596\n",
      "Epoch 31/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0925 - accuracy: 0.9574 - val_loss: 0.0940 - val_accuracy: 0.9600\n",
      "Epoch 32/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0924 - accuracy: 0.9567 - val_loss: 0.0918 - val_accuracy: 0.9608\n",
      "Epoch 33/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0925 - accuracy: 0.9594 - val_loss: 0.1062 - val_accuracy: 0.9547\n",
      "Epoch 34/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0930 - accuracy: 0.9584 - val_loss: 0.0955 - val_accuracy: 0.9571\n",
      "Epoch 35/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0919 - accuracy: 0.9576 - val_loss: 0.0955 - val_accuracy: 0.9571\n",
      "Epoch 36/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0907 - accuracy: 0.9584 - val_loss: 0.0903 - val_accuracy: 0.9559\n",
      "Epoch 37/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0885 - accuracy: 0.9586 - val_loss: 0.0873 - val_accuracy: 0.9613\n",
      "Epoch 38/80\n",
      "21826/21826 [==============================] - 1s 35us/step - loss: 0.0890 - accuracy: 0.9578 - val_loss: 0.0961 - val_accuracy: 0.9551\n",
      "Epoch 39/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0892 - accuracy: 0.9590 - val_loss: 0.0900 - val_accuracy: 0.9559\n",
      "Epoch 40/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0900 - accuracy: 0.9593 - val_loss: 0.0885 - val_accuracy: 0.9625\n",
      "Epoch 41/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0899 - accuracy: 0.9583 - val_loss: 0.0896 - val_accuracy: 0.9608\n",
      "Epoch 42/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0882 - accuracy: 0.9604 - val_loss: 0.0950 - val_accuracy: 0.9563\n",
      "Epoch 43/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0891 - accuracy: 0.9595 - val_loss: 0.0905 - val_accuracy: 0.9596\n",
      "Epoch 44/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0911 - accuracy: 0.9578 - val_loss: 0.0861 - val_accuracy: 0.9629\n",
      "Epoch 45/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0912 - accuracy: 0.9586 - val_loss: 0.0964 - val_accuracy: 0.9559\n",
      "Epoch 46/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0864 - accuracy: 0.9592 - val_loss: 0.0958 - val_accuracy: 0.9559\n",
      "Epoch 47/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0871 - accuracy: 0.9598 - val_loss: 0.0963 - val_accuracy: 0.9580\n",
      "Epoch 48/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0881 - accuracy: 0.9594 - val_loss: 0.0877 - val_accuracy: 0.9584\n",
      "Epoch 49/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0877 - accuracy: 0.9600 - val_loss: 0.0905 - val_accuracy: 0.9563\n",
      "Epoch 50/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0848 - accuracy: 0.9603 - val_loss: 0.0844 - val_accuracy: 0.9633\n",
      "Epoch 51/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0841 - accuracy: 0.9612 - val_loss: 0.0863 - val_accuracy: 0.9596\n",
      "Epoch 52/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0857 - accuracy: 0.9604 - val_loss: 0.0888 - val_accuracy: 0.9604\n",
      "Epoch 53/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0869 - accuracy: 0.9602 - val_loss: 0.0855 - val_accuracy: 0.9575\n",
      "Epoch 54/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0861 - accuracy: 0.9591 - val_loss: 0.0904 - val_accuracy: 0.9571\n",
      "Epoch 55/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0859 - accuracy: 0.9589 - val_loss: 0.0912 - val_accuracy: 0.9580\n",
      "Epoch 56/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0845 - accuracy: 0.9601 - val_loss: 0.0889 - val_accuracy: 0.9575\n",
      "Epoch 57/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0845 - accuracy: 0.9600 - val_loss: 0.0913 - val_accuracy: 0.9588\n",
      "Epoch 58/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0872 - accuracy: 0.9590 - val_loss: 0.0872 - val_accuracy: 0.9596\n",
      "Epoch 59/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0861 - accuracy: 0.9597 - val_loss: 0.0892 - val_accuracy: 0.9559\n",
      "Epoch 60/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0853 - accuracy: 0.9604 - val_loss: 0.0872 - val_accuracy: 0.9584\n",
      "Epoch 00060: early stopping\n",
      "[[1.00000000e+00 7.22310178e-13 7.02666370e-09 4.15861685e-11\n",
      "  1.94749222e-14 2.94209579e-12]\n",
      " [7.68938844e-06 6.29610986e-06 1.37601342e-06 2.75451217e-08\n",
      "  1.04003384e-07 9.99984503e-01]\n",
      " [7.47311060e-06 6.24718041e-06 1.35721530e-06 2.72244751e-08\n",
      "  1.03508270e-07 9.99984741e-01]\n",
      " ...\n",
      " [8.60763284e-14 3.40282351e-11 9.99776781e-01 2.23113209e-04\n",
      "  1.79503370e-15 1.21733137e-07]\n",
      " [1.16870717e-06 2.92869590e-10 9.98540282e-01 2.76217121e-04\n",
      "  9.88853746e-04 1.93529224e-04]\n",
      " [1.23460923e-04 6.76004774e-19 5.29515312e-08 1.60655750e-13\n",
      "  9.99876261e-01 1.91462973e-07]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:04:00.311804\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21826 samples, validate on 2426 samples\n",
      "Epoch 1/80\n",
      "21826/21826 [==============================] - 1s 49us/step - loss: 0.6434 - accuracy: 0.7704 - val_loss: 0.3323 - val_accuracy: 0.8838\n",
      "Epoch 2/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.2558 - accuracy: 0.9087 - val_loss: 0.2125 - val_accuracy: 0.9262\n",
      "Epoch 3/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.1897 - accuracy: 0.9348 - val_loss: 0.1845 - val_accuracy: 0.9361\n",
      "Epoch 4/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.1663 - accuracy: 0.9406 - val_loss: 0.1683 - val_accuracy: 0.9328\n",
      "Epoch 5/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1499 - accuracy: 0.9436 - val_loss: 0.1532 - val_accuracy: 0.9439\n",
      "Epoch 6/80\n",
      "21826/21826 [==============================] - 1s 34us/step - loss: 0.1365 - accuracy: 0.9480 - val_loss: 0.1367 - val_accuracy: 0.9485\n",
      "Epoch 7/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1314 - accuracy: 0.9490 - val_loss: 0.1305 - val_accuracy: 0.9497\n",
      "Epoch 8/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1242 - accuracy: 0.9509 - val_loss: 0.1515 - val_accuracy: 0.9472\n",
      "Epoch 9/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.1188 - accuracy: 0.9524 - val_loss: 0.1110 - val_accuracy: 0.9497\n",
      "Epoch 10/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1124 - accuracy: 0.9525 - val_loss: 0.1101 - val_accuracy: 0.9497\n",
      "Epoch 11/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1078 - accuracy: 0.9549 - val_loss: 0.1173 - val_accuracy: 0.9485\n",
      "Epoch 12/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1038 - accuracy: 0.9555 - val_loss: 0.1229 - val_accuracy: 0.9485\n",
      "Epoch 13/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1032 - accuracy: 0.9549 - val_loss: 0.1108 - val_accuracy: 0.9534\n",
      "Epoch 14/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.1017 - accuracy: 0.9561 - val_loss: 0.1190 - val_accuracy: 0.9472\n",
      "Epoch 15/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0991 - accuracy: 0.9569 - val_loss: 0.1079 - val_accuracy: 0.9497\n",
      "Epoch 16/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0984 - accuracy: 0.9576 - val_loss: 0.0939 - val_accuracy: 0.9555\n",
      "Epoch 17/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0968 - accuracy: 0.9577 - val_loss: 0.1129 - val_accuracy: 0.9530\n",
      "Epoch 18/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0970 - accuracy: 0.9574 - val_loss: 0.1022 - val_accuracy: 0.9559\n",
      "Epoch 19/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0951 - accuracy: 0.9576 - val_loss: 0.0936 - val_accuracy: 0.9575\n",
      "Epoch 20/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0946 - accuracy: 0.9574 - val_loss: 0.0919 - val_accuracy: 0.9600\n",
      "Epoch 21/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0941 - accuracy: 0.9585 - val_loss: 0.0968 - val_accuracy: 0.9580\n",
      "Epoch 22/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0936 - accuracy: 0.9586 - val_loss: 0.0925 - val_accuracy: 0.9580\n",
      "Epoch 23/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0923 - accuracy: 0.9595 - val_loss: 0.0890 - val_accuracy: 0.9555\n",
      "Epoch 24/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0937 - accuracy: 0.9585 - val_loss: 0.0916 - val_accuracy: 0.9592\n",
      "Epoch 25/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0929 - accuracy: 0.9581 - val_loss: 0.1195 - val_accuracy: 0.9501\n",
      "Epoch 26/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0908 - accuracy: 0.9598 - val_loss: 0.0912 - val_accuracy: 0.9534\n",
      "Epoch 27/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0917 - accuracy: 0.9577 - val_loss: 0.0874 - val_accuracy: 0.9596\n",
      "Epoch 28/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0894 - accuracy: 0.9597 - val_loss: 0.0935 - val_accuracy: 0.9551\n",
      "Epoch 29/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0875 - accuracy: 0.9597 - val_loss: 0.0841 - val_accuracy: 0.9617\n",
      "Epoch 30/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0910 - accuracy: 0.9593 - val_loss: 0.0934 - val_accuracy: 0.9559\n",
      "Epoch 31/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0884 - accuracy: 0.9592 - val_loss: 0.0915 - val_accuracy: 0.9596\n",
      "Epoch 32/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0896 - accuracy: 0.9579 - val_loss: 0.0890 - val_accuracy: 0.9608\n",
      "Epoch 33/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0894 - accuracy: 0.9590 - val_loss: 0.0888 - val_accuracy: 0.9551\n",
      "Epoch 34/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0874 - accuracy: 0.9604 - val_loss: 0.0926 - val_accuracy: 0.9571\n",
      "Epoch 35/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0892 - accuracy: 0.9589 - val_loss: 0.0823 - val_accuracy: 0.9637\n",
      "Epoch 36/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0879 - accuracy: 0.9585 - val_loss: 0.0838 - val_accuracy: 0.9596\n",
      "Epoch 37/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0873 - accuracy: 0.9593 - val_loss: 0.0869 - val_accuracy: 0.9584\n",
      "Epoch 38/80\n",
      "21826/21826 [==============================] - 1s 36us/step - loss: 0.0875 - accuracy: 0.9596 - val_loss: 0.0887 - val_accuracy: 0.9613\n",
      "Epoch 39/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0860 - accuracy: 0.9602 - val_loss: 0.0972 - val_accuracy: 0.9551\n",
      "Epoch 40/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0865 - accuracy: 0.9607 - val_loss: 0.0936 - val_accuracy: 0.9534\n",
      "Epoch 41/80\n",
      "21826/21826 [==============================] - 1s 41us/step - loss: 0.0879 - accuracy: 0.9586 - val_loss: 0.0854 - val_accuracy: 0.9625\n",
      "Epoch 42/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0865 - accuracy: 0.9592 - val_loss: 0.0845 - val_accuracy: 0.9571\n",
      "Epoch 43/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0844 - accuracy: 0.9599 - val_loss: 0.0938 - val_accuracy: 0.9559\n",
      "Epoch 44/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0860 - accuracy: 0.9608 - val_loss: 0.0807 - val_accuracy: 0.9588\n",
      "Epoch 45/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0903 - accuracy: 0.9585 - val_loss: 0.0871 - val_accuracy: 0.9596\n",
      "Epoch 46/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0864 - accuracy: 0.9591 - val_loss: 0.0825 - val_accuracy: 0.9596\n",
      "Epoch 47/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0843 - accuracy: 0.9603 - val_loss: 0.0849 - val_accuracy: 0.9588\n",
      "Epoch 48/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0843 - accuracy: 0.9607 - val_loss: 0.0804 - val_accuracy: 0.9613\n",
      "Epoch 49/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0856 - accuracy: 0.9603 - val_loss: 0.0845 - val_accuracy: 0.9580\n",
      "Epoch 50/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0864 - accuracy: 0.9601 - val_loss: 0.0891 - val_accuracy: 0.9584\n",
      "Epoch 51/80\n",
      "21826/21826 [==============================] - 1s 37us/step - loss: 0.0832 - accuracy: 0.9607 - val_loss: 0.0807 - val_accuracy: 0.9592\n",
      "Epoch 52/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0846 - accuracy: 0.9597 - val_loss: 0.0885 - val_accuracy: 0.9584\n",
      "Epoch 53/80\n",
      "21826/21826 [==============================] - 1s 42us/step - loss: 0.0839 - accuracy: 0.9600 - val_loss: 0.0845 - val_accuracy: 0.9625\n",
      "Epoch 54/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0851 - accuracy: 0.9597 - val_loss: 0.0910 - val_accuracy: 0.9580\n",
      "Epoch 55/80\n",
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0848 - accuracy: 0.9597 - val_loss: 0.0843 - val_accuracy: 0.9584\n",
      "Epoch 56/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21826/21826 [==============================] - 1s 39us/step - loss: 0.0820 - accuracy: 0.9618 - val_loss: 0.0889 - val_accuracy: 0.9555\n",
      "Epoch 57/80\n",
      "21826/21826 [==============================] - 1s 40us/step - loss: 0.0847 - accuracy: 0.9614 - val_loss: 0.0823 - val_accuracy: 0.9613\n",
      "Epoch 58/80\n",
      "21826/21826 [==============================] - 1s 38us/step - loss: 0.0819 - accuracy: 0.9611 - val_loss: 0.0815 - val_accuracy: 0.9588\n",
      "Epoch 00058: early stopping\n",
      "[[1.00000000e+00 4.64212653e-13 2.76665718e-10 8.66401256e-11\n",
      "  3.70443173e-12 2.82537904e-12]\n",
      " [1.06044725e-04 4.34755520e-06 2.71449480e-05 1.30700700e-10\n",
      "  6.52177505e-06 9.99855876e-01]\n",
      " [1.04828912e-04 4.28304566e-06 2.69666943e-05 1.28706712e-10\n",
      "  6.53760117e-06 9.99857306e-01]\n",
      " ...\n",
      " [5.64036551e-09 2.32505961e-08 9.96475518e-01 3.52281332e-03\n",
      "  4.09871876e-12 1.61096591e-06]\n",
      " [4.39611686e-05 1.92554111e-12 9.95057702e-01 8.17780616e-04\n",
      "  3.78545257e-03 2.95128208e-04]\n",
      " [2.90326752e-05 4.38873743e-17 8.64377762e-06 2.46243802e-12\n",
      "  9.99893785e-01 6.84860861e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:04:52.898896\n",
      "region_label= [6, 7, 2, 4, 3, 5]\n",
      "dist_table_truth\n",
      " [[1332  865    0 1615   47  306    0    0    0]\n",
      " [ 621    0    2   95  210    3    0    0    0]\n",
      " [ 159  529 2153   25 1092  602    0    0    0]\n",
      " [ 209 1260 2272    0 2125  258    0    0    0]\n",
      " [2265  521   30 2523   54 1374    0    0    0]\n",
      " [ 167  713   12  150  237 1730    0    0    0]]\n",
      "num of merged_region_image 0 4129\n",
      "num of merged_region_image 1 4508\n",
      "num of merged_region_image 2 4228\n",
      "num of merged_region_image 3 4206\n",
      "num of merged_region_image 4 3698\n",
      "num of merged_region_image 5 3483\n",
      "Counter({-1: 18965, 1: 4508, 2: 4228, 3: 4206, 0: 4129, 4: 3698, 5: 3483})\n",
      "===========  ITE = 2   ===========\n",
      "used_img 24252 24252\n",
      "working_img(=other images=unclean images) 18965 18965\n",
      "merged regions 118 118\n",
      "other_regions 82 82\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.49</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.76</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.99</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>195</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>200</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    other index  pred label  truth  rate    0    1  2  3   4    5\n",
       "0             8           5      5  0.39    0    3  0  0   0  361\n",
       "1             9           5      0  0.49  245    0  1  0   1    0\n",
       "2            12           2      4  0.76   13    0  0  0  77    2\n",
       "3            13           0      1  1.00    0   26  0  0   0    0\n",
       "4            16           4      8  0.99    8    0  0  3   0    1\n",
       "..          ...         ...    ...   ...  ...  ... .. ..  ..  ...\n",
       "77          194           0      1  1.00    1  111  0  0   0    2\n",
       "78          195           4      1  0.70    2   21  0  0   0   17\n",
       "79          196           4      0  0.65  372    2  1  1   4    1\n",
       "80          199           2      8  0.66    0    0  2  0   0    0\n",
       "81          200           4      0  1.00  260    0  0  0   1    0\n",
       "\n",
       "[82 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [13, 194, 30, 81, 95] 869\n",
      "added label, regions, img amount: {1} [136, 31, 142] 682\n",
      "added label, regions, img amount: {2} [80, 99, 131, 19, 83] 664\n",
      "added label, regions, img amount: {3} [29, 51, 111, 177, 122] 542\n",
      "added label, regions, img amount: {4} [101, 59, 200, 16, 173] 849\n",
      "added label, regions, img amount: {5} [40] 154\n",
      "Not getting into residuals\n",
      "NUM_region 6\n",
      "number of clean images 28012\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 6\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25210 samples, validate on 2802 samples\n",
      "Epoch 1/80\n",
      "25210/25210 [==============================] - 1s 57us/step - loss: 0.6606 - accuracy: 0.7539 - val_loss: 0.4029 - val_accuracy: 0.8697\n",
      "Epoch 2/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.3117 - accuracy: 0.8915 - val_loss: 0.2693 - val_accuracy: 0.8994\n",
      "Epoch 3/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.2291 - accuracy: 0.9201 - val_loss: 0.2617 - val_accuracy: 0.8783\n",
      "Epoch 4/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.2000 - accuracy: 0.9299 - val_loss: 0.1943 - val_accuracy: 0.9226\n",
      "Epoch 5/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1660 - accuracy: 0.9410 - val_loss: 0.1535 - val_accuracy: 0.9404\n",
      "Epoch 6/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1470 - accuracy: 0.9486 - val_loss: 0.1382 - val_accuracy: 0.9468\n",
      "Epoch 7/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.1301 - accuracy: 0.9537 - val_loss: 0.1323 - val_accuracy: 0.9486\n",
      "Epoch 8/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1242 - accuracy: 0.9547 - val_loss: 0.1298 - val_accuracy: 0.9500\n",
      "Epoch 9/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1163 - accuracy: 0.9585 - val_loss: 0.1180 - val_accuracy: 0.9550\n",
      "Epoch 10/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1169 - accuracy: 0.9590 - val_loss: 0.1132 - val_accuracy: 0.9586\n",
      "Epoch 11/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1091 - accuracy: 0.9608 - val_loss: 0.1098 - val_accuracy: 0.9604\n",
      "Epoch 12/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1070 - accuracy: 0.9619 - val_loss: 0.1166 - val_accuracy: 0.9529\n",
      "Epoch 13/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1054 - accuracy: 0.9629 - val_loss: 0.1092 - val_accuracy: 0.9618\n",
      "Epoch 14/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0996 - accuracy: 0.9639 - val_loss: 0.1083 - val_accuracy: 0.9582\n",
      "Epoch 15/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1006 - accuracy: 0.9632 - val_loss: 0.1096 - val_accuracy: 0.9597\n",
      "Epoch 16/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.1005 - accuracy: 0.9629 - val_loss: 0.1050 - val_accuracy: 0.9625\n",
      "Epoch 17/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.1020 - accuracy: 0.9618 - val_loss: 0.1117 - val_accuracy: 0.9540\n",
      "Epoch 18/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0956 - accuracy: 0.9647 - val_loss: 0.0979 - val_accuracy: 0.9611\n",
      "Epoch 19/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0957 - accuracy: 0.9645 - val_loss: 0.1020 - val_accuracy: 0.9629\n",
      "Epoch 20/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0940 - accuracy: 0.9643 - val_loss: 0.1042 - val_accuracy: 0.9604\n",
      "Epoch 21/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0971 - accuracy: 0.9629 - val_loss: 0.1085 - val_accuracy: 0.9565\n",
      "Epoch 22/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0930 - accuracy: 0.9644 - val_loss: 0.0956 - val_accuracy: 0.9615\n",
      "Epoch 23/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0915 - accuracy: 0.9647 - val_loss: 0.1049 - val_accuracy: 0.9575\n",
      "Epoch 24/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0907 - accuracy: 0.9652 - val_loss: 0.0966 - val_accuracy: 0.9582\n",
      "Epoch 25/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0881 - accuracy: 0.9655 - val_loss: 0.0966 - val_accuracy: 0.9597\n",
      "Epoch 26/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0875 - accuracy: 0.9655 - val_loss: 0.0985 - val_accuracy: 0.9622\n",
      "Epoch 27/80\n",
      "25210/25210 [==============================] - 1s 35us/step - loss: 0.0869 - accuracy: 0.9651 - val_loss: 0.0895 - val_accuracy: 0.9650\n",
      "Epoch 28/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0877 - accuracy: 0.9641 - val_loss: 0.0926 - val_accuracy: 0.9590\n",
      "Epoch 29/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0869 - accuracy: 0.9641 - val_loss: 0.1018 - val_accuracy: 0.9575\n",
      "Epoch 30/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0831 - accuracy: 0.9655 - val_loss: 0.0930 - val_accuracy: 0.9607\n",
      "Epoch 31/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0855 - accuracy: 0.9651 - val_loss: 0.0920 - val_accuracy: 0.9618\n",
      "Epoch 32/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0826 - accuracy: 0.9654 - val_loss: 0.0881 - val_accuracy: 0.9647\n",
      "Epoch 33/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0837 - accuracy: 0.9657 - val_loss: 0.0861 - val_accuracy: 0.9632\n",
      "Epoch 34/80\n",
      "25210/25210 [==============================] - 1s 43us/step - loss: 0.0849 - accuracy: 0.9637 - val_loss: 0.0969 - val_accuracy: 0.9582\n",
      "Epoch 35/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0843 - accuracy: 0.9641 - val_loss: 0.0932 - val_accuracy: 0.9586\n",
      "Epoch 36/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0819 - accuracy: 0.9649 - val_loss: 0.0828 - val_accuracy: 0.9632\n",
      "Epoch 37/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0795 - accuracy: 0.9656 - val_loss: 0.0948 - val_accuracy: 0.9622\n",
      "Epoch 38/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0815 - accuracy: 0.9653 - val_loss: 0.0834 - val_accuracy: 0.9607\n",
      "Epoch 39/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0808 - accuracy: 0.9640 - val_loss: 0.1005 - val_accuracy: 0.9590\n",
      "Epoch 40/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0809 - accuracy: 0.9665 - val_loss: 0.0973 - val_accuracy: 0.9629\n",
      "Epoch 41/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0789 - accuracy: 0.9660 - val_loss: 0.0820 - val_accuracy: 0.9657\n",
      "Epoch 42/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0771 - accuracy: 0.9657 - val_loss: 0.0954 - val_accuracy: 0.9632\n",
      "Epoch 43/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0803 - accuracy: 0.9647 - val_loss: 0.0942 - val_accuracy: 0.9597\n",
      "Epoch 44/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0777 - accuracy: 0.9653 - val_loss: 0.0826 - val_accuracy: 0.9611\n",
      "Epoch 45/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0774 - accuracy: 0.9660 - val_loss: 0.0816 - val_accuracy: 0.9650\n",
      "Epoch 46/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0787 - accuracy: 0.9658 - val_loss: 0.0876 - val_accuracy: 0.9604\n",
      "Epoch 47/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0780 - accuracy: 0.9663 - val_loss: 0.0796 - val_accuracy: 0.9625\n",
      "Epoch 48/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0778 - accuracy: 0.9641 - val_loss: 0.0842 - val_accuracy: 0.9657\n",
      "Epoch 49/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0771 - accuracy: 0.9652 - val_loss: 0.0777 - val_accuracy: 0.9618\n",
      "Epoch 50/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0770 - accuracy: 0.9658 - val_loss: 0.0888 - val_accuracy: 0.9611\n",
      "Epoch 51/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0789 - accuracy: 0.9651 - val_loss: 0.0936 - val_accuracy: 0.9625\n",
      "Epoch 52/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0740 - accuracy: 0.9659 - val_loss: 0.0775 - val_accuracy: 0.9611\n",
      "Epoch 53/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0755 - accuracy: 0.9668 - val_loss: 0.0814 - val_accuracy: 0.9579\n",
      "Epoch 54/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0751 - accuracy: 0.9655 - val_loss: 0.0800 - val_accuracy: 0.9654\n",
      "Epoch 55/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0774 - accuracy: 0.9653 - val_loss: 0.0785 - val_accuracy: 0.9629\n",
      "Epoch 56/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0753 - accuracy: 0.9665 - val_loss: 0.0786 - val_accuracy: 0.9668\n",
      "Epoch 57/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0738 - accuracy: 0.9660 - val_loss: 0.0787 - val_accuracy: 0.9629\n",
      "Epoch 58/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0740 - accuracy: 0.9659 - val_loss: 0.0779 - val_accuracy: 0.9611\n",
      "Epoch 59/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0746 - accuracy: 0.9666 - val_loss: 0.0822 - val_accuracy: 0.9636\n",
      "Epoch 60/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0749 - accuracy: 0.9659 - val_loss: 0.0794 - val_accuracy: 0.9675\n",
      "Epoch 61/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0728 - accuracy: 0.9668 - val_loss: 0.0754 - val_accuracy: 0.9679\n",
      "Epoch 62/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0729 - accuracy: 0.9658 - val_loss: 0.0819 - val_accuracy: 0.9590\n",
      "Epoch 63/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0722 - accuracy: 0.9663 - val_loss: 0.0771 - val_accuracy: 0.9647\n",
      "Epoch 64/80\n",
      "25210/25210 [==============================] - 1s 35us/step - loss: 0.0716 - accuracy: 0.9672 - val_loss: 0.0792 - val_accuracy: 0.9593\n",
      "Epoch 65/80\n",
      "25210/25210 [==============================] - 1s 34us/step - loss: 0.0724 - accuracy: 0.9660 - val_loss: 0.0770 - val_accuracy: 0.9632\n",
      "Epoch 66/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0730 - accuracy: 0.9659 - val_loss: 0.0762 - val_accuracy: 0.9643\n",
      "Epoch 67/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0743 - accuracy: 0.9664 - val_loss: 0.0811 - val_accuracy: 0.9654\n",
      "Epoch 68/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0742 - accuracy: 0.9659 - val_loss: 0.0773 - val_accuracy: 0.9672\n",
      "Epoch 69/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0715 - accuracy: 0.9662 - val_loss: 0.0757 - val_accuracy: 0.9661\n",
      "Epoch 70/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0768 - accuracy: 0.9646 - val_loss: 0.0733 - val_accuracy: 0.9647\n",
      "Epoch 71/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0711 - accuracy: 0.9672 - val_loss: 0.0786 - val_accuracy: 0.9632\n",
      "Epoch 72/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0755 - accuracy: 0.9653 - val_loss: 0.0763 - val_accuracy: 0.9643\n",
      "Epoch 73/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0720 - accuracy: 0.9667 - val_loss: 0.0753 - val_accuracy: 0.9632\n",
      "Epoch 74/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0698 - accuracy: 0.9673 - val_loss: 0.0768 - val_accuracy: 0.9611\n",
      "Epoch 75/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0714 - accuracy: 0.9668 - val_loss: 0.0797 - val_accuracy: 0.9629\n",
      "Epoch 76/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0724 - accuracy: 0.9653 - val_loss: 0.0780 - val_accuracy: 0.9625\n",
      "Epoch 77/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0716 - accuracy: 0.9659 - val_loss: 0.0745 - val_accuracy: 0.9661\n",
      "Epoch 78/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0711 - accuracy: 0.9662 - val_loss: 0.0740 - val_accuracy: 0.9668\n",
      "Epoch 79/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0712 - accuracy: 0.9662 - val_loss: 0.0763 - val_accuracy: 0.9650\n",
      "Epoch 80/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0707 - accuracy: 0.9662 - val_loss: 0.0738 - val_accuracy: 0.9675\n",
      "Epoch 00080: early stopping\n",
      "[[1.0000000e+00 3.5455773e-23 5.2739051e-19 1.1111816e-13 1.8281194e-13\n",
      "  3.2362617e-16]\n",
      " [1.6764008e-05 7.0033231e-08 1.4360707e-07 2.6544669e-09 9.1766033e-06\n",
      "  9.9997389e-01]\n",
      " [1.6493726e-05 6.8809143e-08 1.4407487e-07 2.6288967e-09 9.1555557e-06\n",
      "  9.9997413e-01]\n",
      " ...\n",
      " [5.0524912e-10 3.8266812e-10 9.9950814e-01 4.9168430e-04 2.9703958e-16\n",
      "  1.0513268e-07]\n",
      " [2.1750712e-07 3.5731367e-17 9.9759549e-01 4.1791628e-04 1.9850747e-03\n",
      "  1.3165231e-06]\n",
      " [1.4384772e-06 7.5304362e-19 2.0260024e-10 1.7739586e-11 9.9999595e-01\n",
      "  2.6070818e-06]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:01:22.499996\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25210 samples, validate on 2802 samples\n",
      "Epoch 1/80\n",
      "25210/25210 [==============================] - 1s 48us/step - loss: 0.7083 - accuracy: 0.7028 - val_loss: 0.3352 - val_accuracy: 0.8815\n",
      "Epoch 2/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.2221 - accuracy: 0.9222 - val_loss: 0.1776 - val_accuracy: 0.9415\n",
      "Epoch 3/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1483 - accuracy: 0.9441 - val_loss: 0.1334 - val_accuracy: 0.9515\n",
      "Epoch 4/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1195 - accuracy: 0.9547 - val_loss: 0.1064 - val_accuracy: 0.9582\n",
      "Epoch 5/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.1041 - accuracy: 0.9604 - val_loss: 0.1033 - val_accuracy: 0.9586\n",
      "Epoch 6/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.1012 - accuracy: 0.9615 - val_loss: 0.0906 - val_accuracy: 0.9643\n",
      "Epoch 7/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0939 - accuracy: 0.9631 - val_loss: 0.0897 - val_accuracy: 0.9632\n",
      "Epoch 8/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0943 - accuracy: 0.9623 - val_loss: 0.0916 - val_accuracy: 0.9661\n",
      "Epoch 9/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0914 - accuracy: 0.9631 - val_loss: 0.0897 - val_accuracy: 0.9611\n",
      "Epoch 10/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0896 - accuracy: 0.9640 - val_loss: 0.1042 - val_accuracy: 0.9554\n",
      "Epoch 11/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0893 - accuracy: 0.9632 - val_loss: 0.0811 - val_accuracy: 0.9679\n",
      "Epoch 12/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0897 - accuracy: 0.9637 - val_loss: 0.0832 - val_accuracy: 0.9643\n",
      "Epoch 13/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0888 - accuracy: 0.9625 - val_loss: 0.0842 - val_accuracy: 0.9629\n",
      "Epoch 14/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0874 - accuracy: 0.9633 - val_loss: 0.0873 - val_accuracy: 0.9622\n",
      "Epoch 15/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0853 - accuracy: 0.9635 - val_loss: 0.0788 - val_accuracy: 0.9629\n",
      "Epoch 16/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0843 - accuracy: 0.9628 - val_loss: 0.0795 - val_accuracy: 0.9632\n",
      "Epoch 17/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0857 - accuracy: 0.9636 - val_loss: 0.0812 - val_accuracy: 0.9611\n",
      "Epoch 18/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0871 - accuracy: 0.9627 - val_loss: 0.0924 - val_accuracy: 0.9593\n",
      "Epoch 19/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0848 - accuracy: 0.9630 - val_loss: 0.0737 - val_accuracy: 0.9661\n",
      "Epoch 20/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0848 - accuracy: 0.9628 - val_loss: 0.0812 - val_accuracy: 0.9643\n",
      "Epoch 21/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0833 - accuracy: 0.9626 - val_loss: 0.0936 - val_accuracy: 0.9582\n",
      "Epoch 22/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0808 - accuracy: 0.9645 - val_loss: 0.0966 - val_accuracy: 0.9550\n",
      "Epoch 23/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0822 - accuracy: 0.9637 - val_loss: 0.0759 - val_accuracy: 0.9690\n",
      "Epoch 24/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0846 - accuracy: 0.9618 - val_loss: 0.0845 - val_accuracy: 0.9611\n",
      "Epoch 25/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0794 - accuracy: 0.9643 - val_loss: 0.0735 - val_accuracy: 0.9657\n",
      "Epoch 26/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0795 - accuracy: 0.9641 - val_loss: 0.0770 - val_accuracy: 0.9629\n",
      "Epoch 27/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0782 - accuracy: 0.9652 - val_loss: 0.0711 - val_accuracy: 0.9697\n",
      "Epoch 28/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0900 - accuracy: 0.9612 - val_loss: 0.0780 - val_accuracy: 0.9647\n",
      "Epoch 29/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0801 - accuracy: 0.9635 - val_loss: 0.0741 - val_accuracy: 0.9675\n",
      "Epoch 30/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0808 - accuracy: 0.9649 - val_loss: 0.0713 - val_accuracy: 0.9661\n",
      "Epoch 31/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0776 - accuracy: 0.9644 - val_loss: 0.0746 - val_accuracy: 0.9665\n",
      "Epoch 32/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0768 - accuracy: 0.9659 - val_loss: 0.0710 - val_accuracy: 0.9693\n",
      "Epoch 33/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0772 - accuracy: 0.9654 - val_loss: 0.0682 - val_accuracy: 0.9693\n",
      "Epoch 34/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0781 - accuracy: 0.9646 - val_loss: 0.0737 - val_accuracy: 0.9657\n",
      "Epoch 35/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0756 - accuracy: 0.9645 - val_loss: 0.0732 - val_accuracy: 0.9686\n",
      "Epoch 36/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0787 - accuracy: 0.9646 - val_loss: 0.0762 - val_accuracy: 0.9675\n",
      "Epoch 37/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0788 - accuracy: 0.9642 - val_loss: 0.0750 - val_accuracy: 0.9654\n",
      "Epoch 38/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0783 - accuracy: 0.9637 - val_loss: 0.0808 - val_accuracy: 0.9636\n",
      "Epoch 39/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0778 - accuracy: 0.9648 - val_loss: 0.0698 - val_accuracy: 0.9668\n",
      "Epoch 40/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0787 - accuracy: 0.9644 - val_loss: 0.0790 - val_accuracy: 0.9640\n",
      "Epoch 41/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0778 - accuracy: 0.9649 - val_loss: 0.0683 - val_accuracy: 0.9679\n",
      "Epoch 42/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0846 - accuracy: 0.9624 - val_loss: 0.0753 - val_accuracy: 0.9625\n",
      "Epoch 43/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0757 - accuracy: 0.9658 - val_loss: 0.0680 - val_accuracy: 0.9679\n",
      "Epoch 44/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0759 - accuracy: 0.9651 - val_loss: 0.0671 - val_accuracy: 0.9697\n",
      "Epoch 45/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0739 - accuracy: 0.9671 - val_loss: 0.0668 - val_accuracy: 0.9675\n",
      "Epoch 46/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0740 - accuracy: 0.9651 - val_loss: 0.0726 - val_accuracy: 0.9647\n",
      "Epoch 47/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0746 - accuracy: 0.9658 - val_loss: 0.0787 - val_accuracy: 0.9672\n",
      "Epoch 48/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0750 - accuracy: 0.9657 - val_loss: 0.0855 - val_accuracy: 0.9604\n",
      "Epoch 49/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0766 - accuracy: 0.9656 - val_loss: 0.0784 - val_accuracy: 0.9611\n",
      "Epoch 50/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0733 - accuracy: 0.9657 - val_loss: 0.0713 - val_accuracy: 0.9682\n",
      "Epoch 51/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0784 - accuracy: 0.9652 - val_loss: 0.0765 - val_accuracy: 0.9665\n",
      "Epoch 52/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0745 - accuracy: 0.9653 - val_loss: 0.0738 - val_accuracy: 0.9679\n",
      "Epoch 53/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0740 - accuracy: 0.9658 - val_loss: 0.0683 - val_accuracy: 0.9704\n",
      "Epoch 54/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0751 - accuracy: 0.9643 - val_loss: 0.0674 - val_accuracy: 0.9682\n",
      "Epoch 55/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0816 - accuracy: 0.9647 - val_loss: 0.0668 - val_accuracy: 0.9697\n",
      "Epoch 56/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0760 - accuracy: 0.9643 - val_loss: 0.0663 - val_accuracy: 0.9693\n",
      "Epoch 57/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0751 - accuracy: 0.9650 - val_loss: 0.0723 - val_accuracy: 0.9672\n",
      "Epoch 58/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0748 - accuracy: 0.9650 - val_loss: 0.0754 - val_accuracy: 0.9661\n",
      "Epoch 59/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0731 - accuracy: 0.9655 - val_loss: 0.0690 - val_accuracy: 0.9657\n",
      "Epoch 60/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0744 - accuracy: 0.9662 - val_loss: 0.0664 - val_accuracy: 0.9700\n",
      "Epoch 61/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0748 - accuracy: 0.9653 - val_loss: 0.0773 - val_accuracy: 0.9672\n",
      "Epoch 62/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0737 - accuracy: 0.9656 - val_loss: 0.0722 - val_accuracy: 0.9675\n",
      "Epoch 63/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0746 - accuracy: 0.9648 - val_loss: 0.0684 - val_accuracy: 0.9700\n",
      "Epoch 64/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0723 - accuracy: 0.9667 - val_loss: 0.0664 - val_accuracy: 0.9686\n",
      "Epoch 65/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0732 - accuracy: 0.9649 - val_loss: 0.0672 - val_accuracy: 0.9679\n",
      "Epoch 66/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0734 - accuracy: 0.9655 - val_loss: 0.0656 - val_accuracy: 0.9700\n",
      "Epoch 67/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0716 - accuracy: 0.9667 - val_loss: 0.0647 - val_accuracy: 0.9700\n",
      "Epoch 68/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0745 - accuracy: 0.9659 - val_loss: 0.0726 - val_accuracy: 0.9690\n",
      "Epoch 69/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0753 - accuracy: 0.9646 - val_loss: 0.0782 - val_accuracy: 0.9600\n",
      "Epoch 70/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0758 - accuracy: 0.9634 - val_loss: 0.0674 - val_accuracy: 0.9690\n",
      "Epoch 71/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0745 - accuracy: 0.9660 - val_loss: 0.0683 - val_accuracy: 0.9675\n",
      "Epoch 72/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0736 - accuracy: 0.9656 - val_loss: 0.0704 - val_accuracy: 0.9707\n",
      "Epoch 73/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0735 - accuracy: 0.9656 - val_loss: 0.0694 - val_accuracy: 0.9679\n",
      "Epoch 74/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0742 - accuracy: 0.9652 - val_loss: 0.0710 - val_accuracy: 0.9661\n",
      "Epoch 75/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0746 - accuracy: 0.9645 - val_loss: 0.0716 - val_accuracy: 0.9654\n",
      "Epoch 76/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0745 - accuracy: 0.9649 - val_loss: 0.0675 - val_accuracy: 0.9722\n",
      "Epoch 77/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0734 - accuracy: 0.9651 - val_loss: 0.0738 - val_accuracy: 0.9629\n",
      "Epoch 00077: early stopping\n",
      "[[1.00000000e+00 2.39895213e-12 1.01396809e-20 3.53637315e-26\n",
      "  3.02559490e-25 7.33085874e-14]\n",
      " [2.15394313e-10 3.50412250e-08 1.13180285e-07 1.21660212e-19\n",
      "  5.33208286e-08 9.99999881e-01]\n",
      " [2.02475106e-10 3.44587718e-08 1.13615023e-07 1.19929944e-19\n",
      "  5.37575779e-08 9.99999881e-01]\n",
      " ...\n",
      " [3.92425494e-19 1.05174947e-09 9.99998689e-01 1.14903787e-06\n",
      "  1.86715946e-16 6.89721915e-08]\n",
      " [1.59118500e-11 2.44020998e-10 9.99907851e-01 5.92461838e-05\n",
      "  9.38623373e-07 3.20022773e-05]\n",
      " [1.96991849e-07 4.98567160e-21 1.07742792e-09 2.85518120e-11\n",
      "  9.99977112e-01 2.26087832e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:02:43.140625\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25210 samples, validate on 2802 samples\n",
      "Epoch 1/80\n",
      "25210/25210 [==============================] - 1s 48us/step - loss: 0.5915 - accuracy: 0.7796 - val_loss: 0.3105 - val_accuracy: 0.8844\n",
      "Epoch 2/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.2408 - accuracy: 0.9130 - val_loss: 0.2106 - val_accuracy: 0.9290\n",
      "Epoch 3/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1774 - accuracy: 0.9365 - val_loss: 0.1661 - val_accuracy: 0.9418\n",
      "Epoch 4/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.1407 - accuracy: 0.9497 - val_loss: 0.1404 - val_accuracy: 0.9475\n",
      "Epoch 5/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1252 - accuracy: 0.9554 - val_loss: 0.1326 - val_accuracy: 0.9500\n",
      "Epoch 6/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1180 - accuracy: 0.9577 - val_loss: 0.1198 - val_accuracy: 0.9550\n",
      "Epoch 7/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1113 - accuracy: 0.9605 - val_loss: 0.1117 - val_accuracy: 0.9582\n",
      "Epoch 8/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1079 - accuracy: 0.9615 - val_loss: 0.1083 - val_accuracy: 0.9582\n",
      "Epoch 9/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.1024 - accuracy: 0.9632 - val_loss: 0.1075 - val_accuracy: 0.9572\n",
      "Epoch 10/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0990 - accuracy: 0.9638 - val_loss: 0.1061 - val_accuracy: 0.9611\n",
      "Epoch 11/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0971 - accuracy: 0.9639 - val_loss: 0.1029 - val_accuracy: 0.9604\n",
      "Epoch 12/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0998 - accuracy: 0.9642 - val_loss: 0.1006 - val_accuracy: 0.9604\n",
      "Epoch 13/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0942 - accuracy: 0.9642 - val_loss: 0.1038 - val_accuracy: 0.9607\n",
      "Epoch 14/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0936 - accuracy: 0.9642 - val_loss: 0.1066 - val_accuracy: 0.9590\n",
      "Epoch 15/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0906 - accuracy: 0.9644 - val_loss: 0.0939 - val_accuracy: 0.9625\n",
      "Epoch 16/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0914 - accuracy: 0.9647 - val_loss: 0.1141 - val_accuracy: 0.9579\n",
      "Epoch 17/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0882 - accuracy: 0.9649 - val_loss: 0.0899 - val_accuracy: 0.9618\n",
      "Epoch 18/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0909 - accuracy: 0.9645 - val_loss: 0.0908 - val_accuracy: 0.9622\n",
      "Epoch 19/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0881 - accuracy: 0.9653 - val_loss: 0.0923 - val_accuracy: 0.9597\n",
      "Epoch 20/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0870 - accuracy: 0.9653 - val_loss: 0.0871 - val_accuracy: 0.9632\n",
      "Epoch 21/80\n",
      "25210/25210 [==============================] - 1s 47us/step - loss: 0.0873 - accuracy: 0.9649 - val_loss: 0.0832 - val_accuracy: 0.9632\n",
      "Epoch 22/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0862 - accuracy: 0.9649 - val_loss: 0.1025 - val_accuracy: 0.9590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0830 - accuracy: 0.9655 - val_loss: 0.0879 - val_accuracy: 0.9661\n",
      "Epoch 24/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0848 - accuracy: 0.9643 - val_loss: 0.0823 - val_accuracy: 0.9625\n",
      "Epoch 25/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0855 - accuracy: 0.9645 - val_loss: 0.0813 - val_accuracy: 0.9668\n",
      "Epoch 26/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0829 - accuracy: 0.9659 - val_loss: 0.0819 - val_accuracy: 0.9650\n",
      "Epoch 27/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0816 - accuracy: 0.9656 - val_loss: 0.0852 - val_accuracy: 0.9682\n",
      "Epoch 28/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0826 - accuracy: 0.9644 - val_loss: 0.0836 - val_accuracy: 0.9636\n",
      "Epoch 29/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0802 - accuracy: 0.9654 - val_loss: 0.0788 - val_accuracy: 0.9654\n",
      "Epoch 30/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0795 - accuracy: 0.9662 - val_loss: 0.0877 - val_accuracy: 0.9625\n",
      "Epoch 31/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0799 - accuracy: 0.9662 - val_loss: 0.0777 - val_accuracy: 0.9643\n",
      "Epoch 32/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0795 - accuracy: 0.9658 - val_loss: 0.0858 - val_accuracy: 0.9622\n",
      "Epoch 33/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0797 - accuracy: 0.9653 - val_loss: 0.0806 - val_accuracy: 0.9686\n",
      "Epoch 34/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0805 - accuracy: 0.9644 - val_loss: 0.0773 - val_accuracy: 0.9686\n",
      "Epoch 35/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0769 - accuracy: 0.9664 - val_loss: 0.0857 - val_accuracy: 0.9625\n",
      "Epoch 36/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0765 - accuracy: 0.9661 - val_loss: 0.0777 - val_accuracy: 0.9650\n",
      "Epoch 37/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0813 - accuracy: 0.9645 - val_loss: 0.0851 - val_accuracy: 0.9665\n",
      "Epoch 38/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0772 - accuracy: 0.9652 - val_loss: 0.0842 - val_accuracy: 0.9625\n",
      "Epoch 39/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0772 - accuracy: 0.9652 - val_loss: 0.0873 - val_accuracy: 0.9615\n",
      "Epoch 40/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0769 - accuracy: 0.9652 - val_loss: 0.0751 - val_accuracy: 0.9647\n",
      "Epoch 41/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0760 - accuracy: 0.9658 - val_loss: 0.0801 - val_accuracy: 0.9629\n",
      "Epoch 42/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0775 - accuracy: 0.9653 - val_loss: 0.0842 - val_accuracy: 0.9654\n",
      "Epoch 43/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0784 - accuracy: 0.9638 - val_loss: 0.0843 - val_accuracy: 0.9622\n",
      "Epoch 44/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0742 - accuracy: 0.9654 - val_loss: 0.0825 - val_accuracy: 0.9629\n",
      "Epoch 45/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0756 - accuracy: 0.9653 - val_loss: 0.0778 - val_accuracy: 0.9643\n",
      "Epoch 46/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0753 - accuracy: 0.9660 - val_loss: 0.1016 - val_accuracy: 0.9582\n",
      "Epoch 47/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0751 - accuracy: 0.9658 - val_loss: 0.0723 - val_accuracy: 0.9661\n",
      "Epoch 48/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0767 - accuracy: 0.9647 - val_loss: 0.0798 - val_accuracy: 0.9640\n",
      "Epoch 49/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0774 - accuracy: 0.9645 - val_loss: 0.0736 - val_accuracy: 0.9643\n",
      "Epoch 50/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0747 - accuracy: 0.9653 - val_loss: 0.0814 - val_accuracy: 0.9629\n",
      "Epoch 51/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.0761 - accuracy: 0.9649 - val_loss: 0.0783 - val_accuracy: 0.9643\n",
      "Epoch 52/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0740 - accuracy: 0.9659 - val_loss: 0.0764 - val_accuracy: 0.9632\n",
      "Epoch 53/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0764 - accuracy: 0.9649 - val_loss: 0.0812 - val_accuracy: 0.9640\n",
      "Epoch 54/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0746 - accuracy: 0.9655 - val_loss: 0.0741 - val_accuracy: 0.9647\n",
      "Epoch 55/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0727 - accuracy: 0.9660 - val_loss: 0.0739 - val_accuracy: 0.9668\n",
      "Epoch 56/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0733 - accuracy: 0.9661 - val_loss: 0.0813 - val_accuracy: 0.9647\n",
      "Epoch 57/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0752 - accuracy: 0.9658 - val_loss: 0.0741 - val_accuracy: 0.9679\n",
      "Epoch 00057: early stopping\n",
      "[[1.0000000e+00 1.9854092e-23 2.0912562e-10 8.2521110e-09 6.7347901e-13\n",
      "  7.1200580e-14]\n",
      " [3.8805474e-06 1.4748231e-07 1.2983109e-07 3.6542027e-09 2.5030875e-05\n",
      "  9.9997079e-01]\n",
      " [3.7862822e-06 1.4505292e-07 1.2974022e-07 3.5677965e-09 2.5413121e-05\n",
      "  9.9997056e-01]\n",
      " ...\n",
      " [5.6290680e-11 1.6086644e-09 9.9997938e-01 2.0612686e-05 2.6772815e-20\n",
      "  6.5113959e-09]\n",
      " [1.2054937e-07 2.5895682e-16 9.9556512e-01 3.6560318e-03 5.6718907e-04\n",
      "  2.1143674e-04]\n",
      " [4.6782425e-08 1.2455103e-26 3.7482128e-08 1.2605221e-10 9.9999988e-01\n",
      "  5.5401306e-09]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:03:44.928538\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25210 samples, validate on 2802 samples\n",
      "Epoch 1/80\n",
      "25210/25210 [==============================] - 1s 53us/step - loss: 0.6488 - accuracy: 0.7554 - val_loss: 0.3702 - val_accuracy: 0.8797\n",
      "Epoch 2/80\n",
      "25210/25210 [==============================] - 1s 43us/step - loss: 0.2907 - accuracy: 0.8975 - val_loss: 0.2698 - val_accuracy: 0.9147\n",
      "Epoch 3/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.2279 - accuracy: 0.9206 - val_loss: 0.2149 - val_accuracy: 0.9304\n",
      "Epoch 4/80\n",
      "25210/25210 [==============================] - 1s 42us/step - loss: 0.2033 - accuracy: 0.9275 - val_loss: 0.1763 - val_accuracy: 0.9372\n",
      "Epoch 5/80\n",
      "25210/25210 [==============================] - 1s 43us/step - loss: 0.1761 - accuracy: 0.9348 - val_loss: 0.1798 - val_accuracy: 0.9161\n",
      "Epoch 6/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.1445 - accuracy: 0.9472 - val_loss: 0.1282 - val_accuracy: 0.9607\n",
      "Epoch 7/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1210 - accuracy: 0.9566 - val_loss: 0.1086 - val_accuracy: 0.9611\n",
      "Epoch 8/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.1038 - accuracy: 0.9623 - val_loss: 0.0957 - val_accuracy: 0.9661\n",
      "Epoch 9/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0973 - accuracy: 0.9633 - val_loss: 0.0919 - val_accuracy: 0.9650\n",
      "Epoch 10/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0948 - accuracy: 0.9629 - val_loss: 0.0889 - val_accuracy: 0.9615\n",
      "Epoch 11/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0941 - accuracy: 0.9621 - val_loss: 0.0878 - val_accuracy: 0.9650\n",
      "Epoch 12/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0941 - accuracy: 0.9620 - val_loss: 0.0905 - val_accuracy: 0.9607\n",
      "Epoch 13/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0914 - accuracy: 0.9617 - val_loss: 0.0872 - val_accuracy: 0.9625\n",
      "Epoch 14/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0887 - accuracy: 0.9625 - val_loss: 0.0927 - val_accuracy: 0.9615\n",
      "Epoch 15/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0880 - accuracy: 0.9634 - val_loss: 0.0871 - val_accuracy: 0.9654\n",
      "Epoch 16/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0906 - accuracy: 0.9616 - val_loss: 0.0880 - val_accuracy: 0.9629\n",
      "Epoch 17/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0872 - accuracy: 0.9630 - val_loss: 0.0856 - val_accuracy: 0.9622\n",
      "Epoch 18/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0853 - accuracy: 0.9635 - val_loss: 0.0903 - val_accuracy: 0.9640\n",
      "Epoch 19/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0869 - accuracy: 0.9629 - val_loss: 0.0828 - val_accuracy: 0.9668\n",
      "Epoch 20/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0856 - accuracy: 0.9629 - val_loss: 0.0827 - val_accuracy: 0.9661\n",
      "Epoch 21/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0872 - accuracy: 0.9640 - val_loss: 0.0829 - val_accuracy: 0.9657\n",
      "Epoch 22/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0857 - accuracy: 0.9624 - val_loss: 0.0992 - val_accuracy: 0.9540\n",
      "Epoch 23/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0837 - accuracy: 0.9635 - val_loss: 0.0814 - val_accuracy: 0.9672\n",
      "Epoch 24/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0841 - accuracy: 0.9639 - val_loss: 0.0814 - val_accuracy: 0.9640\n",
      "Epoch 25/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0831 - accuracy: 0.9637 - val_loss: 0.1093 - val_accuracy: 0.9575\n",
      "Epoch 26/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0874 - accuracy: 0.9623 - val_loss: 0.0972 - val_accuracy: 0.9607\n",
      "Epoch 27/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0839 - accuracy: 0.9630 - val_loss: 0.1017 - val_accuracy: 0.9532\n",
      "Epoch 28/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0826 - accuracy: 0.9630 - val_loss: 0.0806 - val_accuracy: 0.9647\n",
      "Epoch 29/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0806 - accuracy: 0.9641 - val_loss: 0.0842 - val_accuracy: 0.9597\n",
      "Epoch 30/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0841 - accuracy: 0.9631 - val_loss: 0.0807 - val_accuracy: 0.9668\n",
      "Epoch 31/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0796 - accuracy: 0.9656 - val_loss: 0.0796 - val_accuracy: 0.9675\n",
      "Epoch 32/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0826 - accuracy: 0.9630 - val_loss: 0.0828 - val_accuracy: 0.9629\n",
      "Epoch 33/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0787 - accuracy: 0.9645 - val_loss: 0.0876 - val_accuracy: 0.9622\n",
      "Epoch 34/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0792 - accuracy: 0.9651 - val_loss: 0.0772 - val_accuracy: 0.9657\n",
      "Epoch 35/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0805 - accuracy: 0.9643 - val_loss: 0.0773 - val_accuracy: 0.9661\n",
      "Epoch 36/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0798 - accuracy: 0.9643 - val_loss: 0.0768 - val_accuracy: 0.9690\n",
      "Epoch 37/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0784 - accuracy: 0.9639 - val_loss: 0.0751 - val_accuracy: 0.9690\n",
      "Epoch 38/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0797 - accuracy: 0.9641 - val_loss: 0.0833 - val_accuracy: 0.9668\n",
      "Epoch 39/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0785 - accuracy: 0.9647 - val_loss: 0.0762 - val_accuracy: 0.9654\n",
      "Epoch 40/80\n",
      "25210/25210 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.96 - 1s 38us/step - loss: 0.0784 - accuracy: 0.9645 - val_loss: 0.0758 - val_accuracy: 0.9679\n",
      "Epoch 41/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0766 - accuracy: 0.9653 - val_loss: 0.0765 - val_accuracy: 0.9672\n",
      "Epoch 42/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0782 - accuracy: 0.9638 - val_loss: 0.0819 - val_accuracy: 0.9604\n",
      "Epoch 43/80\n",
      "25210/25210 [==============================] - 1s 36us/step - loss: 0.0749 - accuracy: 0.9654 - val_loss: 0.0773 - val_accuracy: 0.9675\n",
      "Epoch 44/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0762 - accuracy: 0.9656 - val_loss: 0.0753 - val_accuracy: 0.9647\n",
      "Epoch 45/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0795 - accuracy: 0.9641 - val_loss: 0.0766 - val_accuracy: 0.9682\n",
      "Epoch 46/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0788 - accuracy: 0.9642 - val_loss: 0.0852 - val_accuracy: 0.9650\n",
      "Epoch 47/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0751 - accuracy: 0.9648 - val_loss: 0.0861 - val_accuracy: 0.9622\n",
      "Epoch 00047: early stopping\n",
      "[[1.0000000e+00 5.7595437e-15 4.3563997e-10 2.2239023e-14 3.3099933e-13\n",
      "  2.4431379e-16]\n",
      " [1.2211376e-05 9.6602244e-06 8.3453855e-08 1.3227627e-13 1.3092338e-05\n",
      "  9.9996495e-01]\n",
      " [1.1925103e-05 9.4793186e-06 8.2827192e-08 1.2996079e-13 1.3182463e-05\n",
      "  9.9996531e-01]\n",
      " ...\n",
      " [1.8060144e-09 9.0187189e-15 9.9999583e-01 1.6575987e-07 5.8952322e-13\n",
      "  4.0441623e-06]\n",
      " [8.1912935e-05 2.0287893e-14 9.9601328e-01 1.0758230e-03 2.3588517e-03\n",
      "  4.7006374e-04]\n",
      " [2.0668219e-06 1.6621577e-21 5.0030167e-07 2.6961308e-07 9.9998677e-01\n",
      "  1.0312864e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:04:35.537867\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25210 samples, validate on 2802 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25210/25210 [==============================] - 1s 47us/step - loss: 0.9829 - accuracy: 0.6041 - val_loss: 0.3983 - val_accuracy: 0.8590\n",
      "Epoch 2/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.2831 - accuracy: 0.9031 - val_loss: 0.2242 - val_accuracy: 0.9333\n",
      "Epoch 3/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.1965 - accuracy: 0.9309 - val_loss: 0.1758 - val_accuracy: 0.9336\n",
      "Epoch 4/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1511 - accuracy: 0.9478 - val_loss: 0.1256 - val_accuracy: 0.9540\n",
      "Epoch 5/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.1228 - accuracy: 0.9576 - val_loss: 0.1322 - val_accuracy: 0.9604\n",
      "Epoch 6/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.1088 - accuracy: 0.9621 - val_loss: 0.1269 - val_accuracy: 0.9582\n",
      "Epoch 7/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.1025 - accuracy: 0.9626 - val_loss: 0.0991 - val_accuracy: 0.9629\n",
      "Epoch 8/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0969 - accuracy: 0.9643 - val_loss: 0.1041 - val_accuracy: 0.9568\n",
      "Epoch 9/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0974 - accuracy: 0.9630 - val_loss: 0.1051 - val_accuracy: 0.9579\n",
      "Epoch 10/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0942 - accuracy: 0.9634 - val_loss: 0.0971 - val_accuracy: 0.9622\n",
      "Epoch 11/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0921 - accuracy: 0.9643 - val_loss: 0.1027 - val_accuracy: 0.9572\n",
      "Epoch 12/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0907 - accuracy: 0.9645 - val_loss: 0.0984 - val_accuracy: 0.9586\n",
      "Epoch 13/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0975 - accuracy: 0.9615 - val_loss: 0.1026 - val_accuracy: 0.9547\n",
      "Epoch 14/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0950 - accuracy: 0.9629 - val_loss: 0.0980 - val_accuracy: 0.9640\n",
      "Epoch 15/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0919 - accuracy: 0.9635 - val_loss: 0.0915 - val_accuracy: 0.9593\n",
      "Epoch 16/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0889 - accuracy: 0.9631 - val_loss: 0.1029 - val_accuracy: 0.9550\n",
      "Epoch 17/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0866 - accuracy: 0.9638 - val_loss: 0.0892 - val_accuracy: 0.9607\n",
      "Epoch 18/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0889 - accuracy: 0.9634 - val_loss: 0.0934 - val_accuracy: 0.9604\n",
      "Epoch 19/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0888 - accuracy: 0.9639 - val_loss: 0.0926 - val_accuracy: 0.9618\n",
      "Epoch 20/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0858 - accuracy: 0.9643 - val_loss: 0.0946 - val_accuracy: 0.9611\n",
      "Epoch 21/80\n",
      "25210/25210 [==============================] - 1s 38us/step - loss: 0.0893 - accuracy: 0.9631 - val_loss: 0.1083 - val_accuracy: 0.9522\n",
      "Epoch 22/80\n",
      "25210/25210 [==============================] - 1s 37us/step - loss: 0.0859 - accuracy: 0.9639 - val_loss: 0.1041 - val_accuracy: 0.9582\n",
      "Epoch 23/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0840 - accuracy: 0.9643 - val_loss: 0.0967 - val_accuracy: 0.9618\n",
      "Epoch 24/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0860 - accuracy: 0.9641 - val_loss: 0.1011 - val_accuracy: 0.9565\n",
      "Epoch 25/80\n",
      "25210/25210 [==============================] - 1s 41us/step - loss: 0.0851 - accuracy: 0.9639 - val_loss: 0.0946 - val_accuracy: 0.9657\n",
      "Epoch 26/80\n",
      "25210/25210 [==============================] - 1s 40us/step - loss: 0.0872 - accuracy: 0.9638 - val_loss: 0.0929 - val_accuracy: 0.9597\n",
      "Epoch 27/80\n",
      "25210/25210 [==============================] - 1s 39us/step - loss: 0.0903 - accuracy: 0.9616 - val_loss: 0.1035 - val_accuracy: 0.9657\n",
      "Epoch 00027: early stopping\n",
      "[[9.9999809e-01 1.0291996e-12 4.2233213e-12 5.3605991e-14 1.8956638e-06\n",
      "  2.7670330e-13]\n",
      " [9.8727112e-05 3.3005606e-05 8.7036431e-05 2.0192991e-11 2.0129932e-03\n",
      "  9.9776828e-01]\n",
      " [9.7490141e-05 3.2851454e-05 8.6956883e-05 2.0026437e-11 2.0037389e-03\n",
      "  9.9777895e-01]\n",
      " ...\n",
      " [1.1559048e-11 2.8575126e-17 9.9091971e-01 9.0796025e-03 4.6195967e-14\n",
      "  6.6854290e-07]\n",
      " [3.7318989e-06 3.9212207e-16 8.5868484e-01 1.1626146e-01 2.4831902e-02\n",
      "  2.1815608e-04]\n",
      " [3.7959719e-04 8.1212841e-13 1.2207305e-06 7.1290800e-09 9.9957770e-01\n",
      "  4.1417923e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:05:06.647315\n",
      "region_label= [6, 7, 2, 4, 3, 5]\n",
      "dist_table_truth\n",
      " [[1342  914    0 1637   25  310    0    0    0]\n",
      " [ 625    0    3  192  276    4    0    0    0]\n",
      " [ 179  602 2285   24 1179  577    0    0    0]\n",
      " [ 209 1086 1851    0 1779  262    0    0    0]\n",
      " [2241  500   23 2505   63 1396    0    0    0]\n",
      " [ 167  752   20  157  234 1739    0    0    0]]\n",
      "num of merged_region_image 0 4998\n",
      "num of merged_region_image 1 5190\n",
      "num of merged_region_image 2 4892\n",
      "num of merged_region_image 3 4748\n",
      "num of merged_region_image 4 4547\n",
      "num of merged_region_image 5 3637\n",
      "Counter({-1: 15205, 1: 5190, 0: 4998, 2: 4892, 3: 4748, 4: 4547, 5: 3637})\n",
      "===========  ITE = 3   ===========\n",
      "used_img 28012 28012\n",
      "working_img(=other images=unclean images) 15205 15205\n",
      "merged regions 142 142\n",
      "other_regions 58 58\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.92</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.51</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.66</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.98</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>149</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>152</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>153</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.51</td>\n",
       "      <td>2</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>166</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>169</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.73</td>\n",
       "      <td>6</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.45</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>176</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>476</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>182</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>187</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>188</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>190</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.44</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>195</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.39</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>84</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>106</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>112</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>113</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>54</td>\n",
       "      <td>244</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    other index  pred label  truth  rate    0    1    2    3    4    5\n",
       "0           128           3      4  0.92    5    0    0    0  213    0\n",
       "1             8           5      5  0.45    0    3    0    0    0  361\n",
       "2             9           5      0  0.51  245    0    1    0    1    0\n",
       "3           137           5      5  0.62    0    0    0    0    0  277\n",
       "4            12           2      4  0.66   13    0    0    0   77    2\n",
       "5           144           4      3  0.89    0    0    0  135    0    0\n",
       "6            18           3      8  0.98    4    0    0    0    2    0\n",
       "7            20           5      7  0.51    0    0    0    0    0    0\n",
       "8            21           1      4  0.71    0    0    1    0  137    0\n",
       "9            22          -1      8  0.00    0    0    0    0    0    0\n",
       "10          149          -1      8  0.00    0    0    0    0    1    3\n",
       "11          152          -1      4  0.00    3    0    0    0  399    0\n",
       "12          153          -1      8  0.00    1    0    0    0    0    0\n",
       "13          154           4      8  0.89    0    0    0    0    0    1\n",
       "14           28           4      8  0.88    0    0    0    0    0    0\n",
       "15           32           4      8  0.52    2    0    0    0    0    1\n",
       "16           34           3      1  0.61    0   69    0    0    0    0\n",
       "17          164           2      1  0.51    2  268    0    0    1    1\n",
       "18           37           4      0  0.92  382    0    0    1    0    0\n",
       "19          166           0      1  0.75    0  147    0    0    0    5\n",
       "20           39          -1      4  0.00    2    0    2    0  117    0\n",
       "21          169          -1      5  0.00    0    0    0    0    0  112\n",
       "22          170           4      1  0.73    6  601    0    0    0    2\n",
       "23          171           4      0  0.45  120   12    0    0    0    0\n",
       "24           44           4      8  0.84    1    0    0    0    1    0\n",
       "25           46           2      8  0.45    5    0    0    0    1    0\n",
       "26          176          -1      4  0.00    3    2    0    0   96    0\n",
       "27           53           4      3  0.92    0    0    0  476    0    0\n",
       "28          182          -1      1  0.00    1  233    0    0    0    0\n",
       "29          183           0      3  0.71    0    0    0  202    0    0\n",
       "30          184           2      8  0.41    0    0    0    0    0    0\n",
       "31           57          -1      1  0.00    0  128    0    0    0    0\n",
       "32          186           2      8  0.86    0    0    0    0    0    0\n",
       "33          187          -1      2  0.00    0    0  223    0    0    0\n",
       "34          188           2      4  0.97    0    0    0    0   31    0\n",
       "35          190           4      8  0.61    1    0    0    0    0    4\n",
       "36          191           1      3  0.44    5    0    0  227    1    0\n",
       "37          195           4      1  0.70    2   21    0    0    0   17\n",
       "38          196           4      0  0.65  372    2    1    1    4    1\n",
       "39           69           0      3  0.55    0    0    0  242    0    0\n",
       "40           70           0      3  0.34    0    0    0  183    0    0\n",
       "41           71           0      3  0.79    0    0    0  115    0    0\n",
       "42           72           4      5  0.67    1    0    0    0    0  238\n",
       "43           73           4      0  0.50   30    0    0    0    0    0\n",
       "44          199           2      8  0.66    0    0    2    0    0    0\n",
       "45           79           1      6  0.39    2    0    0    0    0    0\n",
       "46           82           1      7  0.47    0    0    0    0    0    0\n",
       "47           84          -1      8  0.00    0    0    0    0    0    0\n",
       "48           85           4      0  0.95  185    0    0    0    0    3\n",
       "49           94           4      5  0.88    0    0    0    0    0  169\n",
       "50           97           1      6  0.39    0    0    0    0    0    0\n",
       "51          102           3      2  0.60    0    1  629    0    1    0\n",
       "52          105           4      3  0.90    0    0    0  277    0    0\n",
       "53          106           1      7  0.51    1    0    0    0    0    5\n",
       "54          112           2      1  0.95    0   22    0    0    0    0\n",
       "55          113           2      7  0.72    0    0    0    0    0    0\n",
       "56          118           4      8  0.84    3    4    0    1    1    3\n",
       "57          123           5      5  0.24   54  244   98   62   18  457"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [71, 166, 183] 350\n",
      "added label, regions, img amount: {1} [21] 98\n",
      "added label, regions, img amount: {2} [188, 112, 186, 113] 507\n",
      "added label, regions, img amount: {3} [18, 128] 418\n",
      "added label, regions, img amount: {4} [85, 53, 37, 105, 144] 1347\n",
      "Not getting into residuals\n",
      "NUM_region 6\n",
      "number of clean images 30732\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 6\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 27658 samples, validate on 3074 samples\n",
      "Epoch 1/80\n",
      "27658/27658 [==============================] - 2s 56us/step - loss: 0.6631 - accuracy: 0.7529 - val_loss: 0.3625 - val_accuracy: 0.8484\n",
      "Epoch 2/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.2586 - accuracy: 0.9097 - val_loss: 0.2740 - val_accuracy: 0.8874\n",
      "Epoch 3/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.1935 - accuracy: 0.9332 - val_loss: 0.1997 - val_accuracy: 0.9196\n",
      "Epoch 4/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.1563 - accuracy: 0.9443 - val_loss: 0.1568 - val_accuracy: 0.9447\n",
      "Epoch 5/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.1356 - accuracy: 0.9525 - val_loss: 0.1478 - val_accuracy: 0.9437\n",
      "Epoch 6/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.1275 - accuracy: 0.9545 - val_loss: 0.1389 - val_accuracy: 0.9610\n",
      "Epoch 7/80\n",
      "27658/27658 [==============================] - 1s 42us/step - loss: 0.1121 - accuracy: 0.9602 - val_loss: 0.1202 - val_accuracy: 0.9606\n",
      "Epoch 8/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.1052 - accuracy: 0.9633 - val_loss: 0.1072 - val_accuracy: 0.9623\n",
      "Epoch 9/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0993 - accuracy: 0.9658 - val_loss: 0.1053 - val_accuracy: 0.9597\n",
      "Epoch 10/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0990 - accuracy: 0.9646 - val_loss: 0.1017 - val_accuracy: 0.9626\n",
      "Epoch 11/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0981 - accuracy: 0.9653 - val_loss: 0.1164 - val_accuracy: 0.9590\n",
      "Epoch 12/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0941 - accuracy: 0.9664 - val_loss: 0.1060 - val_accuracy: 0.9632\n",
      "Epoch 13/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0917 - accuracy: 0.9665 - val_loss: 0.0979 - val_accuracy: 0.9639\n",
      "Epoch 14/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0890 - accuracy: 0.9680 - val_loss: 0.0942 - val_accuracy: 0.9642\n",
      "Epoch 15/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0909 - accuracy: 0.9656 - val_loss: 0.0971 - val_accuracy: 0.9645\n",
      "Epoch 16/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0857 - accuracy: 0.9673 - val_loss: 0.1125 - val_accuracy: 0.9606\n",
      "Epoch 17/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0863 - accuracy: 0.9674 - val_loss: 0.0906 - val_accuracy: 0.9678\n",
      "Epoch 18/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0841 - accuracy: 0.9678 - val_loss: 0.0934 - val_accuracy: 0.9636\n",
      "Epoch 19/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0824 - accuracy: 0.9677 - val_loss: 0.0892 - val_accuracy: 0.9629\n",
      "Epoch 20/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0809 - accuracy: 0.9678 - val_loss: 0.0896 - val_accuracy: 0.9675\n",
      "Epoch 21/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0824 - accuracy: 0.9670 - val_loss: 0.0839 - val_accuracy: 0.9665\n",
      "Epoch 22/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0785 - accuracy: 0.9679 - val_loss: 0.0894 - val_accuracy: 0.9626\n",
      "Epoch 23/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0802 - accuracy: 0.9671 - val_loss: 0.0793 - val_accuracy: 0.9645\n",
      "Epoch 24/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0789 - accuracy: 0.9676 - val_loss: 0.0802 - val_accuracy: 0.9675\n",
      "Epoch 25/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0778 - accuracy: 0.9674 - val_loss: 0.0807 - val_accuracy: 0.9639\n",
      "Epoch 26/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0774 - accuracy: 0.9671 - val_loss: 0.0783 - val_accuracy: 0.9645\n",
      "Epoch 27/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0774 - accuracy: 0.9676 - val_loss: 0.0822 - val_accuracy: 0.9678\n",
      "Epoch 28/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0748 - accuracy: 0.9673 - val_loss: 0.0818 - val_accuracy: 0.9665\n",
      "Epoch 29/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0759 - accuracy: 0.9672 - val_loss: 0.0885 - val_accuracy: 0.9606\n",
      "Epoch 30/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0773 - accuracy: 0.9668 - val_loss: 0.0776 - val_accuracy: 0.9675\n",
      "Epoch 31/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0736 - accuracy: 0.9681 - val_loss: 0.0983 - val_accuracy: 0.9593\n",
      "Epoch 32/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0770 - accuracy: 0.9665 - val_loss: 0.0831 - val_accuracy: 0.9675\n",
      "Epoch 33/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0724 - accuracy: 0.9674 - val_loss: 0.0847 - val_accuracy: 0.9574\n",
      "Epoch 34/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0721 - accuracy: 0.9669 - val_loss: 0.0770 - val_accuracy: 0.9691\n",
      "Epoch 35/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0729 - accuracy: 0.9678 - val_loss: 0.0772 - val_accuracy: 0.9629\n",
      "Epoch 36/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0703 - accuracy: 0.9685 - val_loss: 0.0851 - val_accuracy: 0.9665\n",
      "Epoch 37/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0722 - accuracy: 0.9682 - val_loss: 0.0971 - val_accuracy: 0.9613\n",
      "Epoch 38/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0717 - accuracy: 0.9675 - val_loss: 0.0813 - val_accuracy: 0.9642\n",
      "Epoch 39/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0693 - accuracy: 0.9684 - val_loss: 0.0733 - val_accuracy: 0.9681\n",
      "Epoch 40/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0705 - accuracy: 0.9681 - val_loss: 0.0766 - val_accuracy: 0.9665\n",
      "Epoch 41/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0701 - accuracy: 0.9687 - val_loss: 0.0829 - val_accuracy: 0.9649\n",
      "Epoch 42/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0697 - accuracy: 0.9680 - val_loss: 0.0741 - val_accuracy: 0.9668\n",
      "Epoch 43/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0699 - accuracy: 0.9692 - val_loss: 0.0723 - val_accuracy: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0721 - accuracy: 0.9677 - val_loss: 0.0802 - val_accuracy: 0.9675\n",
      "Epoch 45/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0686 - accuracy: 0.9684 - val_loss: 0.0745 - val_accuracy: 0.9652\n",
      "Epoch 46/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0689 - accuracy: 0.9680 - val_loss: 0.0849 - val_accuracy: 0.9649\n",
      "Epoch 47/80\n",
      "27658/27658 [==============================] - 1s 42us/step - loss: 0.0689 - accuracy: 0.9680 - val_loss: 0.0719 - val_accuracy: 0.9694\n",
      "Epoch 48/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0677 - accuracy: 0.9689 - val_loss: 0.0905 - val_accuracy: 0.9639\n",
      "Epoch 49/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0706 - accuracy: 0.9675 - val_loss: 0.0790 - val_accuracy: 0.9649\n",
      "Epoch 50/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0686 - accuracy: 0.9680 - val_loss: 0.0807 - val_accuracy: 0.9593\n",
      "Epoch 51/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0679 - accuracy: 0.9683 - val_loss: 0.0743 - val_accuracy: 0.9671\n",
      "Epoch 52/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0702 - accuracy: 0.9681 - val_loss: 0.0788 - val_accuracy: 0.9639\n",
      "Epoch 53/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0679 - accuracy: 0.9688 - val_loss: 0.0788 - val_accuracy: 0.9665\n",
      "Epoch 54/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0692 - accuracy: 0.9676 - val_loss: 0.0723 - val_accuracy: 0.9684\n",
      "Epoch 55/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0678 - accuracy: 0.9676 - val_loss: 0.0730 - val_accuracy: 0.9671\n",
      "Epoch 56/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0660 - accuracy: 0.9691 - val_loss: 0.0790 - val_accuracy: 0.9668\n",
      "Epoch 57/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0689 - accuracy: 0.9684 - val_loss: 0.0764 - val_accuracy: 0.9619\n",
      "Epoch 00057: early stopping\n",
      "[[1.0000000e+00 3.4752678e-20 2.7124832e-21 2.5327855e-18 2.7232789e-19\n",
      "  1.4291677e-18]\n",
      " [2.4824487e-06 3.1908745e-09 5.7473604e-09 1.6571808e-07 9.2836592e-07\n",
      "  9.9999642e-01]\n",
      " [2.4330832e-06 3.1310676e-09 5.7955756e-09 1.6313300e-07 9.3728505e-07\n",
      "  9.9999642e-01]\n",
      " ...\n",
      " [3.2880082e-10 1.6176049e-14 9.9997759e-01 2.1489926e-05 1.0066914e-14\n",
      "  1.0093237e-06]\n",
      " [1.9781639e-07 6.7291680e-16 9.9517459e-01 6.3704344e-04 4.0958757e-03\n",
      "  9.2251066e-05]\n",
      " [1.1713087e-10 6.8668853e-25 8.1566373e-13 2.3740193e-10 9.9999988e-01\n",
      "  1.5212117e-07]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:01:07.944094\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 27658 samples, validate on 3074 samples\n",
      "Epoch 1/80\n",
      "27658/27658 [==============================] - 1s 50us/step - loss: 0.6158 - accuracy: 0.7650 - val_loss: 0.3774 - val_accuracy: 0.8455\n",
      "Epoch 2/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.2701 - accuracy: 0.9065 - val_loss: 0.2607 - val_accuracy: 0.8998\n",
      "Epoch 3/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.2187 - accuracy: 0.9246 - val_loss: 0.2104 - val_accuracy: 0.9294\n",
      "Epoch 4/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1870 - accuracy: 0.9351 - val_loss: 0.1558 - val_accuracy: 0.9457\n",
      "Epoch 5/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.1548 - accuracy: 0.9434 - val_loss: 0.1354 - val_accuracy: 0.9613\n",
      "Epoch 6/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1379 - accuracy: 0.9514 - val_loss: 0.1223 - val_accuracy: 0.9636\n",
      "Epoch 7/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.1197 - accuracy: 0.9590 - val_loss: 0.0991 - val_accuracy: 0.9655\n",
      "Epoch 8/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.1087 - accuracy: 0.9627 - val_loss: 0.1100 - val_accuracy: 0.9652\n",
      "Epoch 9/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1029 - accuracy: 0.9633 - val_loss: 0.0982 - val_accuracy: 0.9691\n",
      "Epoch 10/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0977 - accuracy: 0.9654 - val_loss: 0.0954 - val_accuracy: 0.9642\n",
      "Epoch 11/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0946 - accuracy: 0.9657 - val_loss: 0.0842 - val_accuracy: 0.9684\n",
      "Epoch 12/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0928 - accuracy: 0.9652 - val_loss: 0.0797 - val_accuracy: 0.9688\n",
      "Epoch 13/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0892 - accuracy: 0.9665 - val_loss: 0.0939 - val_accuracy: 0.9613\n",
      "Epoch 14/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0853 - accuracy: 0.9670 - val_loss: 0.0755 - val_accuracy: 0.9710\n",
      "Epoch 15/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0857 - accuracy: 0.9667 - val_loss: 0.0750 - val_accuracy: 0.9694\n",
      "Epoch 16/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0811 - accuracy: 0.9675 - val_loss: 0.0754 - val_accuracy: 0.9645\n",
      "Epoch 17/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0802 - accuracy: 0.9667 - val_loss: 0.1026 - val_accuracy: 0.9567\n",
      "Epoch 18/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0850 - accuracy: 0.9653 - val_loss: 0.0853 - val_accuracy: 0.9681\n",
      "Epoch 19/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0806 - accuracy: 0.9671 - val_loss: 0.0965 - val_accuracy: 0.9613\n",
      "Epoch 20/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0834 - accuracy: 0.9655 - val_loss: 0.0723 - val_accuracy: 0.9707\n",
      "Epoch 21/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0798 - accuracy: 0.9664 - val_loss: 0.0731 - val_accuracy: 0.9694\n",
      "Epoch 22/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0768 - accuracy: 0.9671 - val_loss: 0.0692 - val_accuracy: 0.9704\n",
      "Epoch 23/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0764 - accuracy: 0.9674 - val_loss: 0.0770 - val_accuracy: 0.9665\n",
      "Epoch 24/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0777 - accuracy: 0.9669 - val_loss: 0.0731 - val_accuracy: 0.9655\n",
      "Epoch 25/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0780 - accuracy: 0.9661 - val_loss: 0.0853 - val_accuracy: 0.9668\n",
      "Epoch 26/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0752 - accuracy: 0.9675 - val_loss: 0.0895 - val_accuracy: 0.9658\n",
      "Epoch 27/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0805 - accuracy: 0.9657 - val_loss: 0.0714 - val_accuracy: 0.9697\n",
      "Epoch 28/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0765 - accuracy: 0.9669 - val_loss: 0.0656 - val_accuracy: 0.9714\n",
      "Epoch 29/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0743 - accuracy: 0.9683 - val_loss: 0.0760 - val_accuracy: 0.9655\n",
      "Epoch 30/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0757 - accuracy: 0.9668 - val_loss: 0.0657 - val_accuracy: 0.9723\n",
      "Epoch 31/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0735 - accuracy: 0.9681 - val_loss: 0.0644 - val_accuracy: 0.9707\n",
      "Epoch 32/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0740 - accuracy: 0.9677 - val_loss: 0.0691 - val_accuracy: 0.9704\n",
      "Epoch 33/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0740 - accuracy: 0.9664 - val_loss: 0.0656 - val_accuracy: 0.9710\n",
      "Epoch 34/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0740 - accuracy: 0.9677 - val_loss: 0.0671 - val_accuracy: 0.9717\n",
      "Epoch 35/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0728 - accuracy: 0.9676 - val_loss: 0.0646 - val_accuracy: 0.9733\n",
      "Epoch 36/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0730 - accuracy: 0.9672 - val_loss: 0.0643 - val_accuracy: 0.9717\n",
      "Epoch 37/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0707 - accuracy: 0.9680 - val_loss: 0.0658 - val_accuracy: 0.9710\n",
      "Epoch 38/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0718 - accuracy: 0.9678 - val_loss: 0.0661 - val_accuracy: 0.9688\n",
      "Epoch 39/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0713 - accuracy: 0.9676 - val_loss: 0.0685 - val_accuracy: 0.9694\n",
      "Epoch 40/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0710 - accuracy: 0.9681 - val_loss: 0.0663 - val_accuracy: 0.9717\n",
      "Epoch 41/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0721 - accuracy: 0.9681 - val_loss: 0.0733 - val_accuracy: 0.9675\n",
      "Epoch 42/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0713 - accuracy: 0.9691 - val_loss: 0.0742 - val_accuracy: 0.9697\n",
      "Epoch 43/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0717 - accuracy: 0.9685 - val_loss: 0.0715 - val_accuracy: 0.9675\n",
      "Epoch 44/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0718 - accuracy: 0.9674 - val_loss: 0.0707 - val_accuracy: 0.9678\n",
      "Epoch 45/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0693 - accuracy: 0.9691 - val_loss: 0.0658 - val_accuracy: 0.9691\n",
      "Epoch 46/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0732 - accuracy: 0.9660 - val_loss: 0.0637 - val_accuracy: 0.9701\n",
      "Epoch 47/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0729 - accuracy: 0.9666 - val_loss: 0.0658 - val_accuracy: 0.9704\n",
      "Epoch 48/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0707 - accuracy: 0.9674 - val_loss: 0.0648 - val_accuracy: 0.9704\n",
      "Epoch 49/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0681 - accuracy: 0.9686 - val_loss: 0.0667 - val_accuracy: 0.9710\n",
      "Epoch 50/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.0732 - accuracy: 0.9671 - val_loss: 0.0732 - val_accuracy: 0.9697\n",
      "Epoch 51/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0711 - accuracy: 0.9686 - val_loss: 0.0661 - val_accuracy: 0.9723\n",
      "Epoch 52/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0691 - accuracy: 0.9684 - val_loss: 0.0684 - val_accuracy: 0.9658\n",
      "Epoch 53/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0703 - accuracy: 0.9686 - val_loss: 0.0644 - val_accuracy: 0.9675\n",
      "Epoch 54/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0674 - accuracy: 0.9688 - val_loss: 0.0669 - val_accuracy: 0.9740\n",
      "Epoch 55/80\n",
      "27658/27658 [==============================] - 1s 34us/step - loss: 0.0713 - accuracy: 0.9674 - val_loss: 0.0670 - val_accuracy: 0.9707\n",
      "Epoch 56/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0711 - accuracy: 0.9679 - val_loss: 0.0663 - val_accuracy: 0.9733\n",
      "Epoch 00056: early stopping\n",
      "[[1.0000000e+00 4.5317048e-20 9.7411261e-13 2.9211429e-09 1.3079841e-13\n",
      "  1.2746310e-15]\n",
      " [4.8942238e-06 9.8763273e-09 1.4078571e-08 5.2618764e-11 1.0965088e-05\n",
      "  9.9998415e-01]\n",
      " [4.7607077e-06 9.7333039e-09 1.3887163e-08 5.1545154e-11 1.0953718e-05\n",
      "  9.9998426e-01]\n",
      " ...\n",
      " [7.9932859e-14 3.4555887e-13 9.9951720e-01 4.8270196e-04 1.0601473e-16\n",
      "  1.3637364e-07]\n",
      " [4.2572175e-08 2.1401321e-16 9.9859720e-01 1.0215980e-03 1.3542792e-04\n",
      "  2.4572274e-04]\n",
      " [1.3064929e-09 3.6261074e-27 5.2584952e-08 6.1498531e-09 9.9999988e-01\n",
      "  5.8964066e-08]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:02:12.990972\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 27658 samples, validate on 3074 samples\n",
      "Epoch 1/80\n",
      "27658/27658 [==============================] - 1s 48us/step - loss: 0.6741 - accuracy: 0.7410 - val_loss: 0.3646 - val_accuracy: 0.8793\n",
      "Epoch 2/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.3029 - accuracy: 0.8938 - val_loss: 0.2557 - val_accuracy: 0.9096\n",
      "Epoch 3/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.2266 - accuracy: 0.9210 - val_loss: 0.2097 - val_accuracy: 0.9327\n",
      "Epoch 4/80\n",
      "27658/27658 [==============================] - 1s 41us/step - loss: 0.1759 - accuracy: 0.9389 - val_loss: 0.1594 - val_accuracy: 0.9509\n",
      "Epoch 5/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.1484 - accuracy: 0.9487 - val_loss: 0.1676 - val_accuracy: 0.9418\n",
      "Epoch 6/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1347 - accuracy: 0.9529 - val_loss: 0.1267 - val_accuracy: 0.9545\n",
      "Epoch 7/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1238 - accuracy: 0.9577 - val_loss: 0.1253 - val_accuracy: 0.9561\n",
      "Epoch 8/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.1181 - accuracy: 0.9593 - val_loss: 0.1104 - val_accuracy: 0.9610\n",
      "Epoch 9/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.1110 - accuracy: 0.9619 - val_loss: 0.1174 - val_accuracy: 0.9597\n",
      "Epoch 10/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.1087 - accuracy: 0.9628 - val_loss: 0.1075 - val_accuracy: 0.9636\n",
      "Epoch 11/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1022 - accuracy: 0.9648 - val_loss: 0.0959 - val_accuracy: 0.9652\n",
      "Epoch 12/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0990 - accuracy: 0.9660 - val_loss: 0.1054 - val_accuracy: 0.9636\n",
      "Epoch 13/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0992 - accuracy: 0.9649 - val_loss: 0.1064 - val_accuracy: 0.9616\n",
      "Epoch 14/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0970 - accuracy: 0.9655 - val_loss: 0.0976 - val_accuracy: 0.9632\n",
      "Epoch 15/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0947 - accuracy: 0.9664 - val_loss: 0.0967 - val_accuracy: 0.9671\n",
      "Epoch 16/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0948 - accuracy: 0.9656 - val_loss: 0.0935 - val_accuracy: 0.9675\n",
      "Epoch 17/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0930 - accuracy: 0.9659 - val_loss: 0.0901 - val_accuracy: 0.9681\n",
      "Epoch 18/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0908 - accuracy: 0.9676 - val_loss: 0.1032 - val_accuracy: 0.9662\n",
      "Epoch 19/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0907 - accuracy: 0.9670 - val_loss: 0.0818 - val_accuracy: 0.9662\n",
      "Epoch 20/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0866 - accuracy: 0.9669 - val_loss: 0.1029 - val_accuracy: 0.9678\n",
      "Epoch 21/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0890 - accuracy: 0.9667 - val_loss: 0.0961 - val_accuracy: 0.9662\n",
      "Epoch 22/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0836 - accuracy: 0.9676 - val_loss: 0.0801 - val_accuracy: 0.9684\n",
      "Epoch 23/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0840 - accuracy: 0.9658 - val_loss: 0.0867 - val_accuracy: 0.9626\n",
      "Epoch 24/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0812 - accuracy: 0.9681 - val_loss: 0.0814 - val_accuracy: 0.9678\n",
      "Epoch 25/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0787 - accuracy: 0.9682 - val_loss: 0.0804 - val_accuracy: 0.9678\n",
      "Epoch 26/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0794 - accuracy: 0.9674 - val_loss: 0.0773 - val_accuracy: 0.9684\n",
      "Epoch 27/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0827 - accuracy: 0.9669 - val_loss: 0.0735 - val_accuracy: 0.9697\n",
      "Epoch 28/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0770 - accuracy: 0.9682 - val_loss: 0.0740 - val_accuracy: 0.9671\n",
      "Epoch 29/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0757 - accuracy: 0.9682 - val_loss: 0.0825 - val_accuracy: 0.9662\n",
      "Epoch 30/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0739 - accuracy: 0.9685 - val_loss: 0.0771 - val_accuracy: 0.9662\n",
      "Epoch 31/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0775 - accuracy: 0.9671 - val_loss: 0.0738 - val_accuracy: 0.9662\n",
      "Epoch 32/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0743 - accuracy: 0.9683 - val_loss: 0.0778 - val_accuracy: 0.9684\n",
      "Epoch 33/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0721 - accuracy: 0.9681 - val_loss: 0.0998 - val_accuracy: 0.9649\n",
      "Epoch 34/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0731 - accuracy: 0.9680 - val_loss: 0.0741 - val_accuracy: 0.9671\n",
      "Epoch 35/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0743 - accuracy: 0.9675 - val_loss: 0.0763 - val_accuracy: 0.9665\n",
      "Epoch 36/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0725 - accuracy: 0.9691 - val_loss: 0.0905 - val_accuracy: 0.9652\n",
      "Epoch 37/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0748 - accuracy: 0.9672 - val_loss: 0.0733 - val_accuracy: 0.9671\n",
      "Epoch 38/80\n",
      "27658/27658 [==============================] - 1s 34us/step - loss: 0.0734 - accuracy: 0.9679 - val_loss: 0.0693 - val_accuracy: 0.9681\n",
      "Epoch 39/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0722 - accuracy: 0.9681 - val_loss: 0.0921 - val_accuracy: 0.9649\n",
      "Epoch 40/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0738 - accuracy: 0.9674 - val_loss: 0.0698 - val_accuracy: 0.9681\n",
      "Epoch 41/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0699 - accuracy: 0.9692 - val_loss: 0.0687 - val_accuracy: 0.9681\n",
      "Epoch 42/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0715 - accuracy: 0.9687 - val_loss: 0.0717 - val_accuracy: 0.9694\n",
      "Epoch 43/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0696 - accuracy: 0.9689 - val_loss: 0.0669 - val_accuracy: 0.9694\n",
      "Epoch 44/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0704 - accuracy: 0.9681 - val_loss: 0.0727 - val_accuracy: 0.9671\n",
      "Epoch 45/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0717 - accuracy: 0.9681 - val_loss: 0.0730 - val_accuracy: 0.9655\n",
      "Epoch 46/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0704 - accuracy: 0.9682 - val_loss: 0.0679 - val_accuracy: 0.9691\n",
      "Epoch 47/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0697 - accuracy: 0.9691 - val_loss: 0.0720 - val_accuracy: 0.9684\n",
      "Epoch 48/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0696 - accuracy: 0.9698 - val_loss: 0.0687 - val_accuracy: 0.9684\n",
      "Epoch 49/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0711 - accuracy: 0.9680 - val_loss: 0.0682 - val_accuracy: 0.9665\n",
      "Epoch 50/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0703 - accuracy: 0.9671 - val_loss: 0.0695 - val_accuracy: 0.9697\n",
      "Epoch 51/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0690 - accuracy: 0.9685 - val_loss: 0.0744 - val_accuracy: 0.9678\n",
      "Epoch 52/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0687 - accuracy: 0.9689 - val_loss: 0.0721 - val_accuracy: 0.9675\n",
      "Epoch 53/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0707 - accuracy: 0.9679 - val_loss: 0.0685 - val_accuracy: 0.9688\n",
      "Epoch 00053: early stopping\n",
      "[[1.0000000e+00 6.6908660e-19 4.8447031e-20 1.1312728e-17 5.1471830e-17\n",
      "  2.5170173e-19]\n",
      " [7.4219670e-06 4.6751339e-07 1.5961471e-09 3.8615144e-07 1.2547397e-07\n",
      "  9.9999166e-01]\n",
      " [7.2722528e-06 4.6100291e-07 1.5909346e-09 3.8068177e-07 1.2547542e-07\n",
      "  9.9999177e-01]\n",
      " ...\n",
      " [1.3550515e-12 2.9153988e-11 9.9990952e-01 8.9825029e-05 3.4213882e-15\n",
      "  6.2183136e-07]\n",
      " [2.1619000e-07 3.3061986e-16 9.9810469e-01 4.5450975e-04 1.2801264e-03\n",
      "  1.6038622e-04]\n",
      " [8.1964778e-08 6.7156816e-20 4.3566214e-08 1.0513280e-09 9.9996030e-01\n",
      "  3.9616742e-05]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:03:12.975323\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 27658 samples, validate on 3074 samples\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27658/27658 [==============================] - 1s 45us/step - loss: 0.6269 - accuracy: 0.7737 - val_loss: 0.3124 - val_accuracy: 0.8878\n",
      "Epoch 2/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.2542 - accuracy: 0.9102 - val_loss: 0.2171 - val_accuracy: 0.9284\n",
      "Epoch 3/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1981 - accuracy: 0.9296 - val_loss: 0.2511 - val_accuracy: 0.8865\n",
      "Epoch 4/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.1717 - accuracy: 0.9382 - val_loss: 0.1681 - val_accuracy: 0.9457\n",
      "Epoch 5/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.1480 - accuracy: 0.9483 - val_loss: 0.1431 - val_accuracy: 0.9587\n",
      "Epoch 6/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.1337 - accuracy: 0.9527 - val_loss: 0.1395 - val_accuracy: 0.9525\n",
      "Epoch 7/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.1240 - accuracy: 0.9565 - val_loss: 0.1363 - val_accuracy: 0.9525\n",
      "Epoch 8/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.1136 - accuracy: 0.9603 - val_loss: 0.1212 - val_accuracy: 0.9623\n",
      "Epoch 9/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.1041 - accuracy: 0.9636 - val_loss: 0.1243 - val_accuracy: 0.9610\n",
      "Epoch 10/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.1009 - accuracy: 0.9649 - val_loss: 0.1012 - val_accuracy: 0.9658\n",
      "Epoch 11/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0982 - accuracy: 0.9657 - val_loss: 0.1024 - val_accuracy: 0.9613\n",
      "Epoch 12/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0962 - accuracy: 0.9659 - val_loss: 0.1033 - val_accuracy: 0.9642\n",
      "Epoch 13/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0962 - accuracy: 0.9656 - val_loss: 0.1060 - val_accuracy: 0.9694\n",
      "Epoch 14/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0944 - accuracy: 0.9660 - val_loss: 0.0953 - val_accuracy: 0.9688\n",
      "Epoch 15/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0913 - accuracy: 0.9666 - val_loss: 0.1001 - val_accuracy: 0.9639\n",
      "Epoch 16/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0900 - accuracy: 0.9670 - val_loss: 0.0929 - val_accuracy: 0.9681\n",
      "Epoch 17/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0907 - accuracy: 0.9672 - val_loss: 0.0930 - val_accuracy: 0.9688\n",
      "Epoch 18/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0883 - accuracy: 0.9667 - val_loss: 0.0882 - val_accuracy: 0.9671\n",
      "Epoch 19/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0864 - accuracy: 0.9680 - val_loss: 0.0838 - val_accuracy: 0.9697\n",
      "Epoch 20/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0846 - accuracy: 0.9672 - val_loss: 0.0918 - val_accuracy: 0.9681\n",
      "Epoch 21/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0813 - accuracy: 0.9674 - val_loss: 0.0904 - val_accuracy: 0.9649\n",
      "Epoch 22/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0836 - accuracy: 0.9671 - val_loss: 0.0858 - val_accuracy: 0.9665\n",
      "Epoch 23/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0810 - accuracy: 0.9668 - val_loss: 0.0872 - val_accuracy: 0.9678\n",
      "Epoch 24/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0810 - accuracy: 0.9682 - val_loss: 0.0816 - val_accuracy: 0.9681\n",
      "Epoch 25/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0799 - accuracy: 0.9672 - val_loss: 0.0844 - val_accuracy: 0.9681\n",
      "Epoch 26/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0795 - accuracy: 0.9682 - val_loss: 0.0793 - val_accuracy: 0.9694\n",
      "Epoch 27/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0771 - accuracy: 0.9679 - val_loss: 0.0800 - val_accuracy: 0.9697\n",
      "Epoch 28/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0749 - accuracy: 0.9682 - val_loss: 0.0811 - val_accuracy: 0.9668\n",
      "Epoch 29/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0765 - accuracy: 0.9678 - val_loss: 0.0793 - val_accuracy: 0.9675\n",
      "Epoch 30/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0776 - accuracy: 0.9675 - val_loss: 0.0779 - val_accuracy: 0.9681\n",
      "Epoch 31/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0743 - accuracy: 0.9685 - val_loss: 0.0749 - val_accuracy: 0.9684\n",
      "Epoch 32/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0747 - accuracy: 0.9680 - val_loss: 0.0735 - val_accuracy: 0.9697\n",
      "Epoch 33/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0734 - accuracy: 0.9684 - val_loss: 0.0816 - val_accuracy: 0.9652\n",
      "Epoch 34/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0738 - accuracy: 0.9673 - val_loss: 0.0853 - val_accuracy: 0.9645\n",
      "Epoch 35/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0731 - accuracy: 0.9676 - val_loss: 0.0717 - val_accuracy: 0.9694\n",
      "Epoch 36/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0742 - accuracy: 0.9672 - val_loss: 0.0859 - val_accuracy: 0.9606\n",
      "Epoch 37/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0758 - accuracy: 0.9666 - val_loss: 0.0721 - val_accuracy: 0.9701\n",
      "Epoch 38/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0715 - accuracy: 0.9679 - val_loss: 0.0724 - val_accuracy: 0.9688\n",
      "Epoch 39/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0709 - accuracy: 0.9681 - val_loss: 0.0872 - val_accuracy: 0.9629\n",
      "Epoch 40/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0727 - accuracy: 0.9675 - val_loss: 0.0712 - val_accuracy: 0.9691\n",
      "Epoch 41/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0710 - accuracy: 0.9689 - val_loss: 0.0751 - val_accuracy: 0.9662\n",
      "Epoch 42/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0723 - accuracy: 0.9681 - val_loss: 0.0693 - val_accuracy: 0.9694\n",
      "Epoch 43/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0697 - accuracy: 0.9681 - val_loss: 0.0722 - val_accuracy: 0.9691\n",
      "Epoch 44/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0708 - accuracy: 0.9681 - val_loss: 0.0705 - val_accuracy: 0.9694\n",
      "Epoch 45/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0704 - accuracy: 0.9679 - val_loss: 0.0694 - val_accuracy: 0.9701\n",
      "Epoch 46/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0692 - accuracy: 0.9688 - val_loss: 0.0735 - val_accuracy: 0.9681\n",
      "Epoch 47/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0712 - accuracy: 0.9682 - val_loss: 0.0700 - val_accuracy: 0.9714\n",
      "Epoch 48/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0697 - accuracy: 0.9686 - val_loss: 0.0705 - val_accuracy: 0.9678\n",
      "Epoch 49/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0699 - accuracy: 0.9683 - val_loss: 0.0689 - val_accuracy: 0.9691\n",
      "Epoch 50/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0686 - accuracy: 0.9681 - val_loss: 0.0720 - val_accuracy: 0.9671\n",
      "Epoch 51/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0707 - accuracy: 0.9679 - val_loss: 0.0747 - val_accuracy: 0.9655\n",
      "Epoch 52/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0714 - accuracy: 0.9683 - val_loss: 0.0784 - val_accuracy: 0.9658\n",
      "Epoch 53/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0685 - accuracy: 0.9684 - val_loss: 0.0696 - val_accuracy: 0.9691\n",
      "Epoch 54/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0684 - accuracy: 0.9688 - val_loss: 0.0689 - val_accuracy: 0.9697\n",
      "Epoch 55/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0677 - accuracy: 0.9687 - val_loss: 0.0770 - val_accuracy: 0.9678\n",
      "Epoch 56/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0691 - accuracy: 0.9687 - val_loss: 0.0686 - val_accuracy: 0.9691\n",
      "Epoch 57/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0669 - accuracy: 0.9692 - val_loss: 0.0742 - val_accuracy: 0.9671\n",
      "Epoch 58/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0694 - accuracy: 0.9688 - val_loss: 0.0686 - val_accuracy: 0.9694\n",
      "Epoch 59/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0679 - accuracy: 0.9685 - val_loss: 0.0730 - val_accuracy: 0.9691\n",
      "Epoch 60/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0669 - accuracy: 0.9698 - val_loss: 0.0694 - val_accuracy: 0.9688\n",
      "Epoch 61/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0679 - accuracy: 0.9686 - val_loss: 0.0761 - val_accuracy: 0.9658\n",
      "Epoch 62/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0675 - accuracy: 0.9691 - val_loss: 0.0730 - val_accuracy: 0.9662\n",
      "Epoch 63/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0722 - accuracy: 0.9672 - val_loss: 0.0683 - val_accuracy: 0.9691\n",
      "Epoch 64/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0662 - accuracy: 0.9704 - val_loss: 0.0717 - val_accuracy: 0.9684\n",
      "Epoch 65/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0685 - accuracy: 0.9679 - val_loss: 0.0678 - val_accuracy: 0.9694\n",
      "Epoch 66/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0672 - accuracy: 0.9705 - val_loss: 0.0738 - val_accuracy: 0.9681\n",
      "Epoch 67/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0663 - accuracy: 0.9702 - val_loss: 0.0685 - val_accuracy: 0.9704\n",
      "Epoch 68/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0663 - accuracy: 0.9693 - val_loss: 0.0693 - val_accuracy: 0.9684\n",
      "Epoch 69/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0663 - accuracy: 0.9688 - val_loss: 0.0738 - val_accuracy: 0.9681\n",
      "Epoch 70/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0672 - accuracy: 0.9696 - val_loss: 0.0749 - val_accuracy: 0.9639\n",
      "Epoch 71/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0682 - accuracy: 0.9690 - val_loss: 0.0699 - val_accuracy: 0.9662\n",
      "Epoch 72/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0669 - accuracy: 0.9696 - val_loss: 0.0671 - val_accuracy: 0.9684\n",
      "Epoch 73/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0657 - accuracy: 0.9697 - val_loss: 0.0754 - val_accuracy: 0.9668\n",
      "Epoch 74/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0661 - accuracy: 0.9691 - val_loss: 0.0708 - val_accuracy: 0.9694\n",
      "Epoch 75/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0650 - accuracy: 0.9697 - val_loss: 0.0729 - val_accuracy: 0.9675\n",
      "Epoch 76/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0697 - accuracy: 0.9679 - val_loss: 0.0720 - val_accuracy: 0.9668\n",
      "Epoch 77/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0656 - accuracy: 0.9696 - val_loss: 0.0723 - val_accuracy: 0.9697\n",
      "Epoch 78/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0659 - accuracy: 0.9694 - val_loss: 0.0699 - val_accuracy: 0.9684\n",
      "Epoch 79/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0667 - accuracy: 0.9689 - val_loss: 0.0719 - val_accuracy: 0.9697\n",
      "Epoch 80/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0646 - accuracy: 0.9689 - val_loss: 0.0672 - val_accuracy: 0.9691\n",
      "[[1.0000000e+00 7.6062953e-23 3.0518612e-18 1.5879316e-16 1.6611234e-26\n",
      "  2.3004352e-32]\n",
      " [4.3706755e-06 3.6936758e-09 6.3688020e-11 1.3447735e-08 2.2206230e-08\n",
      "  9.9999559e-01]\n",
      " [4.2978745e-06 3.6569923e-09 6.3498568e-11 1.3332630e-08 2.2288765e-08\n",
      "  9.9999571e-01]\n",
      " ...\n",
      " [2.7031705e-15 1.2691687e-11 9.9998343e-01 1.6431191e-05 3.6588419e-18\n",
      "  9.0804313e-08]\n",
      " [1.8595105e-07 4.2006534e-17 9.9949598e-01 2.8665943e-04 7.8449084e-05\n",
      "  1.3861124e-04]\n",
      " [1.8213980e-07 4.6260889e-25 2.9066703e-08 6.6609196e-10 9.9999762e-01\n",
      "  2.1660633e-06]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:04:39.162745\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 3, 6)              12        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 1, 6)              0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1, 16)             112       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 1, 16)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 1, 120)            2040      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 6)                 510       \n",
      "=================================================================\n",
      "Total params: 12,838\n",
      "Trainable params: 12,838\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 27658 samples, validate on 3074 samples\n",
      "Epoch 1/80\n",
      "27658/27658 [==============================] - 1s 49us/step - loss: 0.6273 - accuracy: 0.7736 - val_loss: 0.3273 - val_accuracy: 0.8988\n",
      "Epoch 2/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.2319 - accuracy: 0.9214 - val_loss: 0.1701 - val_accuracy: 0.9418\n",
      "Epoch 3/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.1666 - accuracy: 0.9429 - val_loss: 0.1446 - val_accuracy: 0.9463\n",
      "Epoch 4/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.1366 - accuracy: 0.9535 - val_loss: 0.2211 - val_accuracy: 0.8578\n",
      "Epoch 5/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.1201 - accuracy: 0.9572 - val_loss: 0.1020 - val_accuracy: 0.9707\n",
      "Epoch 6/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.1059 - accuracy: 0.9627 - val_loss: 0.0852 - val_accuracy: 0.9710\n",
      "Epoch 7/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0962 - accuracy: 0.9637 - val_loss: 0.0899 - val_accuracy: 0.9662\n",
      "Epoch 8/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0924 - accuracy: 0.9647 - val_loss: 0.0826 - val_accuracy: 0.9691\n",
      "Epoch 9/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0889 - accuracy: 0.9659 - val_loss: 0.0836 - val_accuracy: 0.9684\n",
      "Epoch 10/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0884 - accuracy: 0.9657 - val_loss: 0.0836 - val_accuracy: 0.9714\n",
      "Epoch 11/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0851 - accuracy: 0.9654 - val_loss: 0.0750 - val_accuracy: 0.9701\n",
      "Epoch 12/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0830 - accuracy: 0.9659 - val_loss: 0.0835 - val_accuracy: 0.9701\n",
      "Epoch 13/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0843 - accuracy: 0.9645 - val_loss: 0.0699 - val_accuracy: 0.9727\n",
      "Epoch 14/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0812 - accuracy: 0.9668 - val_loss: 0.0724 - val_accuracy: 0.9720\n",
      "Epoch 15/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0828 - accuracy: 0.9660 - val_loss: 0.0736 - val_accuracy: 0.9736\n",
      "Epoch 16/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0805 - accuracy: 0.9670 - val_loss: 0.0914 - val_accuracy: 0.9668\n",
      "Epoch 17/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0806 - accuracy: 0.9666 - val_loss: 0.0804 - val_accuracy: 0.9623\n",
      "Epoch 18/80\n",
      "27658/27658 [==============================] - 1s 40us/step - loss: 0.0816 - accuracy: 0.9660 - val_loss: 0.0723 - val_accuracy: 0.9730\n",
      "Epoch 19/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0778 - accuracy: 0.9664 - val_loss: 0.0712 - val_accuracy: 0.9750\n",
      "Epoch 20/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0784 - accuracy: 0.9660 - val_loss: 0.0688 - val_accuracy: 0.9714\n",
      "Epoch 21/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0796 - accuracy: 0.9656 - val_loss: 0.0689 - val_accuracy: 0.9717\n",
      "Epoch 22/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0783 - accuracy: 0.9667 - val_loss: 0.0671 - val_accuracy: 0.9717\n",
      "Epoch 23/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0780 - accuracy: 0.9670 - val_loss: 0.0712 - val_accuracy: 0.9723\n",
      "Epoch 24/80\n",
      "27658/27658 [==============================] - 1s 35us/step - loss: 0.0733 - accuracy: 0.9681 - val_loss: 0.0716 - val_accuracy: 0.9720\n",
      "Epoch 25/80\n",
      "27658/27658 [==============================] - 1s 36us/step - loss: 0.0785 - accuracy: 0.9667 - val_loss: 0.0692 - val_accuracy: 0.9691\n",
      "Epoch 26/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0770 - accuracy: 0.9664 - val_loss: 0.0725 - val_accuracy: 0.9723\n",
      "Epoch 27/80\n",
      "27658/27658 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.96 - 1s 37us/step - loss: 0.0765 - accuracy: 0.9662 - val_loss: 0.0882 - val_accuracy: 0.9587\n",
      "Epoch 28/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0767 - accuracy: 0.9670 - val_loss: 0.0686 - val_accuracy: 0.9730\n",
      "Epoch 29/80\n",
      "27658/27658 [==============================] - 1s 37us/step - loss: 0.0757 - accuracy: 0.9676 - val_loss: 0.0746 - val_accuracy: 0.9671\n",
      "Epoch 30/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0750 - accuracy: 0.9672 - val_loss: 0.0677 - val_accuracy: 0.9743\n",
      "Epoch 31/80\n",
      "27658/27658 [==============================] - 1s 39us/step - loss: 0.0741 - accuracy: 0.9670 - val_loss: 0.0698 - val_accuracy: 0.9697\n",
      "Epoch 32/80\n",
      "27658/27658 [==============================] - 1s 38us/step - loss: 0.0747 - accuracy: 0.9674 - val_loss: 0.0765 - val_accuracy: 0.9645\n",
      "Epoch 00032: early stopping\n",
      "[[1.0000000e+00 4.7080886e-15 2.4039449e-12 7.1158912e-10 1.3513439e-19\n",
      "  2.9548158e-20]\n",
      " [1.5079772e-05 1.3199026e-07 7.8477171e-08 6.2134930e-08 1.4052978e-06\n",
      "  9.9998319e-01]\n",
      " [1.4657435e-05 1.2922057e-07 7.7285442e-08 6.0858476e-08 1.4124518e-06\n",
      "  9.9998367e-01]\n",
      " ...\n",
      " [1.0309376e-09 1.7187045e-10 9.9982029e-01 1.4943493e-04 1.6033973e-09\n",
      "  3.0268366e-05]\n",
      " [3.6399640e-04 6.4330923e-12 9.1734517e-01 7.9470202e-03 7.4058674e-02\n",
      "  2.8502068e-04]\n",
      " [1.1464669e-05 1.0726651e-18 1.9652285e-08 1.3987892e-07 9.9998784e-01\n",
      "  4.6563298e-07]]\n",
      "[0 5 5 ... 2 2 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:05:16.881544\n",
      "region_label= [6, 7, 2, 4, 3, 5]\n",
      "dist_table_truth\n",
      " [[1342  897    0 1553   28  307    0    0    0]\n",
      " [ 619    0    3  139  247    3    0    0    0]\n",
      " [ 180  666 2306   17 1250  543    0    0    0]\n",
      " [ 218 1181 2086    0 2142  262    0    0    0]\n",
      " [2275  544   28 2532   58 1424    0    0    0]\n",
      " [ 146  721    7  114  233 1727    0    0    0]]\n",
      "num of merged_region_image 0 5348\n",
      "num of merged_region_image 1 5288\n",
      "num of merged_region_image 2 5399\n",
      "num of merged_region_image 3 5166\n",
      "num of merged_region_image 4 5894\n",
      "num of merged_region_image 5 3637\n",
      "Counter({-1: 12485, 4: 5894, 2: 5399, 0: 5348, 1: 5288, 3: 5166, 5: 3637})\n",
      "===========  ITE = 4   ===========\n",
      "used_img 30732 30732\n",
      "working_img(=other images=unclean images) 12485 12485\n",
      "merged regions 157 157\n",
      "other_regions 43 43\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>149</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>152</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.52</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>169</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>176</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>182</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>184</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>190</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>191</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>195</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>106</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.23</td>\n",
       "      <td>54</td>\n",
       "      <td>244</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    other index  pred label  truth  rate    0    1    2    3    4    5\n",
       "0             8           5      5  0.38    0    3    0    0    0  361\n",
       "1             9           5      0  0.41  245    0    1    0    1    0\n",
       "2           137           5      5  0.59    0    0    0    0    0  277\n",
       "3            12           2      4  0.82   13    0    0    0   77    2\n",
       "4            20           5      7  0.54    0    0    0    0    0    0\n",
       "5           149          -1      8  0.00    0    0    0    0    1    3\n",
       "6            22           2      8  0.57    0    0    0    0    0    0\n",
       "7           152           3      4  0.52    3    0    0    0  399    0\n",
       "8           153           3      8  0.49    1    0    0    0    0    0\n",
       "9           154           4      8  0.93    0    0    0    0    0    1\n",
       "10           28           4      8  0.87    0    0    0    0    0    0\n",
       "11           32           4      8  0.54    2    0    0    0    0    1\n",
       "12           34           3      1  0.61    0   69    0    0    0    0\n",
       "13          164           2      1  0.71    2  268    0    0    1    1\n",
       "14           39          -1      4  0.00    2    0    2    0  117    0\n",
       "15          169          -1      5  0.00    0    0    0    0    0  112\n",
       "16          170           4      1  0.79    6  601    0    0    0    2\n",
       "17          171           4      0  0.47  120   12    0    0    0    0\n",
       "18           44           4      8  0.76    1    0    0    0    1    0\n",
       "19           46           2      8  0.40    5    0    0    0    1    0\n",
       "20          176           3      4  0.54    3    2    0    0   96    0\n",
       "21          182           3      1  0.65    1  233    0    0    0    0\n",
       "22          184          -1      8  0.00    0    0    0    0    0    0\n",
       "23           57           2      1  0.48    0  128    0    0    0    0\n",
       "24          187           3      2  0.51    0    0  223    0    0    0\n",
       "25          190           4      8  0.71    1    0    0    0    0    4\n",
       "26          191          -1      3  0.00    5    0    0  227    1    0\n",
       "27          195           4      1  0.70    2   21    0    0    0   17\n",
       "28          196           4      0  0.66  372    2    1    1    4    1\n",
       "29           69           0      3  0.52    0    0    0  242    0    0\n",
       "30           70          -1      3  0.00    0    0    0  183    0    0\n",
       "31          199           2      8  0.75    0    0    2    0    0    0\n",
       "32           72           4      5  0.60    1    0    0    0    0  238\n",
       "33           73           4      0  0.50   30    0    0    0    0    0\n",
       "34           79           1      6  0.38    2    0    0    0    0    0\n",
       "35           82           1      7  0.37    0    0    0    0    0    0\n",
       "36           84           2      8  0.46    0    0    0    0    0    0\n",
       "37           94           4      5  0.89    0    0    0    0    0  169\n",
       "38           97           1      6  0.44    0    0    0    0    0    0\n",
       "39          102           3      2  0.76    0    1  629    0    1    0\n",
       "40          106          -1      7  0.00    1    0    0    0    0    5\n",
       "41          118           4      8  0.86    3    4    0    1    1    3\n",
       "42          123           5      5  0.23   54  244   98   62   18  457"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {2} [12, 199, 164] 368\n",
      "added label, regions, img amount: {3} [102] 481\n",
      "added label, regions, img amount: {4} [154, 94, 28, 118, 170] 1492\n",
      "Not getting into residuals\n"
     ]
    }
   ],
   "source": [
    "for case_i in range(NUM_CASE):\n",
    "\n",
    "    #===== create folder case1, case2, case3...\n",
    "    print(\"case=\",case_i+1)\n",
    "    newpath = './case' + str(case_i+1)\n",
    "    if (not INTE_bool):\n",
    "        if not os.path.exists(newpath):   #No necessary in Integration\n",
    "            os.makedirs(newpath)\n",
    "    \n",
    "    #==== open csv 1\n",
    "    csv_path1 = newpath+'/' + 'accu_history.csv'\n",
    "    with open(csv_path1, 'a', newline='') as f:\n",
    "        csv_file = csv.writer(f)\n",
    "        csv_file.writerow(['ITE', 'correct', 'denominator', 'accu', 'description'])\n",
    "\n",
    "# 1.\n",
    "    if (not INTE_bool):\n",
    "        create_image_0(PATH6, case_i)   #No necessary in Integration\n",
    "\n",
    "\n",
    "    for ITE in range(ITE_START, ITE_END):\n",
    "# 2. CNN\n",
    "        CNN_part(PATH5,ITE)\n",
    "\n",
    "# 3. statistic\n",
    "        statistic(PATH5,ITE)\n",
    "\n",
    "# 4. merged_and_expand \n",
    "        merged_and_expand(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret the accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(overall 5-consensus)\n",
      "criterion 1\n",
      "correct in 5-consensus\n",
      "------------------------------------\n",
      "5-consensus\n",
      "\n",
      "ITE ITE     correct / denominator = accu\n",
      "ITE 0     2625 / 29339 = 0.089\n",
      "ITE 0     2625 / 29339 = 0.089\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "criterion 2\n",
      "correct in 5-consensus\n",
      "------------------------------------\n",
      "all\n",
      "\n",
      "ITE ITE     correct / denominator = accu\n",
      "ITE 0     2625 / 43217 = 0.061\n",
      "ITE 0     2625 / 43217 = 0.061\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(clean)\n",
      "criterion 3\n",
      "correct in train in 5-consensus\n",
      "------------------------------------\n",
      "train in 5-consensus\n",
      "\n",
      "ITE ITE     correct / denominator = accu\n",
      "ITE 0     9403 / 43217 = 0.218\n",
      "ITE 0     9403 / 43217 = 0.218\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "criterion 4\n",
      "correct in train in 5-consensus \n",
      "------------------------------------\n",
      "all\n",
      "\n",
      "ITE 0     8326 / 38182 = 0.218\n",
      "ITE 0     8326 / 38182 = 0.218\n",
      "ITE ITE     correct / denominator = accu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(unclean)\n",
      "criterion 5\n",
      "correct in test in 5-consensus\n",
      "------------------------------------\n",
      "test in 5-consensus\n",
      "\n",
      "ITE 0     8326 / 43217 = 0.193\n",
      "ITE 0     8326 / 43217 = 0.193\n",
      "ITE 0     7809 / 37255 = 0.21\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "criterion 6\n",
      "correct in test in 5-consensus\n",
      "------------------------------------\n",
      "all\n",
      "\n",
      "ITE 0     7243 / 13878 = 0.522\n",
      "ITE 0     7243 / 13878 = 0.522\n",
      "ITE 0     7809 / 43217 = 0.181\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(majority)\n",
      "criterion 7\n",
      "correct\n",
      "------------------\n",
      "all\n",
      "\n",
      "ITE 0     7243 / 43217 = 0.168\n",
      "ITE 0     7243 / 43217 = 0.168\n",
      "ITE 0     7243 / 13878 = 0.522\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "interpret_accu_results(interpret_path, AMOUNT_ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region_label= [6, 7, 2, 4, 3, 5]\n",
      "dist_table_truth\n",
      " [[1342  897    0 1553   28  307 1566   21  527]\n",
      " [ 619    0    3  139  247    3 1994 2917   42]\n",
      " [ 180  666 2306   17 1250  543   19  427  926]\n",
      " [ 218 1181 2086    0 2142  262  109    0  558]\n",
      " [2275  544   28 2532   58 1424   60   23 2257]\n",
      " [ 146  721    7  114  233 1727 1003  493  224]]\n",
      "num of merged_region_image 0 5348\n",
      "num of merged_region_image 1 5288\n",
      "num of merged_region_image 2 5399\n",
      "num of merged_region_image 3 5166\n",
      "num of merged_region_image 4 5894\n",
      "num of merged_region_image 5 3637\n",
      "Counter({-1: 12485, 4: 5894, 2: 5399, 0: 5348, 1: 5288, 3: 5166, 5: 3637})\n"
     ]
    }
   ],
   "source": [
    "statistic(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==== merged_and_expand(PATH5,ITE) ====\n",
      "===========  ITE = 4   ===========\n",
      "used_img 30732 30732\n",
      "working_img(=other images=unclean images) 12485 12485\n",
      "merged regions 157 157\n",
      "other_regions 43 43\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>245</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.82</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>149</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>152</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.52</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>153</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>154</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>164</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.71</td>\n",
       "      <td>2</td>\n",
       "      <td>268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>39</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>169</td>\n",
       "      <td>-1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>6</td>\n",
       "      <td>601</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>120</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.40</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>176</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.54</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>182</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>184</td>\n",
       "      <td>-1</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>187</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>190</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>191</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>195</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.70</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>196</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.66</td>\n",
       "      <td>372</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>70</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.38</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>106</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>123</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.23</td>\n",
       "      <td>54</td>\n",
       "      <td>244</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    other index  pred label  truth  rate    0    1    2    3    4    5\n",
       "0             8           5      5  0.38    0    3    0    0    0  361\n",
       "1             9           5      0  0.41  245    0    1    0    1    0\n",
       "2           137           5      5  0.59    0    0    0    0    0  277\n",
       "3            12           2      4  0.82   13    0    0    0   77    2\n",
       "4            20           5      7  0.54    0    0    0    0    0    0\n",
       "5           149          -1      8  0.00    0    0    0    0    1    3\n",
       "6            22           2      8  0.57    0    0    0    0    0    0\n",
       "7           152           3      4  0.52    3    0    0    0  399    0\n",
       "8           153           3      8  0.49    1    0    0    0    0    0\n",
       "9           154           4      8  0.93    0    0    0    0    0    1\n",
       "10           28           4      8  0.87    0    0    0    0    0    0\n",
       "11           32           4      8  0.54    2    0    0    0    0    1\n",
       "12           34           3      1  0.61    0   69    0    0    0    0\n",
       "13          164           2      1  0.71    2  268    0    0    1    1\n",
       "14           39          -1      4  0.00    2    0    2    0  117    0\n",
       "15          169          -1      5  0.00    0    0    0    0    0  112\n",
       "16          170           4      1  0.79    6  601    0    0    0    2\n",
       "17          171           4      0  0.47  120   12    0    0    0    0\n",
       "18           44           4      8  0.76    1    0    0    0    1    0\n",
       "19           46           2      8  0.40    5    0    0    0    1    0\n",
       "20          176           3      4  0.54    3    2    0    0   96    0\n",
       "21          182           3      1  0.65    1  233    0    0    0    0\n",
       "22          184          -1      8  0.00    0    0    0    0    0    0\n",
       "23           57           2      1  0.48    0  128    0    0    0    0\n",
       "24          187           3      2  0.51    0    0  223    0    0    0\n",
       "25          190           4      8  0.71    1    0    0    0    0    4\n",
       "26          191          -1      3  0.00    5    0    0  227    1    0\n",
       "27          195           4      1  0.70    2   21    0    0    0   17\n",
       "28          196           4      0  0.66  372    2    1    1    4    1\n",
       "29           69           0      3  0.52    0    0    0  242    0    0\n",
       "30           70          -1      3  0.00    0    0    0  183    0    0\n",
       "31          199           2      8  0.75    0    0    2    0    0    0\n",
       "32           72           4      5  0.60    1    0    0    0    0  238\n",
       "33           73           4      0  0.50   30    0    0    0    0    0\n",
       "34           79           1      6  0.38    2    0    0    0    0    0\n",
       "35           82           1      7  0.37    0    0    0    0    0    0\n",
       "36           84           2      8  0.46    0    0    0    0    0    0\n",
       "37           94           4      5  0.89    0    0    0    0    0  169\n",
       "38           97           1      6  0.44    0    0    0    0    0    0\n",
       "39          102           3      2  0.76    0    1  629    0    1    0\n",
       "40          106          -1      7  0.00    1    0    0    0    0    5\n",
       "41          118           4      8  0.86    3    4    0    1    1    3\n",
       "42          123           5      5  0.23   54  244   98   62   18  457"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {2} [12, 199, 164] 368\n",
      "added label, regions, img amount: {3} [102] 481\n",
      "added label, regions, img amount: {4} [154, 94, 28, 118, 170] 1492\n",
      "Not getting into residuals\n"
     ]
    }
   ],
   "source": [
    "merged_and_expand(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
