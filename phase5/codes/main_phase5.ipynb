{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; Feb, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 5 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@stat.sinica.edu.tw </p>\n",
    "### <p> Institute of Statistical Science, Academia Sinica, Taipei, Taiwan.</p>  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import random \n",
    "import time\n",
    "import datetime\n",
    "from itertools import chain\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "MNIST     = False\n",
    "NUM_CASE  = 1\n",
    "INTE_bool = False  #True: Integrate two networks VGG+ResNet    False: single network\n",
    "SAVE_bool = True\n",
    "ITE_FROM  = 5 # This setting is ONLY for Integration\n",
    "REG_COLUMN = \"Spec200\"\n",
    "RAW_2D_DATA = False\n",
    "\n",
    "\n",
    "PATH4='../../phase3/data/ResNet18_PlantDisease_45K_Spec200.csv'\n",
    "PATH5='../../phase3/data/embedded_data.pickle'\n",
    "PATH6='../data/PlantDisease_ResNet_K200_mergedseedclasslabels_version2.txt'\n",
    "PATH7='../../phase3/data/region_for_phase5.pickle'\n",
    "\n",
    "#=================================================================\n",
    "# 20240319\n",
    "if RAW_2D_DATA: # 2D\n",
    "    from CNN_Modules import ME_CNN\n",
    "else: # 1D\n",
    "    from CNN_Modules_1D import ME_CNN\n",
    "    \n",
    "\n",
    "if (INTE_bool):\n",
    "    ITE_START=ITE_FROM\n",
    "    ITE_END=ITE_FROM+4\n",
    "else:\n",
    "    ITE_START=0\n",
    "    ITE_END=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_csv(x, path):\n",
    "    with open(path,'a+', newline='') as f:\n",
    "        csv_file = csv.writer(f)#   = f.write()\n",
    "        csv_file.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only for single network. No necessary in Integrated networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not INTE_bool):\n",
    "    def create_image_0(PATH6, case_i):\n",
    "        # ===================\n",
    "        #\n",
    "        #  prepare  merged_region_image_0\n",
    "        #\n",
    "        #====================\n",
    "        # (A)\n",
    "        #get \"(1)merged_region\"      only seed regions, no neighboring regions\n",
    "        df = pandas.read_csv(PATH6, delim_whitespace=' ', header=0,  index_col=None)\n",
    "        table = df.to_numpy()\n",
    "        print(\"mergedseedclasslabels table\")\n",
    "        display(table)\n",
    "\n",
    "        merged_region=[]\n",
    "        for i in range(min(table.T[case_i+1]), max(table.T[case_i+1])+1):  #18 ---merge to --> 10\n",
    "            addr=np.where(table.T[case_i+1]==i)[0] # 2nd column equal to 0(min),1,2,3...10(max); DO NOT consider 3rd column, which is hidden\n",
    "            if(len(addr) and i>0): #if not empty and i=0 is the invalid seed region.\n",
    "                merged_region.append(table[addr][:,0].tolist())\n",
    "        print(\"merged_region\")\n",
    "        display(merged_region)\n",
    "\n",
    "\n",
    "        # (B)\n",
    "        #get \"merged_reg_and_nei\"\n",
    "        #get \"merged_reg_and_nei_image\"\n",
    "        #generate \"merged_region_image_0.pickle\"\n",
    "\n",
    "        # (B_a)=== without neighbors ====\n",
    "        #if ((DATASET == 2) or (DATASET == 4)): \n",
    "        ##20240105\n",
    "        if (not True): \n",
    "            # ==== collect regions. No neighbors, just use merged regions ====\n",
    "            merged_reg_and_nei=merged_region.copy()\n",
    "\n",
    "            # ==== collect images ====\n",
    "            img_temp=[]\n",
    "            for i in range(len(merged_region)):\n",
    "                addr=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    temp=np.where(all_region_index==merged_region[i][j])[0].tolist()   #tolist(): convert temp into list\n",
    "                    addr=addr+temp\n",
    "                    print(len(temp),end=' ')\n",
    "                img_temp.append(addr)\n",
    "                print(\"=\",len(img_temp[i]))\n",
    "            merged_reg_and_nei_image = img_temp.copy()\n",
    "\n",
    "\n",
    "        # (B_b)=== with neighbors ==== \n",
    "        else: \n",
    "            with open(PATH7, 'rb') as f:\n",
    "                pre_region, pre_reg_nei, pre_region_image_pure, pre_region_image= pickle.load(f)\n",
    "            #    1reg         2reg+nei        1's img            2's img\n",
    "\n",
    "            # ==== collect regions with neighbors====\n",
    "            # remove duplicate  -->  https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them\n",
    "            merged_reg_and_nei=[]\n",
    "            NUM_region=len(merged_region)\n",
    "            for i in range(NUM_region):\n",
    "                temp=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0] \n",
    "                    temp=temp+pre_reg_nei[idx]\n",
    "                    print(idx,pre_region[idx])\n",
    "                merged_reg_and_nei.append(temp)\n",
    "\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(merged_reg_and_nei[i]) != len(set(merged_reg_and_nei[i]))):\n",
    "                    a=merged_reg_and_nei[i].copy()\n",
    "\n",
    "                    # find the duplicate.\n",
    "                    seen = set()\n",
    "                    dupli                 = [x for x in a if (x in seen or seen.add(x))]\n",
    "                    print(\"***duplicates:\",dupli)\n",
    "\n",
    "                    # keep fisrt one, remove succeeding duplicates.\n",
    "                    seen = set()\n",
    "                    merged_reg_and_nei[i] = [x for x in a if not (x in seen or seen.add(x))]  # a is the data to process; x is a working varialbe\n",
    "                    print(\"unique:\",merged_reg_and_nei[i])\n",
    "\n",
    "                print(\"total\",len(merged_reg_and_nei[i]),end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei\")\n",
    "            for i in range(len(merged_reg_and_nei)):\n",
    "                print(merged_reg_and_nei[i])\n",
    "\n",
    "\n",
    "            # Collect images\n",
    "            merged_reg_and_nei_image=[]\n",
    "            for i in range(NUM_region):\n",
    "                #search and add\n",
    "                img=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0]\n",
    "                    print(len(pre_region_image[idx]),\"(\",idx,\")\",end=' ')\n",
    "                    img=img+pre_region_image[idx] \n",
    "                print(\"=\",len(img),end=\" \")\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(img) != len(set(img))):\n",
    "                    img=list(set(img)) #remove duplicates\n",
    "                    print(\"     **duplicate, shrink to\",len(img),end=\"\\n\")  \n",
    "                else:\n",
    "                    print(end=\"\\n\")\n",
    "\n",
    "                #append\n",
    "                merged_reg_and_nei_image.append(img)\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei_image\")\n",
    "            for i in range(len(merged_reg_and_nei_image)):\n",
    "                print(len(merged_reg_and_nei_image[i]),merged_reg_and_nei_image[i][:5],\"...\")\n",
    "\n",
    "        # save\n",
    "        if (SAVE_bool):\n",
    "            with open(newpath+'/merged_region_image_0.pickle', 'wb') as f:\n",
    "                pickle.dump([merged_reg_and_nei, merged_reg_and_nei_image], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_part(PATH5,ITE):\n",
    "    TRIALS          = 5\n",
    "\n",
    "    savelog_path = newpath+'/' + 'log.txt'\n",
    "\n",
    "    # ==== test_array ====\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "        \n",
    "    if RAW_2D_DATA: # 2D\n",
    "        print(\"\")\n",
    "    else: # 1D\n",
    "        test_array = np.expand_dims(test_array, axis = -1)\n",
    "\n",
    "    \n",
    "    #if((DATASET==2) or (DATASET==4)):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #elif(DATASET==1):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #    test_array /= 255\n",
    "    #elif(DATASET==0):\n",
    "    #    test_array /= 255\n",
    "    #display(np.shape(test_array))\n",
    "\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    region_image=merged_region_image.copy()\n",
    "    del merged_reg_and_nei\n",
    "\n",
    "\n",
    "    NUM_region=len(region_image)\n",
    "    print(\"NUM_region\",NUM_region)\n",
    "\n",
    "\n",
    "    from itertools import chain\n",
    "    region_image_flatten=list(chain.from_iterable(region_image))\n",
    "    print(\"number of clean images\",len(region_image_flatten))\n",
    "\n",
    "\n",
    "    ROUND_start = time.time()\n",
    "    #========  merge ==========\n",
    "    #prepare selected_region, region\n",
    "    for n in range(1): #extra_original\n",
    "    #   #reset\n",
    "        region=region_image.copy()\n",
    "        region=list(region)\n",
    "        selected_region = list(range(NUM_region))  #[0,1,2, ... ,29]\n",
    "\n",
    "        #merge\n",
    "        if (n > 4):\n",
    "            p1=comb[n-1][0]\n",
    "            p2=comb[n-1][1]\n",
    "            region[p1]=region[p1]+region[p2]\n",
    "            region.pop(p2)\n",
    "            selected_region.pop(-1)  # remove last region index\n",
    "        #original\n",
    "        else:  #n=0\n",
    "            p1=0\n",
    "            p2=0\n",
    "\n",
    "        print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "        # ===== one CNN =============\n",
    "        NUM_CLASSES = len(selected_region)  #NUM_CLASSES should be here to update for each loop\n",
    "\n",
    "        # input image and label\n",
    "        Input_img     = []\n",
    "        Input_img_len = []\n",
    "        for c,sel in enumerate(selected_region, start=0):\n",
    "            Input_img = Input_img + list(region[sel])\n",
    "            Input_img_len.append(len(region[sel])) #can only concatenate list (not \"int\") to list    \n",
    "            \n",
    "        # 20240319\n",
    "        if RAW_2D_DATA: # 2D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            H           = np.shape(test_array[0])[1]\n",
    "            train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "        else: # 1D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "                  \n",
    "        train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "        # fill up the training label to each training image\n",
    "        current_train_label = np.zeros(len(train_array), dtype=int)  # Assign 0 to the label\n",
    "        accum_base=0  #accumulate\n",
    "        for label in range(1,NUM_CLASSES):\n",
    "            sector = Input_img_len[label-1]\n",
    "            accum_base = accum_base + sector  # sector is the sector length\n",
    "            current_train_label[accum_base:] = label  # fill the label\n",
    "\n",
    "\n",
    "        # CNN\n",
    "        #===============================================\n",
    "        one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "        one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "        model_history = np.zeros(TRIALS, dtype=list)\n",
    "        print(\"NUM_CLASSES\",NUM_CLASSES)\n",
    "        print(\"current_train_label: \",list(set(current_train_label)))\n",
    "        for r in range(TRIALS):  #10\n",
    "            one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                    x_train     = train_array,\n",
    "                    train_label = current_train_label,\n",
    "                    test_array  = test_array,\n",
    "                    true_answer = test_label_answer,\n",
    "                    Num_Classes = NUM_CLASSES\n",
    "                    )\n",
    "            print(type(model_history))\n",
    "\n",
    "\n",
    "            # ===== delete CNN tensors =====\n",
    "            from keras import backend as K\n",
    "            K.clear_session()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"One CNN, r: \",r)\n",
    "            ROUND_duration = time.time() - ROUND_start\n",
    "            print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "\n",
    "        # === save to file ===\n",
    "        #This is useless in phase IV. Prepare for further checking in the future.\n",
    "        savefile_path = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'_'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path, 'wb') as f:\n",
    "            pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "        savefile_path2 = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_5_tests_simple_ITE'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path2, 'wb') as f:\n",
    "            pickle.dump([one_predicted_results, one_predict_percentage], f)\n",
    "\n",
    "        # === save to log ===    \n",
    "        savelog = open(savelog_path, 'a+')\n",
    "        print(\"\\n\", savefile_path, file = savelog)\n",
    "        print(\"Saved parameters: Input_img, Input_img_len, one_predicted_results, one_predict_percentage\", file = savelog) #0722\n",
    "\n",
    "        # total time\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Completion time: \", datetime.datetime.now(), file = savelog)\n",
    "        print(\"Total Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)), file = savelog)\n",
    "\n",
    "        savelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_method(PATH5,NUM_region,region_label,table_1D):\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "    \n",
    "    dist_table_truth=np.zeros((NUM_region,NUM_region),dtype=int)\n",
    "    region_correct=[]\n",
    "    region_amount=[]\n",
    "    overall_correct=0\n",
    "    overall_amount=0\n",
    "\n",
    "    for i in range(NUM_region):\n",
    "        #(1) input\n",
    "        region_image=np.where(table_1D==i)[0]\n",
    "        #region_image=merged_region_image[i].copy()\n",
    "        \n",
    "        #(2) establish confusion matrix\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(test_label_answer[region_image]==j)[0]) #the number of images which equals to true answer \n",
    "        \n",
    "        #(3) statisitc\n",
    "        region_correct.append(dist_table_truth[i][region_label[i]])\n",
    "        region_amount.append(len(region_image))\n",
    "      \n",
    "    #(4) statistic for overall\n",
    "    overall_correct=sum(region_correct)\n",
    "    overall_amount=sum(region_amount)\n",
    "\n",
    "    return region_correct,region_amount,overall_correct,overall_amount,dist_table_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(PATH5,ITE):\n",
    "    # input 1:\n",
    "    # (1)merged_region_image_(ITE)\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    del merged_reg_and_nei\n",
    "    NUM_region=len(merged_region_image)\n",
    "    \n",
    "    # (2)test_label_answer\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "    # (3)get consistent result table\n",
    "    with open(newpath+'/(classes=' + str(NUM_region) + ')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "    LENGTH=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    " \n",
    "    # (4) get region_label \n",
    "    region_label=[] #true label by selecting dominate ones\n",
    "    for i in range(NUM_region):\n",
    "        region_image=merged_region_image[i].copy()\n",
    "        region_label.append(collections.Counter(test_label_answer[region_image]).most_common()[0][0])  #images --> true label --> most_common label\n",
    "\n",
    "\n",
    "   #========================================     \n",
    "   # (1)train + test\n",
    "    a2,b2,c2,d2,e2=statistic_method(PATH5,NUM_region,region_label,Original_result)\n",
    "    na2=np.asarray(a2)\n",
    "    nb2=np.asarray(b2)\n",
    "    nc2=np.asarray(c2)\n",
    "    nd2=np.asarray(d2)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c2, d2,      round(nc2/nd2    ,3), \"5con over all, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c2, all_num, round(nc2/all_num,3), \"5con over all\"], csv_path1)\n",
    " \n",
    "\n",
    "        \n",
    "    # (2)train\n",
    "    train_results=-1*np.ones(LENGTH,dtype=int)\n",
    "    for i in range(NUM_region):\n",
    "        images=merged_region_image[i]\n",
    "        train_results[images]=i\n",
    "        print(\"num of merged_region_image\",i,len(merged_region_image[i]))\n",
    "    print(collections.Counter(train_results))\n",
    "        \n",
    "    a1,b1,c1,d1,e1=statistic_method(PATH5,NUM_region,region_label,train_results)\n",
    "    na1=np.asarray(a1)  #region_correct\n",
    "    nb1=np.asarray(b1)  #region_amount\n",
    "    nc1=np.asarray(c1)  #overall_correct\n",
    "    nd1=np.asarray(d1)  #overall_amount\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c1, d1, round(nc1/nd1,3), \"5con over trained\"], csv_path1)\n",
    "    append_csv([ITE, c1, all_num, round(nc1/all_num,3), \"5con over all\"], csv_path1)\n",
    "    \n",
    "        \n",
    "    # (3)test\n",
    "    # remove training data(good images), only check test data(bad images)\n",
    "    from itertools import chain\n",
    "    used_image=merged_region_image.copy()\n",
    "    used_image=list(chain.from_iterable(used_image))\n",
    "    Original_result2=Original_result.copy()\n",
    "    Original_result2[used_image]=-2\n",
    "\n",
    "        \n",
    "    a4,b4,c4,d4,e4=statistic_method(PATH5,NUM_region,region_label, Original_result2)\n",
    "    na4=np.asarray(a4)\n",
    "    nb4=np.asarray(b4)\n",
    "    nc4=np.asarray(c4)\n",
    "    nd4=np.asarray(d4) #this is len(Original_result)-len(used_image)-len(unconsistent)\n",
    "    untrain=len(Original_result)-len(used_image)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c4, untrain, round(nc4/untrain, 3), \"5con over untrained, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c4, all_num, round(nc4/all_num, 3), \"5con over untrained (unclean)\"], csv_path1)\n",
    "\n",
    "        \n",
    "    # (4)set majority as label for each image        \n",
    "    # set majority from 5 trias as label for each image\n",
    "    predicted_results_major=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        predicted_results_major[i]=collections.Counter(one_predicted_results.T[i]).most_common()[0][0]\n",
    "    \n",
    "\n",
    "    a3,b3,c3,d3,e3=statistic_method(PATH5,NUM_region,region_label,predicted_results_major)\n",
    "    na3=np.asarray(a3)\n",
    "    nb3=np.asarray(b3)\n",
    "    nc3=np.asarray(c3)\n",
    "    nd3=np.asarray(d3) #this is all in majority criterion\n",
    "    append_csv([ITE, c3, d3, round(nc3/nd3    ,3), \"majo over all\"], csv_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_and_expand(PATH5,ITE):\n",
    "# all4.     \n",
    "    # load\n",
    "    with open('./region_initials.pickle', 'rb') as f:\n",
    "        all_region_index, all_region_image = pickle.load(f)\n",
    "    MAX_region = max(all_region_index)\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    NUM_region = len(merged_reg_and_nei) # NUM_region is the number of clusters\n",
    "\n",
    "    with open(newpath+'/(classes='+str(NUM_region)+')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "\n",
    "    # choose absolutely consistent images\n",
    "    NUM_test=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(NUM_test,dtype=int)\n",
    "\n",
    "    # (***)\n",
    "    # As set contains only unique elements, so convert the list to set.\n",
    "    # If set size is 1 then it means all elements in given list are same\n",
    "    for i in range(NUM_test):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    "    \n",
    "    used_img=list(chain.from_iterable(merged_region_image))\n",
    "    used_img=np.sort(used_img)\n",
    "    working_img = np.asarray(list(  set(range(NUM_test))-set(used_img)  ))  #working_img means the unclean ones for working on the further adding process\n",
    "    print(\"===========  ITE =\",ITE, \"  ===========\")    \n",
    "    print(\"used_img\",len(used_img), len(set(used_img)))    \n",
    "    print(\"working_img(=other images=unclean images)\",len(working_img), len(set(working_img)))\n",
    "\n",
    "    # save clean and unclean images\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/clean_and_unclean_image_ITE='+str(ITE)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([used_img, working_img], f) #used_img is clean, working_img is others\n",
    "\n",
    "    # other_regions\n",
    "    # ==== Process of other regions. Generate \"other_regions\" ====\n",
    "    merged_reg_and_nei_flatten=list(chain.from_iterable(merged_reg_and_nei))\n",
    "    print(\"merged regions\", len(merged_reg_and_nei_flatten), len(set(merged_reg_and_nei_flatten)))\n",
    "    other_regions       = list(  set(range(1,MAX_region+1))-set(merged_reg_and_nei_flatten)  ) #region index exclude used regions. 1 to 200.\n",
    "    print(\"other_regions\",len(other_regions), len(set(other_regions)))\n",
    "    \n",
    "    dmn_img             = [] # Index of dmn_img is consistent with other_regions\n",
    "    NUM_other_regions   = len(other_regions) # number of clusters in other regions\n",
    "    dist_table_truth    = np.zeros((NUM_other_regions,NUM_region),dtype=int)\n",
    "    p_reg_label_dmn     = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in predicted lagels.\n",
    "    grd_reg_answer_dmn  = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in true answers.\n",
    "    p_reg_dmn_rate      = np.zeros(NUM_other_regions,dtype=float) #one value. dominate ratio in a region\n",
    "\n",
    "    #(1) other_regions      --> establish all other region table \n",
    "    for i,region_name in enumerate(other_regions): #check all other regions\n",
    "\n",
    "        #(a)===== predicted images (multiple values) =====\n",
    "        p_img        = all_region_image[region_name-1] # In this region, get their images. \"region_name-1\" is due to region index starts from 1 to 200\n",
    "        p_img_label  = Original_result[p_img]   # Predicted labels in the region.\n",
    "        p_img_total  = len(p_img)\n",
    "        # the value of predicted labels is the index of trainning region. These indices are the labels\n",
    "        # but these p_img_answer are predicted, may not always be the truth.\n",
    "\n",
    "\n",
    "        #(b)===== region dominate; one value =====\n",
    "        #region label\n",
    "        p_reg_label_dmn[i] = collections.Counter(p_img_label).most_common()[0][0] # one value\n",
    "        # region dominate rate\n",
    "        if(p_reg_label_dmn[i]>=0):\n",
    "            p_reg_dmn_rate[i] = collections.Counter(p_img_label).most_common()[0][1]/p_img_total\n",
    "        else:              # means invalid label\n",
    "            p_reg_dmn_rate[i] = 0\n",
    "\n",
    "\n",
    "        #(c)==== ground truth =====\n",
    "        grd_label                 = test_label_answer[p_img]  #multiple values\n",
    "        grd_reg_answer_dmn[i]     = collections.Counter(grd_label).most_common()[0][0] #one value\n",
    "\n",
    "\n",
    "        #(d)==== establish confusion table=====\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(grd_label==j)[0])\n",
    "\n",
    "\n",
    "        #(e)=== collect dominated images =============\n",
    "        addr2=np.where( (p_img_label==p_reg_label_dmn[i]) & (p_img_label>=0) )[0] # ignore -1 which are non-consistency\n",
    "        #         the labels which  == 7               the labels which >= 0\n",
    "        temp=[]\n",
    "        for k in range(len(addr2)):\n",
    "            temp.append(p_img[addr2[k]])\n",
    "        dmn_img.append(temp)\n",
    "        #=============================================\n",
    "\n",
    "\n",
    "    df1 = pandas.DataFrame({\"other index\":other_regions}) # 1 to 200   other region index\n",
    "    df2 = pandas.DataFrame({\"pred label\":p_reg_label_dmn})      \n",
    "    df4 = pandas.DataFrame({\"truth\":grd_reg_answer_dmn})\n",
    "    df6 = pandas.DataFrame({\"rate\":np.round(p_reg_dmn_rate,2)})\n",
    "    df7 = pandas.DataFrame(dist_table_truth)\n",
    "    entire_table=pandas.concat([df1, df2, df4, df6, df7], axis=1)\n",
    "    print(\"All other regions\")\n",
    "    display(entire_table)\n",
    "\n",
    "\n",
    "\n",
    "    #(2)get regions according to conditions\n",
    "    NN=5 #choose top 5 regions\n",
    "    RATE=0.7\n",
    "    candidate_reg_by_top_NN=[]\n",
    "\n",
    "    # === get candidate regions by the order of dmn label 0 to 9 ====\n",
    "    for i in range(NUM_region):\n",
    "        # (2-1) ==== select region by rate > 0.7 and top 5 ====\n",
    "        index     = np.where(p_reg_label_dmn==i)[0] # index is the index of other_regions(0~183), rather than original entire region index 1 to 200        \n",
    "        working_table = entire_table.iloc[index]\n",
    "        working_table = working_table.sort_values(by=['rate'], ascending=False)\n",
    "        working_table = working_table.loc[working_table['rate'] > RATE]  #rate > 0.7\n",
    "        NUM_region_in_one_class = len(working_table.iloc[:NN])  #top 5\n",
    "               \n",
    "        # (2-2) ==== get candidate regions ====\n",
    "        # get top N records; save only the column 'other_reg', and transfer it to list from DataFrame by \"tolist()\"\n",
    "        candidate_reg_by_top_NN.append(working_table[:NN]['other index'].tolist())\n",
    "         \n",
    "    #(3) add regions and images\n",
    "    for i in range(NUM_region):\n",
    "        added_img=[]\n",
    "        if (len(candidate_reg_by_top_NN[i])>0):\n",
    "            for j in range(len(candidate_reg_by_top_NN[i])):\n",
    "                reg_addr  = np.where( np.array(other_regions)==candidate_reg_by_top_NN[i][j] )[0][0].tolist()\n",
    "                added_img = added_img + dmn_img[reg_addr]\n",
    "            # (3-1) add image\n",
    "            temp=len(merged_region_image[i])\n",
    "            merged_region_image[i] = merged_region_image[i] + added_img\n",
    "            merged_region_image[i] = list(set(merged_region_image[i]))\n",
    "            img_amount=len(merged_region_image[i])-temp\n",
    "            \n",
    "            # (3-2) add region\n",
    "            merged_reg_and_nei[i]  = merged_reg_and_nei[i] + candidate_reg_by_top_NN[i]\n",
    "            \n",
    "            # (3-3) print out\n",
    "            print(\"added label, regions, img amount:\", set(Original_result[added_img]), candidate_reg_by_top_NN[i], img_amount)\n",
    "\n",
    "            \n",
    "    # (4) collect residual images\n",
    "    # This works only for CIFAR10. All images in the MNIST and MNIST-TRAN are clean. No this issue.        \n",
    "    #20240105\n",
    "    if (not MNIST):\n",
    "    #if ((DATASET==2) or (DATASET==4)):\n",
    "        if (len(list(chain.from_iterable(candidate_reg_by_top_NN))) == 0):  #if no extra regions\n",
    "            #20240105\n",
    "            if (True):\n",
    "            #if(DATASET==4):\n",
    "                df = pandas.read_csv(PATH4)\n",
    "                tSNE_table = df.to_numpy()[:,:3]\n",
    "            else:\n",
    "                df = pandas.read_csv(PATH8)\n",
    "                tSNE_table = df.to_numpy()\n",
    "            print(\"tSNE_table\",np.shape(tSNE_table))\n",
    "\n",
    "            working_table=tSNE_table[working_img]\n",
    "            pairwise_dist=squareform(pdist(working_table, 'euclidean'))\n",
    "            print(\"pairwise_dist\",np.shape(pairwise_dist)) #value of data point, rather than image index\n",
    "\n",
    "            TopN=10\n",
    "            M=len(working_img)\n",
    "            nei_table_images  = np.zeros((M,TopN),dtype=int)  #contain top 10 images\n",
    "            nei_table_label   = np.zeros((M,TopN),dtype=int)\n",
    "            working_img_label = np.zeros(M,dtype=int)\n",
    "            for i in range(M):   \n",
    "                # fill up top 10 \n",
    "                addr=np.argsort(pairwise_dist[i])\n",
    "                for j in range(TopN):\n",
    "                    nei_table_images[i][j]=working_img[ addr[j+1] ] #Ignore first one. First one is itself\n",
    "                    nei_table_label[i][j] =Original_result[nei_table_images[i][j]]\n",
    "                # consistent\n",
    "                if (len(set(nei_table_label[i])) == 1): #only get the one which is entire consistent\n",
    "                    working_img_label[i]=nei_table_label[i][0]\n",
    "                else:\n",
    "                    working_img_label[i]=-1\n",
    "\n",
    "            print(\"nei_table_images\",np.shape(nei_table_images))\n",
    "            print(\"working_img_label\",working_img_label)\n",
    "\n",
    "            new_img=[] # just  for monitoring\n",
    "            for i in range(NUM_region):\n",
    "                addr=np.where(working_img_label==i)[0].tolist()\n",
    "                new_img.append(working_img[addr])\n",
    "                merged_region_image[i].extend(working_img[addr])\n",
    "            print(\"add residuals \",len(list(chain.from_iterable(new_img))))\n",
    "            print(\"number of next merged_region_image\", len(list(chain.from_iterable(merged_region_image))))\n",
    "        else:\n",
    "            print(\"Not getting into residuals\")\n",
    "\n",
    "    #save\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/merged_region_image_'+str(ITE+1)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([merged_reg_and_nei, merged_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup region_initials.pickle\n",
    "#### For both single network and integrate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>Spec200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.131136</td>\n",
       "      <td>-11.714952</td>\n",
       "      <td>14.559869</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.653914</td>\n",
       "      <td>-15.913769</td>\n",
       "      <td>8.271565</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.666949</td>\n",
       "      <td>-15.911179</td>\n",
       "      <td>8.316633</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.160013</td>\n",
       "      <td>-8.344077</td>\n",
       "      <td>19.694381</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.430728</td>\n",
       "      <td>-5.483346</td>\n",
       "      <td>17.380125</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1         X2         X3   Class  Label  Spec200\n",
       "0 -6.131136 -11.714952  14.559869  Cherry      2      136\n",
       "1  4.653914 -15.913769   8.271565  Cherry      2      136\n",
       "2  4.666949 -15.911179   8.316633  Cherry      2      136\n",
       "3 -1.160013  -8.344077  19.694381  Cherry      2        8\n",
       "4 -0.430728  -5.483346  17.380125  Cherry      2        8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43217\n",
      "all_region_index\n",
      " [136 136 136   8   8]\n"
     ]
    }
   ],
   "source": [
    "df = pandas.read_csv(PATH4)\n",
    "display(df.head())\n",
    "#all_region_index = df.to_numpy().T[REGION_INDEX_LOC].astype(int)\n",
    "#print(len(all_region_index))\n",
    "all_region_index  = df[REG_COLUMN].to_numpy().astype(int)\n",
    "print(len(all_region_index))\n",
    "print(\"all_region_index\\n\",all_region_index[:5])\n",
    "\n",
    "all_region_image=[]\n",
    "MAX_region=max(all_region_index)\n",
    "for i in range(MAX_region):\n",
    "    addr=list(np.where(all_region_index==i+1)[0])\n",
    "    all_region_image.append(addr)    \n",
    "\n",
    "#save\n",
    "if (SAVE_bool):\n",
    "    with open('./region_initials.pickle', 'wb') as f:\n",
    "        pickle.dump([all_region_index, all_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case= 1\n",
      "mergedseedclasslabels table\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[149,   9,   9],\n",
       "       [188,   2,   2],\n",
       "       [132,   0,   0],\n",
       "       [143,   1,   1],\n",
       "       [ 75,  10,  10],\n",
       "       [169,   3,   3],\n",
       "       [ 57,   8,   8],\n",
       "       [ 52,   1,   1],\n",
       "       [155,   8,   8],\n",
       "       [ 92,   5,   5],\n",
       "       [ 29,   6,   6],\n",
       "       [127,  10,  10],\n",
       "       [152,   2,   2],\n",
       "       [ 40,   4,   4],\n",
       "       [110,   9,   9],\n",
       "       [  3,   5,   5],\n",
       "       [140,   3,   4],\n",
       "       [119,   7,   7],\n",
       "       [161,   0,   0]], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged_region\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[143, 52],\n",
       " [188, 152],\n",
       " [169, 140],\n",
       " [40],\n",
       " [92, 3],\n",
       " [29],\n",
       " [119],\n",
       " [57, 155],\n",
       " [149, 110],\n",
       " [75, 127]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 143\n",
      "7 52\n",
      "total 8\n",
      "\n",
      "1 188\n",
      "12 152\n",
      "total 8\n",
      "\n",
      "5 169\n",
      "16 140\n",
      "total 9\n",
      "\n",
      "13 40\n",
      "total 5\n",
      "\n",
      "9 92\n",
      "15 3\n",
      "total 7\n",
      "\n",
      "10 29\n",
      "total 1\n",
      "\n",
      "17 119\n",
      "total 1\n",
      "\n",
      "6 57\n",
      "8 155\n",
      "total 9\n",
      "\n",
      "0 149\n",
      "14 110\n",
      "total 8\n",
      "\n",
      "4 75\n",
      "11 127\n",
      "total 9\n",
      "\n",
      "\n",
      "merged_reg_and_nei\n",
      "[143, 183, 101, 60, 22, 52, 166, 16]\n",
      "[188, 63, 67, 113, 156, 152, 76, 130]\n",
      "[169, 184, 27, 138, 197, 140, 104, 49, 85]\n",
      "[40, 147, 148, 135, 66]\n",
      "[92, 97, 123, 133, 3, 1, 139]\n",
      "[29]\n",
      "[119]\n",
      "[57, 181, 93, 59, 35, 155, 31, 62, 128]\n",
      "[149, 83, 131, 198, 154, 110, 2, 185]\n",
      "[75, 44, 114, 105, 73, 127, 122, 96, 71]\n",
      "1538 ( 3 ) 1022 ( 7 ) = 2560 \n",
      "869 ( 1 ) 556 ( 12 ) = 1425 \n",
      "748 ( 5 ) 1082 ( 16 ) = 1830 \n",
      "1289 ( 13 ) = 1289 \n",
      "1142 ( 9 ) 619 ( 15 ) = 1761 \n",
      "169 ( 10 ) = 169 \n",
      "151 ( 17 ) = 151 \n",
      "1153 ( 6 ) 727 ( 8 ) = 1880 \n",
      "1123 ( 0 ) 818 ( 14 ) = 1941 \n",
      "1444 ( 4 ) 1130 ( 11 ) = 2574 \n",
      "\n",
      "merged_reg_and_nei_image\n",
      "2560 [28218, 28224, 28249, 28253, 28264] ...\n",
      "1425 [18277, 18345, 18743, 18856, 19106] ...\n",
      "1830 [23266, 23399, 23407, 23536, 23540] ...\n",
      "1289 [18219, 18241, 18256, 18264, 18273] ...\n",
      "1761 [13347, 13368, 13375, 13383, 13394] ...\n",
      "169 [38227, 38274, 38288, 38316, 38317] ...\n",
      "151 [6694, 7108, 7222, 8338, 37272] ...\n",
      "1880 [33299, 33328, 33334, 33348, 33352] ...\n",
      "1941 [16083, 16233, 17237, 18235, 18258] ...\n",
      "2574 [4710, 5468, 6685, 6715, 6716] ...\n",
      "NUM_region 10\n",
      "number of clean images 15580\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 10\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14022 samples, validate on 1558 samples\n",
      "Epoch 1/80\n",
      "14022/14022 [==============================] - 3s 212us/step - loss: 0.3163 - accuracy: 0.9007 - val_loss: 0.0931 - val_accuracy: 0.9647\n",
      "Epoch 2/80\n",
      "14022/14022 [==============================] - 1s 94us/step - loss: 0.0694 - accuracy: 0.9767 - val_loss: 0.0716 - val_accuracy: 0.9750\n",
      "Epoch 3/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 0.0419 - accuracy: 0.9855 - val_loss: 0.0427 - val_accuracy: 0.9833\n",
      "Epoch 4/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 0.0323 - accuracy: 0.9889 - val_loss: 0.0422 - val_accuracy: 0.9872\n",
      "Epoch 5/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 0.0221 - accuracy: 0.9935 - val_loss: 0.1057 - val_accuracy: 0.9666\n",
      "Epoch 6/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.0426 - val_accuracy: 0.9820\n",
      "Epoch 7/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 0.0111 - accuracy: 0.9967 - val_loss: 0.0495 - val_accuracy: 0.9852\n",
      "Epoch 8/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0341 - val_accuracy: 0.9859\n",
      "Epoch 9/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.0381 - val_accuracy: 0.9846\n",
      "Epoch 10/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0669 - val_accuracy: 0.9801\n",
      "Epoch 11/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0320 - val_accuracy: 0.9878\n",
      "Epoch 12/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 3.4394e-04 - accuracy: 1.0000 - val_loss: 0.0272 - val_accuracy: 0.9891\n",
      "Epoch 13/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 1.7397e-04 - accuracy: 1.0000 - val_loss: 0.0268 - val_accuracy: 0.9878\n",
      "Epoch 14/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 1.3848e-04 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 0.9872\n",
      "Epoch 15/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 9.9371e-05 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 0.9878\n",
      "Epoch 16/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 8.1823e-05 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 0.9884\n",
      "Epoch 17/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 6.7375e-05 - accuracy: 1.0000 - val_loss: 0.0281 - val_accuracy: 0.9878\n",
      "Epoch 18/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 5.8961e-05 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 0.9872\n",
      "Epoch 19/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 5.2109e-05 - accuracy: 1.0000 - val_loss: 0.0292 - val_accuracy: 0.9884\n",
      "Epoch 20/80\n",
      "14022/14022 [==============================] - 1s 88us/step - loss: 4.7575e-05 - accuracy: 1.0000 - val_loss: 0.0300 - val_accuracy: 0.9878\n",
      "Epoch 21/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 4.2037e-05 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9884\n",
      "Epoch 22/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 3.8357e-05 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9878\n",
      "Epoch 23/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 3.5458e-05 - accuracy: 1.0000 - val_loss: 0.0315 - val_accuracy: 0.9884\n",
      "Epoch 00023: early stopping\n",
      "[[6.17289558e-14 1.00416287e-07 8.97348329e-09 ... 2.96917211e-08\n",
      "  8.92767993e-09 3.33375971e-09]\n",
      " [2.02088604e-13 5.80914440e-12 1.01846233e-10 ... 1.92276880e-12\n",
      "  1.19392329e-10 9.32773511e-11]\n",
      " [1.26112391e-13 2.70099854e-08 1.66942057e-10 ... 2.72367173e-09\n",
      "  3.41113526e-11 3.82016641e-11]\n",
      " ...\n",
      " [3.76678146e-16 1.46950430e-09 6.74657196e-23 ... 1.67664582e-09\n",
      "  2.81849975e-16 1.16481553e-12]\n",
      " [3.24165805e-09 9.99985933e-01 3.29349287e-12 ... 3.32222885e-06\n",
      "  2.46367620e-07 2.79103740e-09]\n",
      " [4.03928453e-11 7.71945238e-01 8.90533272e-11 ... 6.61886763e-03\n",
      "  1.11975785e-11 1.42910130e-06]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:00:42.112757\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14022 samples, validate on 1558 samples\n",
      "Epoch 1/80\n",
      "14022/14022 [==============================] - 1s 98us/step - loss: 0.3086 - accuracy: 0.8993 - val_loss: 0.1358 - val_accuracy: 0.9531\n",
      "Epoch 2/80\n",
      "14022/14022 [==============================] - 1s 84us/step - loss: 0.0716 - accuracy: 0.9768 - val_loss: 0.0757 - val_accuracy: 0.9756\n",
      "Epoch 3/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0478 - accuracy: 0.9831 - val_loss: 0.0580 - val_accuracy: 0.9827\n",
      "Epoch 4/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0309 - accuracy: 0.9889 - val_loss: 0.0571 - val_accuracy: 0.9833\n",
      "Epoch 5/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0212 - accuracy: 0.9932 - val_loss: 0.0623 - val_accuracy: 0.9795\n",
      "Epoch 6/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0723 - val_accuracy: 0.9769\n",
      "Epoch 7/80\n",
      "14022/14022 [==============================] - 1s 84us/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0890 - val_accuracy: 0.9763\n",
      "Epoch 8/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0094 - accuracy: 0.9971 - val_loss: 0.0522 - val_accuracy: 0.9807\n",
      "Epoch 9/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 0.0598 - val_accuracy: 0.9827\n",
      "Epoch 10/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 0.0600 - val_accuracy: 0.9814\n",
      "Epoch 11/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0051 - accuracy: 0.9987 - val_loss: 0.0643 - val_accuracy: 0.9820\n",
      "Epoch 12/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0203 - accuracy: 0.9941 - val_loss: 0.0701 - val_accuracy: 0.9807\n",
      "Epoch 13/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0780 - val_accuracy: 0.9820\n",
      "Epoch 14/80\n",
      "14022/14022 [==============================] - 1s 84us/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0668 - val_accuracy: 0.9827\n",
      "Epoch 15/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 5.1998e-04 - accuracy: 0.9999 - val_loss: 0.0617 - val_accuracy: 0.9827\n",
      "Epoch 16/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 1.6472e-04 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9840\n",
      "Epoch 17/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 8.8994e-05 - accuracy: 1.0000 - val_loss: 0.0692 - val_accuracy: 0.9840\n",
      "Epoch 18/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 6.9250e-05 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9840\n",
      "Epoch 00018: early stopping\n",
      "[[7.51439189e-10 1.37579335e-07 2.75063576e-05 ... 4.48629116e-08\n",
      "  5.98010786e-07 3.27925881e-10]\n",
      " [8.26786550e-09 1.20934110e-10 1.48051527e-06 ... 1.61220892e-09\n",
      "  2.77617403e-07 5.86577897e-10]\n",
      " [4.07279632e-09 2.33750015e-08 4.73568235e-07 ... 1.02271958e-09\n",
      "  1.14872265e-07 6.07777051e-10]\n",
      " ...\n",
      " [3.36709281e-16 1.31074946e-08 7.16393565e-14 ... 2.87557489e-09\n",
      "  3.11160298e-14 5.18787193e-16]\n",
      " [2.14047624e-09 9.99993205e-01 3.36538903e-08 ... 2.72360801e-08\n",
      "  5.83670271e-07 2.71881961e-10]\n",
      " [4.02875185e-06 7.08224416e-01 4.52218228e-05 ... 3.33704837e-02\n",
      "  8.09415894e-08 7.03641945e-06]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:01:10.062348\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14022 samples, validate on 1558 samples\n",
      "Epoch 1/80\n",
      "14022/14022 [==============================] - 1s 102us/step - loss: 0.3795 - accuracy: 0.8837 - val_loss: 0.0991 - val_accuracy: 0.9660\n",
      "Epoch 2/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0839 - accuracy: 0.9703 - val_loss: 0.0763 - val_accuracy: 0.9769\n",
      "Epoch 3/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0566 - accuracy: 0.9794 - val_loss: 0.0583 - val_accuracy: 0.9807\n",
      "Epoch 4/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0361 - accuracy: 0.9874 - val_loss: 0.0689 - val_accuracy: 0.9756\n",
      "Epoch 5/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0406 - accuracy: 0.9869 - val_loss: 0.0546 - val_accuracy: 0.9820\n",
      "Epoch 6/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0216 - accuracy: 0.9930 - val_loss: 0.0580 - val_accuracy: 0.9827\n",
      "Epoch 7/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0181 - accuracy: 0.9932 - val_loss: 0.0628 - val_accuracy: 0.9801\n",
      "Epoch 8/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0152 - accuracy: 0.9948 - val_loss: 0.0505 - val_accuracy: 0.9859\n",
      "Epoch 9/80\n",
      "14022/14022 [==============================] - 1s 89us/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0556 - val_accuracy: 0.9833\n",
      "Epoch 10/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0487 - val_accuracy: 0.9859\n",
      "Epoch 11/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 0.0036 - accuracy: 0.9992 - val_loss: 0.0519 - val_accuracy: 0.9872\n",
      "Epoch 12/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0752 - val_accuracy: 0.9795\n",
      "Epoch 13/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.0578 - val_accuracy: 0.9833\n",
      "Epoch 14/80\n",
      "14022/14022 [==============================] - 1s 89us/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 0.0511 - val_accuracy: 0.9884\n",
      "Epoch 15/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 7.2825e-04 - accuracy: 0.9999 - val_loss: 0.0533 - val_accuracy: 0.9859\n",
      "Epoch 16/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 2.5621e-04 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 0.9865\n",
      "Epoch 17/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 1.3435e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9859\n",
      "Epoch 18/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 1.1010e-04 - accuracy: 1.0000 - val_loss: 0.0553 - val_accuracy: 0.9865\n",
      "Epoch 19/80\n",
      "14022/14022 [==============================] - 1s 90us/step - loss: 9.1236e-05 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9852\n",
      "Epoch 20/80\n",
      "14022/14022 [==============================] - 1s 91us/step - loss: 7.8366e-05 - accuracy: 1.0000 - val_loss: 0.0569 - val_accuracy: 0.9852\n",
      "Epoch 00020: early stopping\n",
      "[[2.5439596e-14 3.5736846e-08 4.4025335e-04 ... 3.5889877e-07\n",
      "  5.7613207e-08 3.1979691e-10]\n",
      " [7.5437290e-14 1.6001946e-12 6.0910395e-07 ... 3.7647871e-12\n",
      "  3.0491279e-07 3.6850138e-11]\n",
      " [2.8301272e-14 1.3340633e-10 1.8373407e-07 ... 4.4185586e-10\n",
      "  5.6300262e-09 1.3941472e-11]\n",
      " ...\n",
      " [2.0834486e-14 7.8135978e-09 8.2276190e-18 ... 2.1321226e-08\n",
      "  6.6342098e-13 2.8649698e-17]\n",
      " [8.1289052e-08 9.9997699e-01 1.8922856e-10 ... 5.5737801e-06\n",
      "  1.3795213e-05 4.6585442e-09]\n",
      " [1.7223113e-10 5.4914421e-01 9.7587451e-07 ... 4.1539630e-01\n",
      "  3.7847687e-13 1.2186525e-08]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:01:41.843550\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14022 samples, validate on 1558 samples\n",
      "Epoch 1/80\n",
      "14022/14022 [==============================] - 1s 105us/step - loss: 0.2512 - accuracy: 0.9165 - val_loss: 0.1061 - val_accuracy: 0.9634\n",
      "Epoch 2/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0641 - accuracy: 0.9781 - val_loss: 0.1151 - val_accuracy: 0.9615\n",
      "Epoch 3/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0383 - accuracy: 0.9857 - val_loss: 0.0538 - val_accuracy: 0.9827\n",
      "Epoch 4/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0224 - accuracy: 0.9917 - val_loss: 0.0587 - val_accuracy: 0.9807\n",
      "Epoch 5/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0218 - accuracy: 0.9929 - val_loss: 0.0480 - val_accuracy: 0.9833\n",
      "Epoch 6/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0548 - val_accuracy: 0.9827\n",
      "Epoch 7/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0123 - accuracy: 0.9953 - val_loss: 0.0538 - val_accuracy: 0.9807\n",
      "Epoch 8/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0490 - val_accuracy: 0.9852\n",
      "Epoch 9/80\n",
      "14022/14022 [==============================] - 1s 88us/step - loss: 0.0049 - accuracy: 0.9983 - val_loss: 0.0553 - val_accuracy: 0.9852\n",
      "Epoch 10/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0060 - accuracy: 0.9974 - val_loss: 0.0580 - val_accuracy: 0.9820\n",
      "Epoch 11/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0555 - val_accuracy: 0.9852\n",
      "Epoch 12/80\n",
      "14022/14022 [==============================] - 1s 89us/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0611 - val_accuracy: 0.9833\n",
      "Epoch 13/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0861 - val_accuracy: 0.9788\n",
      "Epoch 14/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0107 - accuracy: 0.9966 - val_loss: 0.0880 - val_accuracy: 0.9795\n",
      "Epoch 15/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.0817 - val_accuracy: 0.9782\n",
      "Epoch 00015: early stopping\n",
      "[[3.7977347e-07 1.4638515e-05 1.3599407e-02 ... 7.5744349e-04\n",
      "  1.0352595e-03 2.0446154e-07]\n",
      " [5.3635790e-07 2.5574016e-09 1.8893939e-04 ... 5.9964641e-06\n",
      "  6.9007001e-05 5.4096532e-09]\n",
      " [4.5115871e-07 9.0894480e-07 4.3843870e-04 ... 1.5051961e-05\n",
      "  1.4067895e-04 5.0864206e-09]\n",
      " ...\n",
      " [4.0675734e-09 7.6369452e-06 1.9329165e-07 ... 1.1223946e-03\n",
      "  1.7893699e-07 9.2999364e-10]\n",
      " [2.0590576e-04 9.8923838e-01 1.9277165e-03 ... 1.0062618e-03\n",
      "  2.1406075e-03 4.4702460e-06]\n",
      " [2.2621414e-04 3.9226539e-03 1.5054436e-02 ... 1.1555784e-01\n",
      "  4.4753338e-06 3.1815220e-05]]\n",
      "[4 4 4 ... 5 1 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:02:06.484177\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 14022 samples, validate on 1558 samples\n",
      "Epoch 1/80\n",
      "14022/14022 [==============================] - 1s 103us/step - loss: 0.3046 - accuracy: 0.8995 - val_loss: 0.0855 - val_accuracy: 0.9698\n",
      "Epoch 2/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0655 - accuracy: 0.9781 - val_loss: 0.0761 - val_accuracy: 0.9711\n",
      "Epoch 3/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0480 - accuracy: 0.9837 - val_loss: 0.0554 - val_accuracy: 0.9814\n",
      "Epoch 4/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0303 - accuracy: 0.9895 - val_loss: 0.0429 - val_accuracy: 0.9872\n",
      "Epoch 5/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.0479 - val_accuracy: 0.9833\n",
      "Epoch 6/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0169 - accuracy: 0.9942 - val_loss: 0.0996 - val_accuracy: 0.9718\n",
      "Epoch 7/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 0.0427 - val_accuracy: 0.9872\n",
      "Epoch 8/80\n",
      "14022/14022 [==============================] - 1s 85us/step - loss: 0.0082 - accuracy: 0.9977 - val_loss: 0.0524 - val_accuracy: 0.9852\n",
      "Epoch 9/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0467 - val_accuracy: 0.9878\n",
      "Epoch 10/80\n",
      "14022/14022 [==============================] - 1s 84us/step - loss: 0.0085 - accuracy: 0.9967 - val_loss: 0.0601 - val_accuracy: 0.9827\n",
      "Epoch 11/80\n",
      "14022/14022 [==============================] - 1s 87us/step - loss: 0.0091 - accuracy: 0.9974 - val_loss: 0.0956 - val_accuracy: 0.9763\n",
      "Epoch 12/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0087 - accuracy: 0.9970 - val_loss: 0.0666 - val_accuracy: 0.9795\n",
      "Epoch 13/80\n",
      "14022/14022 [==============================] - 1s 84us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0537 - val_accuracy: 0.9852\n",
      "Epoch 14/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0665 - val_accuracy: 0.9827\n",
      "Epoch 15/80\n",
      "14022/14022 [==============================] - 1s 88us/step - loss: 2.6321e-04 - accuracy: 1.0000 - val_loss: 0.0555 - val_accuracy: 0.9878\n",
      "Epoch 16/80\n",
      "14022/14022 [==============================] - 1s 86us/step - loss: 9.7811e-05 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9878\n",
      "Epoch 17/80\n",
      "14022/14022 [==============================] - 1s 84us/step - loss: 7.2824e-05 - accuracy: 1.0000 - val_loss: 0.0560 - val_accuracy: 0.9884\n",
      "Epoch 00017: early stopping\n",
      "[[8.27977335e-13 5.25648680e-08 1.60496757e-06 ... 7.80452865e-07\n",
      "  9.67754943e-08 1.17226122e-08]\n",
      " [2.94100265e-11 1.44964787e-11 1.81660127e-08 ... 2.57358940e-10\n",
      "  1.44567750e-07 5.55287538e-10]\n",
      " [5.12529220e-11 2.53360977e-08 5.36210720e-08 ... 2.57036366e-08\n",
      "  1.03843824e-07 9.47282519e-09]\n",
      " ...\n",
      " [4.94663582e-14 1.54333293e-08 1.56710529e-15 ... 2.69315725e-09\n",
      "  2.94895971e-13 1.67484950e-13]\n",
      " [1.75041626e-09 9.99940753e-01 1.24282529e-09 ... 3.54267911e-07\n",
      "  4.69938914e-06 7.99036659e-09]\n",
      " [8.35889313e-09 8.31770360e-01 1.94917993e-05 ... 7.72161782e-02\n",
      "  4.89450598e-11 1.29172179e-06]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:02:33.452916\n",
      "num of merged_region_image 0 2560\n",
      "num of merged_region_image 1 1425\n",
      "num of merged_region_image 2 1830\n",
      "num of merged_region_image 3 1289\n",
      "num of merged_region_image 4 1761\n",
      "num of merged_region_image 5 169\n",
      "num of merged_region_image 6 151\n",
      "num of merged_region_image 7 1880\n",
      "num of merged_region_image 8 1941\n",
      "num of merged_region_image 9 2574\n",
      "Counter({-1: 27637, 9: 2574, 0: 2560, 8: 1941, 7: 1880, 2: 1830, 4: 1761, 1: 1425, 3: 1289, 5: 169, 6: 151})\n",
      "===========  ITE = 0   ===========\n",
      "used_img 15580 15580\n",
      "working_img(=other images=unclean images) 27637 27637\n",
      "merged regions 65 65\n",
      "other_regions 135 135\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>306</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48</td>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>194</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>195</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>196</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>199</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>43</td>\n",
       "      <td>604</td>\n",
       "      <td>214</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>184</td>\n",
       "      <td>16</td>\n",
       "      <td>242</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     other index  pred label  truth  rate    0    1    2    3   4    5   6  \\\n",
       "0              4          -1      0  0.00  115    0    0    0   1    0   2   \n",
       "1              5           3      7  0.56    0    0    0    3   0    0   2   \n",
       "2              6           8      0  1.00  306    0    1    4   0    2   4   \n",
       "3              7           9      4  0.98    0    0    0    0  65    0   0   \n",
       "4              8           7      1  0.48    1  266    0    0   0    3   0   \n",
       "..           ...         ...    ...   ...  ...  ...  ...  ...  ..  ...  ..   \n",
       "130          194           2      3  0.60    0    0    0  348   0    0   0   \n",
       "131          195           2      3  0.97    0    0    0  192   3    0   0   \n",
       "132          196          -1      7  0.00    3    0    0    0   1    0   0   \n",
       "133          199           2      3  0.98    0    0    0   43   0    0   0   \n",
       "134          200           7      1  0.28   43  604  214  170   4  184  16   \n",
       "\n",
       "       7    8  9  \n",
       "0      0    1  0  \n",
       "1    183    0  0  \n",
       "2      1    2  0  \n",
       "3      0    0  0  \n",
       "4      0    0  0  \n",
       "..   ...  ... ..  \n",
       "130    0    0  0  \n",
       "131    0    2  0  \n",
       "132  340    0  0  \n",
       "133    0    0  0  \n",
       "134  242  160  0  \n",
       "\n",
       "[135 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [145, 182, 162, 178] 1056\n",
      "added label, regions, img amount: {1} [94, 126, 84, 179, 192] 522\n",
      "added label, regions, img amount: {2} [58, 199, 195, 13, 77] 758\n",
      "added label, regions, img amount: {3} [121] 398\n",
      "added label, regions, img amount: {4} [12, 55, 109, 88, 21] 1166\n",
      "added label, regions, img amount: {5} [90] 68\n",
      "added label, regions, img amount: {7} [102, 187, 125, 19, 23] 547\n",
      "added label, regions, img amount: {8} [6, 30, 46, 108, 164] 1185\n",
      "added label, regions, img amount: {9} [65, 7, 20] 218\n",
      "Not getting into residuals\n",
      "NUM_region 10\n",
      "number of clean images 21498\n",
      "n, p1, p2 0 0 0\n",
      "NUM_CLASSES 10\n",
      "current_train_label:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19348 samples, validate on 2150 samples\n",
      "Epoch 1/80\n",
      "19348/19348 [==============================] - 2s 122us/step - loss: 0.2981 - accuracy: 0.9066 - val_loss: 0.0635 - val_accuracy: 0.9809\n",
      "Epoch 2/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0554 - accuracy: 0.9817 - val_loss: 0.0650 - val_accuracy: 0.9791\n",
      "Epoch 3/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0362 - accuracy: 0.9881 - val_loss: 0.0394 - val_accuracy: 0.9860\n",
      "Epoch 4/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0271 - accuracy: 0.9917 - val_loss: 0.0400 - val_accuracy: 0.9847\n",
      "Epoch 5/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0196 - accuracy: 0.9932 - val_loss: 0.0547 - val_accuracy: 0.9814\n",
      "Epoch 6/80\n",
      "19348/19348 [==============================] - 2s 89us/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.0358 - val_accuracy: 0.9865\n",
      "Epoch 7/80\n",
      "19348/19348 [==============================] - 2s 91us/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 0.0408 - val_accuracy: 0.9856\n",
      "Epoch 8/80\n",
      "19348/19348 [==============================] - 2s 90us/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0481 - val_accuracy: 0.9856\n",
      "Epoch 9/80\n",
      "19348/19348 [==============================] - 2s 91us/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.0504 - val_accuracy: 0.9847\n",
      "Epoch 10/80\n",
      "19348/19348 [==============================] - 2s 91us/step - loss: 0.0049 - accuracy: 0.9987 - val_loss: 0.0598 - val_accuracy: 0.9847\n",
      "Epoch 11/80\n",
      "19348/19348 [==============================] - 2s 91us/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0371 - val_accuracy: 0.9898\n",
      "Epoch 12/80\n",
      "19348/19348 [==============================] - 2s 90us/step - loss: 0.0040 - accuracy: 0.9986 - val_loss: 0.0479 - val_accuracy: 0.9847\n",
      "Epoch 13/80\n",
      "19348/19348 [==============================] - 2s 90us/step - loss: 0.0095 - accuracy: 0.9972 - val_loss: 0.0529 - val_accuracy: 0.9847\n",
      "Epoch 14/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0076 - accuracy: 0.9974 - val_loss: 0.0603 - val_accuracy: 0.9837\n",
      "Epoch 15/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.0478 - val_accuracy: 0.9851\n",
      "Epoch 16/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0017 - accuracy: 0.9995 - val_loss: 0.0420 - val_accuracy: 0.9879\n",
      "Epoch 00016: early stopping\n",
      "[[1.32734046e-10 2.38289502e-07 5.20488029e-05 ... 5.80979860e-04\n",
      "  1.41553770e-04 1.28507083e-09]\n",
      " [3.86253486e-12 1.49668548e-12 1.78663209e-10 ... 2.21444445e-08\n",
      "  7.75550143e-06 1.49983845e-11]\n",
      " [2.35720089e-11 7.99054156e-10 4.41317483e-10 ... 2.35676708e-07\n",
      "  2.21460618e-06 3.31687074e-11]\n",
      " ...\n",
      " [1.32913944e-13 7.70054354e-10 1.03379934e-16 ... 8.69725092e-09\n",
      "  5.18220076e-14 3.73864148e-14]\n",
      " [9.33091648e-09 9.99954700e-01 2.88847790e-09 ... 4.32237102e-05\n",
      "  8.08126401e-08 4.63942040e-09]\n",
      " [9.22266541e-10 9.98156369e-01 8.38999634e-11 ... 1.47376675e-03\n",
      "  3.23354760e-10 2.74118062e-09]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  0\n",
      "Computing Time:  0:00:35.077979\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19348 samples, validate on 2150 samples\n",
      "Epoch 1/80\n",
      "19348/19348 [==============================] - 2s 102us/step - loss: 0.2755 - accuracy: 0.9149 - val_loss: 0.0667 - val_accuracy: 0.9777\n",
      "Epoch 2/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0595 - accuracy: 0.9798 - val_loss: 0.0533 - val_accuracy: 0.9800\n",
      "Epoch 3/80\n",
      "19348/19348 [==============================] - 2s 93us/step - loss: 0.0393 - accuracy: 0.9875 - val_loss: 0.0518 - val_accuracy: 0.9847\n",
      "Epoch 4/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0253 - accuracy: 0.9918 - val_loss: 0.0496 - val_accuracy: 0.9842\n",
      "Epoch 5/80\n",
      "19348/19348 [==============================] - 2s 90us/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.1245 - val_accuracy: 0.9600\n",
      "Epoch 6/80\n",
      "19348/19348 [==============================] - 2s 93us/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.0710 - val_accuracy: 0.9777\n",
      "Epoch 7/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0142 - accuracy: 0.9963 - val_loss: 0.0322 - val_accuracy: 0.9898\n",
      "Epoch 8/80\n",
      "19348/19348 [==============================] - 2s 93us/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.0523 - val_accuracy: 0.9851\n",
      "Epoch 9/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0071 - accuracy: 0.9979 - val_loss: 0.0387 - val_accuracy: 0.9879\n",
      "Epoch 10/80\n",
      "19348/19348 [==============================] - 2s 92us/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0403 - val_accuracy: 0.9893\n",
      "Epoch 11/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0413 - val_accuracy: 0.9898\n",
      "Epoch 12/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 3.9702e-04 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9888\n",
      "Epoch 13/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 4.9119e-04 - accuracy: 1.0000 - val_loss: 0.0369 - val_accuracy: 0.9907\n",
      "Epoch 14/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 4.4714e-04 - accuracy: 0.9999 - val_loss: 0.0405 - val_accuracy: 0.9902\n",
      "Epoch 15/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 1.0757e-04 - accuracy: 1.0000 - val_loss: 0.0397 - val_accuracy: 0.9898\n",
      "Epoch 16/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 7.6837e-05 - accuracy: 1.0000 - val_loss: 0.0398 - val_accuracy: 0.9912\n",
      "Epoch 17/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 5.6915e-05 - accuracy: 1.0000 - val_loss: 0.0405 - val_accuracy: 0.9907\n",
      "Epoch 00017: early stopping\n",
      "[[2.2201213e-14 1.5417204e-08 1.4064425e-05 ... 7.4659429e-06\n",
      "  7.2613443e-07 3.4990248e-07]\n",
      " [1.8248993e-14 1.8608563e-11 2.1682030e-08 ... 1.1160857e-10\n",
      "  3.1570089e-09 3.5838564e-08]\n",
      " [6.6918500e-15 3.5605663e-09 2.4060081e-09 ... 1.1366434e-09\n",
      "  1.7846943e-10 5.6586778e-08]\n",
      " ...\n",
      " [4.0497890e-18 3.4311026e-11 4.5662188e-13 ... 8.3900845e-11\n",
      "  4.7275721e-18 4.8832775e-20]\n",
      " [2.3904774e-11 9.9999869e-01 3.7059994e-10 ... 7.4199171e-08\n",
      "  3.9839657e-10 2.4283661e-12]\n",
      " [3.1727728e-07 1.8066767e-01 1.8270039e-07 ... 3.1412598e-02\n",
      "  2.7057761e-13 2.4289679e-07]]\n",
      "[4 4 4 ... 5 1 4]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  1\n",
      "Computing Time:  0:01:11.109488\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19348 samples, validate on 2150 samples\n",
      "Epoch 1/80\n",
      "19348/19348 [==============================] - 2s 100us/step - loss: 0.2265 - accuracy: 0.9256 - val_loss: 0.0655 - val_accuracy: 0.9758\n",
      "Epoch 2/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0530 - accuracy: 0.9824 - val_loss: 0.0599 - val_accuracy: 0.9800\n",
      "Epoch 3/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0311 - accuracy: 0.9895 - val_loss: 0.0440 - val_accuracy: 0.9865\n",
      "Epoch 4/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0198 - accuracy: 0.9929 - val_loss: 0.0535 - val_accuracy: 0.9856\n",
      "Epoch 5/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0139 - accuracy: 0.9950 - val_loss: 0.0412 - val_accuracy: 0.9847\n",
      "Epoch 6/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0095 - accuracy: 0.9969 - val_loss: 0.0322 - val_accuracy: 0.9884\n",
      "Epoch 7/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0427 - val_accuracy: 0.9842\n",
      "Epoch 8/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0079 - accuracy: 0.9973 - val_loss: 0.0320 - val_accuracy: 0.9879\n",
      "Epoch 9/80\n",
      "19348/19348 [==============================] - 2s 85us/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0309 - val_accuracy: 0.9888\n",
      "Epoch 10/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0348 - val_accuracy: 0.9893\n",
      "Epoch 11/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0488 - val_accuracy: 0.9856\n",
      "Epoch 12/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0064 - accuracy: 0.9980 - val_loss: 0.0339 - val_accuracy: 0.9888\n",
      "Epoch 13/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0363 - val_accuracy: 0.9879\n",
      "Epoch 14/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 3.4873e-04 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 0.9912\n",
      "Epoch 15/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 7.3982e-05 - accuracy: 1.0000 - val_loss: 0.0317 - val_accuracy: 0.9912\n",
      "Epoch 16/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 4.5567e-05 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 0.9912\n",
      "Epoch 17/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 3.8133e-05 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 0.9912\n",
      "Epoch 18/80\n",
      "19348/19348 [==============================] - 2s 85us/step - loss: 2.7804e-05 - accuracy: 1.0000 - val_loss: 0.0321 - val_accuracy: 0.9916\n",
      "Epoch 19/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 2.3988e-05 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9902\n",
      "Epoch 20/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 2.0627e-05 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 0.9907\n",
      "Epoch 21/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 1.7656e-05 - accuracy: 1.0000 - val_loss: 0.0329 - val_accuracy: 0.9912\n",
      "Epoch 22/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 1.5377e-05 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 0.9902\n",
      "Epoch 23/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 1.3781e-05 - accuracy: 1.0000 - val_loss: 0.0338 - val_accuracy: 0.9907\n",
      "Epoch 24/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 1.2244e-05 - accuracy: 1.0000 - val_loss: 0.0337 - val_accuracy: 0.9902\n",
      "Epoch 00024: early stopping\n",
      "[[3.9708877e-14 8.0569391e-11 9.6923304e-07 ... 2.7682763e-09\n",
      "  8.5881274e-10 1.8596161e-11]\n",
      " [1.2858881e-15 5.6397849e-17 3.6622292e-09 ... 1.0008749e-13\n",
      "  6.1870738e-11 2.0360585e-11]\n",
      " [1.1695115e-14 4.4783098e-13 3.0172888e-11 ... 7.5975830e-11\n",
      "  1.8676583e-11 1.8500329e-12]\n",
      " ...\n",
      " [4.5902204e-18 4.1748452e-11 2.8575219e-20 ... 1.8847403e-13\n",
      "  7.7974904e-16 6.5281313e-24]\n",
      " [2.1818255e-13 1.0000000e+00 8.7018249e-14 ... 4.0663620e-10\n",
      "  9.1444949e-11 2.9017314e-15]\n",
      " [5.3581210e-14 9.9424040e-01 1.4358063e-10 ... 1.8217102e-05\n",
      "  2.2461325e-17 1.3057970e-09]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  2\n",
      "Computing Time:  0:01:58.609250\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19348 samples, validate on 2150 samples\n",
      "Epoch 1/80\n",
      "19348/19348 [==============================] - 2s 99us/step - loss: 0.2827 - accuracy: 0.9119 - val_loss: 0.0836 - val_accuracy: 0.9707\n",
      "Epoch 2/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0546 - accuracy: 0.9815 - val_loss: 0.0569 - val_accuracy: 0.9809\n",
      "Epoch 3/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0368 - accuracy: 0.9875 - val_loss: 0.0817 - val_accuracy: 0.9726\n",
      "Epoch 4/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0275 - accuracy: 0.9907 - val_loss: 0.0445 - val_accuracy: 0.9847\n",
      "Epoch 5/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0176 - accuracy: 0.9936 - val_loss: 0.0370 - val_accuracy: 0.9865\n",
      "Epoch 6/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 0.0434 - val_accuracy: 0.9870\n",
      "Epoch 7/80\n",
      "19348/19348 [==============================] - 2s 89us/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0348 - val_accuracy: 0.9874\n",
      "Epoch 8/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 0.0454 - val_accuracy: 0.9842\n",
      "Epoch 9/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0514 - val_accuracy: 0.9833\n",
      "Epoch 10/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0264 - val_accuracy: 0.9935\n",
      "Epoch 11/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0295 - val_accuracy: 0.9926\n",
      "Epoch 12/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 9.7705e-04 - accuracy: 0.9998 - val_loss: 0.0899 - val_accuracy: 0.9767\n",
      "Epoch 13/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0045 - accuracy: 0.9980 - val_loss: 0.0632 - val_accuracy: 0.9828\n",
      "Epoch 14/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0176 - accuracy: 0.9946 - val_loss: 0.0346 - val_accuracy: 0.9879\n",
      "Epoch 15/80\n",
      "19348/19348 [==============================] - 2s 89us/step - loss: 0.0087 - accuracy: 0.9969 - val_loss: 0.0417 - val_accuracy: 0.9842\n",
      "Epoch 16/80\n",
      "19348/19348 [==============================] - 2s 89us/step - loss: 0.0033 - accuracy: 0.9990 - val_loss: 0.0288 - val_accuracy: 0.9902\n",
      "Epoch 17/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 3.3328e-04 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 0.9907\n",
      "Epoch 18/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 7.0992e-05 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 0.9912\n",
      "Epoch 19/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 4.6808e-05 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 0.9912\n",
      "Epoch 20/80\n",
      "19348/19348 [==============================] - 2s 89us/step - loss: 3.5673e-05 - accuracy: 1.0000 - val_loss: 0.0301 - val_accuracy: 0.9916\n",
      "Epoch 00020: early stopping\n",
      "[[7.7878528e-16 2.5940641e-10 6.6426509e-07 ... 1.6368563e-08\n",
      "  6.6774604e-08 3.0754774e-11]\n",
      " [1.4398902e-15 2.5123029e-15 1.8040561e-09 ... 4.1206242e-14\n",
      "  2.3726796e-10 3.6767918e-14]\n",
      " [1.0193110e-15 4.3058226e-12 4.3362056e-10 ... 3.4911654e-13\n",
      "  4.0164050e-10 4.6660500e-13]\n",
      " ...\n",
      " [6.4324390e-17 3.2913854e-12 8.1303230e-14 ... 2.8966263e-12\n",
      "  6.9003294e-13 2.0136810e-19]\n",
      " [6.4686291e-11 9.9999976e-01 2.7563672e-08 ... 8.6247285e-09\n",
      "  1.8174397e-08 5.4793908e-12]\n",
      " [8.6668261e-10 9.9359858e-01 1.0143013e-08 ... 7.4868323e-04\n",
      "  4.4009592e-12 1.8760470e-07]]\n",
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  3\n",
      "Computing Time:  0:02:39.015481\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 1000, 6)           36        \n",
      "_________________________________________________________________\n",
      "average_pooling1d_1 (Average (None, 500, 6)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 496, 16)           496       \n",
      "_________________________________________________________________\n",
      "average_pooling1d_2 (Average (None, 248, 16)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 244, 120)          9720      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 29280)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                2459604   \n",
      "_________________________________________________________________\n",
      "preds (Dense)                (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 2,470,706\n",
      "Trainable params: 2,470,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19348 samples, validate on 2150 samples\n",
      "Epoch 1/80\n",
      "19348/19348 [==============================] - 2s 97us/step - loss: 0.2353 - accuracy: 0.9262 - val_loss: 0.0649 - val_accuracy: 0.9772\n",
      "Epoch 2/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0519 - accuracy: 0.9825 - val_loss: 0.0373 - val_accuracy: 0.9837\n",
      "Epoch 3/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.0330 - val_accuracy: 0.9874\n",
      "Epoch 4/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0390 - val_accuracy: 0.9879\n",
      "Epoch 5/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.0859 - val_accuracy: 0.9721\n",
      "Epoch 6/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0133 - accuracy: 0.9955 - val_loss: 0.0378 - val_accuracy: 0.9888\n",
      "Epoch 7/80\n",
      "19348/19348 [==============================] - 2s 88us/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0307 - val_accuracy: 0.9898\n",
      "Epoch 8/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0329 - val_accuracy: 0.9865\n",
      "Epoch 9/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 0.0466 - val_accuracy: 0.9851\n",
      "Epoch 10/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.0433 - val_accuracy: 0.9860\n",
      "Epoch 11/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.0331 - val_accuracy: 0.9874\n",
      "Epoch 12/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.0311 - val_accuracy: 0.9888\n",
      "Epoch 13/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.0364 - val_accuracy: 0.9888\n",
      "Epoch 14/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0602 - val_accuracy: 0.9860\n",
      "Epoch 15/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0805 - val_accuracy: 0.9809\n",
      "Epoch 16/80\n",
      "19348/19348 [==============================] - 2s 87us/step - loss: 0.0099 - accuracy: 0.9968 - val_loss: 0.0473 - val_accuracy: 0.9833\n",
      "Epoch 17/80\n",
      "19348/19348 [==============================] - 2s 86us/step - loss: 0.0049 - accuracy: 0.9981 - val_loss: 0.0594 - val_accuracy: 0.9847\n",
      "Epoch 00017: early stopping\n",
      "[[1.69267953e-12 1.79294517e-08 5.43475337e-02 ... 3.80319221e-07\n",
      "  4.21857794e-06 4.20839363e-09]\n",
      " [5.88299374e-12 6.75063283e-10 9.18365549e-04 ... 2.82979649e-08\n",
      "  5.00835824e-07 8.28559055e-10]\n",
      " [6.59275246e-10 2.55538748e-06 1.29005581e-03 ... 2.63023071e-06\n",
      "  1.48971521e-05 1.57452440e-09]\n",
      " ...\n",
      " [5.65210812e-12 5.05545206e-09 4.21636365e-10 ... 2.83136581e-09\n",
      "  2.36211242e-11 1.56102468e-16]\n",
      " [1.02070616e-10 9.99999642e-01 1.14700305e-09 ... 1.41008449e-09\n",
      "  3.29134693e-08 2.86751829e-12]\n",
      " [2.29175012e-12 9.99547184e-01 9.95756000e-06 ... 9.63085131e-06\n",
      "  3.55230428e-11 1.89885732e-10]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 4 ... 5 1 1]\n",
      "<class 'numpy.ndarray'>\n",
      "One CNN, r:  4\n",
      "Computing Time:  0:03:14.215626\n",
      "num of merged_region_image 0 3616\n",
      "num of merged_region_image 1 1947\n",
      "num of merged_region_image 2 2588\n",
      "num of merged_region_image 3 1687\n",
      "num of merged_region_image 4 2927\n",
      "num of merged_region_image 5 237\n",
      "num of merged_region_image 6 151\n",
      "num of merged_region_image 7 2427\n",
      "num of merged_region_image 8 3126\n",
      "num of merged_region_image 9 2792\n",
      "Counter({-1: 21719, 0: 3616, 8: 3126, 4: 2927, 9: 2792, 2: 2588, 7: 2427, 1: 1947, 3: 1687, 5: 237, 6: 151})\n",
      "===========  ITE = 1   ===========\n",
      "used_img 21498 21498\n",
      "working_img(=other images=unclean images) 21719 21719\n",
      "merged regions 99 99\n",
      "other_regions 101 101\n",
      "All other regions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>other index</th>\n",
       "      <th>pred label</th>\n",
       "      <th>truth</th>\n",
       "      <th>rate</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>213</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>191</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>193</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.89</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>194</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>348</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>196</td>\n",
       "      <td>-1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>200</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.27</td>\n",
       "      <td>43</td>\n",
       "      <td>604</td>\n",
       "      <td>214</td>\n",
       "      <td>170</td>\n",
       "      <td>4</td>\n",
       "      <td>184</td>\n",
       "      <td>16</td>\n",
       "      <td>242</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     other index  pred label  truth  rate    0    1    2    3    4    5   6  \\\n",
       "0              4          -1      0  0.00  115    0    0    0    1    0   2   \n",
       "1              5          -1      7  0.00    0    0    0    3    0    0   2   \n",
       "2              8          -1      1  0.00    1  266    0    0    0    3   0   \n",
       "3              9          -1      1  0.00    3  250    0    0    0    2   0   \n",
       "4             10          -1      4  0.00    5    1    0    0  213    2   0   \n",
       "..           ...         ...    ...   ...  ...  ...  ...  ...  ...  ...  ..   \n",
       "96           191          -1      7  0.00    2    0    0    0    0    0   0   \n",
       "97           193           2      3  0.89    2    0    0  143    0    0   2   \n",
       "98           194           2      3  0.64    0    0    0  348    0    0   0   \n",
       "99           196          -1      7  0.00    3    0    0    0    1    0   0   \n",
       "100          200           7      1  0.27   43  604  214  170    4  184  16   \n",
       "\n",
       "       7    8  9  \n",
       "0      0    1  0  \n",
       "1    183    0  0  \n",
       "2      0    0  0  \n",
       "3      0    0  0  \n",
       "4      0    5  0  \n",
       "..   ...  ... ..  \n",
       "96   197    0  0  \n",
       "97     0    2  0  \n",
       "98     0    0  0  \n",
       "99   340    0  0  \n",
       "100  242  160  0  \n",
       "\n",
       "[101 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added label, regions, img amount: {0} [132, 45] 364\n",
      "added label, regions, img amount: {1} [91, 142, 33, 47, 34] 900\n",
      "added label, regions, img amount: {2} [186, 103, 180, 193, 129] 1003\n",
      "added label, regions, img amount: {4} [80, 190, 112, 118] 518\n",
      "added label, regions, img amount: {7} [41] 67\n",
      "added label, regions, img amount: {8} [74, 26] 213\n",
      "Not getting into residuals\n"
     ]
    }
   ],
   "source": [
    "for case_i in range(NUM_CASE):\n",
    "\n",
    "    #===== create folder case1, case2, case3...\n",
    "    print(\"case=\",case_i+1)\n",
    "    newpath = './case' + str(case_i+1)\n",
    "    if (not INTE_bool):\n",
    "        if not os.path.exists(newpath):   #No necessary in Integration\n",
    "            os.makedirs(newpath)\n",
    "    \n",
    "    #==== open csv 1\n",
    "    csv_path1 = newpath+'/' + 'accu_history.csv'\n",
    "    with open(csv_path1, 'a', newline='') as f:\n",
    "        csv_file = csv.writer(f)\n",
    "        csv_file.writerow(['ITE', 'correct', 'denominator', 'accu', 'description'])\n",
    "\n",
    "# 1.\n",
    "    if (not INTE_bool):\n",
    "        create_image_0(PATH6, case_i)   #No necessary in Integration\n",
    "\n",
    "\n",
    "    for ITE in range(ITE_START, ITE_END):\n",
    "# 2. CNN\n",
    "        CNN_part(PATH5,ITE)\n",
    "\n",
    "# 3. statistic\n",
    "        statistic(PATH5,ITE)\n",
    "\n",
    "# 4. merged_and_expand \n",
    "        merged_and_expand(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
