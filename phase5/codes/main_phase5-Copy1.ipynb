{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; Feb, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 5 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@stat.sinica.edu.tw </p>\n",
    "### <p> Institute of Statistical Science, Academia Sinica, Taipei, Taiwan.</p>  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import random \n",
    "import time\n",
    "import datetime\n",
    "from itertools import chain\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "MNIST     = False\n",
    "NUM_CASE  = 1\n",
    "INTE_bool = False  #True: Integrate two networks VGG+ResNet    False: single network\n",
    "SAVE_bool = True\n",
    "ITE_FROM  = 5 # This setting is ONLY for Integration\n",
    "REG_COLUMN = \"Spec200\"\n",
    "RAW_2D_DATA = False\n",
    "\n",
    "\n",
    "PATH4='../../phase3/data/ResNet18_PlantDisease_45K_Spec200.csv'\n",
    "PATH5='../../phase3/data/embedded_data.pickle'\n",
    "PATH6='../data/PlantDisease_ResNet_K200_mergedseedclasslabels_version2.txt'\n",
    "PATH7='../../phase3/data/region_for_phase5.pickle'\n",
    "\n",
    "#=================================================================\n",
    "# 20240319\n",
    "if RAW_2D_DATA: # 2D\n",
    "    from CNN_Modules import ME_CNN\n",
    "else: # 1D\n",
    "    from CNN_Modules_1D import ME_CNN\n",
    "    \n",
    "\n",
    "if (INTE_bool):\n",
    "    ITE_START=ITE_FROM\n",
    "    ITE_END=ITE_FROM+4\n",
    "else:\n",
    "    ITE_START=0\n",
    "    ITE_END=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_csv(x, path):\n",
    "    with open(path,'a+', newline='') as f:\n",
    "        csv_file = csv.writer(f)#   = f.write()\n",
    "        csv_file.writerow(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only for single network. No necessary in Integrated networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not INTE_bool):\n",
    "    def create_image_0(PATH6, case_i):\n",
    "        # ===================\n",
    "        #\n",
    "        #  prepare  merged_region_image_0\n",
    "        #\n",
    "        #====================\n",
    "        # (A)\n",
    "        #get \"(1)merged_region\"      only seed regions, no neighboring regions\n",
    "        df = pandas.read_csv(PATH6, delim_whitespace=' ', header=0,  index_col=None)\n",
    "        table = df.to_numpy()\n",
    "        print(\"mergedseedclasslabels table\")\n",
    "        display(table)\n",
    "\n",
    "        merged_region=[]\n",
    "        for i in range(min(table.T[case_i+1]), max(table.T[case_i+1])+1):  #18 ---merge to --> 10\n",
    "            addr=np.where(table.T[case_i+1]==i)[0] # 2nd column equal to 0(min),1,2,3...10(max); DO NOT consider 3rd column, which is hidden\n",
    "            if(len(addr) and i>0): #if not empty and i=0 is the invalid seed region.\n",
    "                merged_region.append(table[addr][:,0].tolist())\n",
    "        print(\"merged_region\")\n",
    "        display(merged_region)\n",
    "\n",
    "\n",
    "        # (B)\n",
    "        #get \"merged_reg_and_nei\"\n",
    "        #get \"merged_reg_and_nei_image\"\n",
    "        #generate \"merged_region_image_0.pickle\"\n",
    "\n",
    "        # (B_a)=== without neighbors ====\n",
    "        #if ((DATASET == 2) or (DATASET == 4)): \n",
    "        ##20240105\n",
    "        if (not True): \n",
    "            # ==== collect regions. No neighbors, just use merged regions ====\n",
    "            merged_reg_and_nei=merged_region.copy()\n",
    "\n",
    "            # ==== collect images ====\n",
    "            img_temp=[]\n",
    "            for i in range(len(merged_region)):\n",
    "                addr=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    temp=np.where(all_region_index==merged_region[i][j])[0].tolist()   #tolist(): convert temp into list\n",
    "                    addr=addr+temp\n",
    "                    print(len(temp),end=' ')\n",
    "                img_temp.append(addr)\n",
    "                print(\"=\",len(img_temp[i]))\n",
    "            merged_reg_and_nei_image = img_temp.copy()\n",
    "\n",
    "\n",
    "        # (B_b)=== with neighbors ==== \n",
    "        else: \n",
    "            with open(PATH7, 'rb') as f:\n",
    "                pre_region, pre_reg_nei, pre_region_image_pure, pre_region_image= pickle.load(f)\n",
    "            #    1reg         2reg+nei        1's img            2's img\n",
    "\n",
    "            # ==== collect regions with neighbors====\n",
    "            # remove duplicate  -->  https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them\n",
    "            merged_reg_and_nei=[]\n",
    "            NUM_region=len(merged_region)\n",
    "            for i in range(NUM_region):\n",
    "                temp=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0] \n",
    "                    temp=temp+pre_reg_nei[idx]\n",
    "                    print(idx,pre_region[idx])\n",
    "                merged_reg_and_nei.append(temp)\n",
    "\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(merged_reg_and_nei[i]) != len(set(merged_reg_and_nei[i]))):\n",
    "                    a=merged_reg_and_nei[i].copy()\n",
    "\n",
    "                    # find the duplicate.\n",
    "                    seen = set()\n",
    "                    dupli                 = [x for x in a if (x in seen or seen.add(x))]\n",
    "                    print(\"***duplicates:\",dupli)\n",
    "\n",
    "                    # keep fisrt one, remove succeeding duplicates.\n",
    "                    seen = set()\n",
    "                    merged_reg_and_nei[i] = [x for x in a if not (x in seen or seen.add(x))]  # a is the data to process; x is a working varialbe\n",
    "                    print(\"unique:\",merged_reg_and_nei[i])\n",
    "\n",
    "                print(\"total\",len(merged_reg_and_nei[i]),end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei\")\n",
    "            for i in range(len(merged_reg_and_nei)):\n",
    "                print(merged_reg_and_nei[i])\n",
    "\n",
    "\n",
    "            # Collect images\n",
    "            merged_reg_and_nei_image=[]\n",
    "            for i in range(NUM_region):\n",
    "                #search and add\n",
    "                img=[]\n",
    "                for j in range(len(merged_region[i])):\n",
    "                    idx=np.where(pre_region==merged_region[i][j])[0][0]\n",
    "                    print(len(pre_region_image[idx]),\"(\",idx,\")\",end=' ')\n",
    "                    img=img+pre_region_image[idx] \n",
    "                print(\"=\",len(img),end=\" \")\n",
    "\n",
    "                #check whether it has duplicates\n",
    "                if (len(img) != len(set(img))):\n",
    "                    img=list(set(img)) #remove duplicates\n",
    "                    print(\"     **duplicate, shrink to\",len(img),end=\"\\n\")  \n",
    "                else:\n",
    "                    print(end=\"\\n\")\n",
    "\n",
    "                #append\n",
    "                merged_reg_and_nei_image.append(img)\n",
    "\n",
    "            print(\"\\nmerged_reg_and_nei_image\")\n",
    "            for i in range(len(merged_reg_and_nei_image)):\n",
    "                print(len(merged_reg_and_nei_image[i]),merged_reg_and_nei_image[i][:5],\"...\")\n",
    "\n",
    "        # save\n",
    "        if (SAVE_bool):\n",
    "            with open(newpath+'/merged_region_image_0.pickle', 'wb') as f:\n",
    "                pickle.dump([merged_reg_and_nei, merged_reg_and_nei_image], f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_part(PATH5,ITE):\n",
    "    TRIALS          = 5\n",
    "\n",
    "    savelog_path = newpath+'/' + 'log.txt'\n",
    "\n",
    "    # ==== test_array ====\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "        \n",
    "    if RAW_2D_DATA: # 2D\n",
    "        print(\"\")\n",
    "    else: # 1D\n",
    "        test_array = np.expand_dims(test_array, axis = -1)\n",
    "\n",
    "    \n",
    "    #if((DATASET==2) or (DATASET==4)):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #elif(DATASET==1):\n",
    "    #    test_array = np.expand_dims(test_array, axis = -1)\n",
    "    #    test_array /= 255\n",
    "    #elif(DATASET==0):\n",
    "    #    test_array /= 255\n",
    "    #display(np.shape(test_array))\n",
    "\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    region_image=merged_region_image.copy()\n",
    "    del merged_reg_and_nei\n",
    "\n",
    "\n",
    "    NUM_region=len(region_image)\n",
    "    print(\"NUM_region\",NUM_region)\n",
    "\n",
    "\n",
    "    from itertools import chain\n",
    "    region_image_flatten=list(chain.from_iterable(region_image))\n",
    "    print(\"number of clean images\",len(region_image_flatten))\n",
    "\n",
    "\n",
    "    ROUND_start = time.time()\n",
    "    #========  merge ==========\n",
    "    #prepare selected_region, region\n",
    "    for n in range(1): #extra_original\n",
    "    #   #reset\n",
    "        region=region_image.copy()\n",
    "        region=list(region)\n",
    "        selected_region = list(range(NUM_region))  #[0,1,2, ... ,29]\n",
    "\n",
    "        #merge\n",
    "        if (n > 4):\n",
    "            p1=comb[n-1][0]\n",
    "            p2=comb[n-1][1]\n",
    "            region[p1]=region[p1]+region[p2]\n",
    "            region.pop(p2)\n",
    "            selected_region.pop(-1)  # remove last region index\n",
    "        #original\n",
    "        else:  #n=0\n",
    "            p1=0\n",
    "            p2=0\n",
    "\n",
    "        print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "        # ===== one CNN =============\n",
    "        NUM_CLASSES = len(selected_region)  #NUM_CLASSES should be here to update for each loop\n",
    "\n",
    "        # input image and label\n",
    "        Input_img     = []\n",
    "        Input_img_len = []\n",
    "        for c,sel in enumerate(selected_region, start=0):\n",
    "            Input_img = Input_img + list(region[sel])\n",
    "            Input_img_len.append(len(region[sel])) #can only concatenate list (not \"int\") to list    \n",
    "            \n",
    "        # 20240319\n",
    "        if RAW_2D_DATA: # 2D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            H           = np.shape(test_array[0])[1]\n",
    "            train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "        else: # 1D\n",
    "            W           = np.shape(test_array[0])[0]\n",
    "            train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "            for i in range (len(Input_img)):\n",
    "                train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "                  \n",
    "        train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "        # fill up the training label to each training image\n",
    "        current_train_label = np.zeros(len(train_array), dtype=int)  # Assign 0 to the label\n",
    "        accum_base=0  #accumulate\n",
    "        for label in range(1,NUM_CLASSES):\n",
    "            sector = Input_img_len[label-1]\n",
    "            accum_base = accum_base + sector  # sector is the sector length\n",
    "            current_train_label[accum_base:] = label  # fill the label\n",
    "\n",
    "\n",
    "        # CNN\n",
    "        #===============================================\n",
    "        one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "        one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "        model_history = np.zeros(TRIALS, dtype=list)\n",
    "        print(\"NUM_CLASSES\",NUM_CLASSES)\n",
    "        print(\"current_train_label: \",list(set(current_train_label)))\n",
    "        for r in range(TRIALS):  #10\n",
    "            one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                    x_train     = train_array,\n",
    "                    train_label = current_train_label,\n",
    "                    test_array  = test_array,\n",
    "                    true_answer = test_label_answer,\n",
    "                    Num_Classes = NUM_CLASSES\n",
    "                    )\n",
    "            print(type(model_history))\n",
    "\n",
    "\n",
    "            # ===== delete CNN tensors =====\n",
    "            from keras import backend as K\n",
    "            K.clear_session()\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "            print(\"One CNN, r: \",r)\n",
    "            ROUND_duration = time.time() - ROUND_start\n",
    "            print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "\n",
    "        # === save to file ===\n",
    "        #This is useless in phase IV. Prepare for further checking in the future.\n",
    "        savefile_path = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'_'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path, 'wb') as f:\n",
    "            pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "        savefile_path2 = str(newpath) +  '/(classes=' + str(NUM_CLASSES)+')_5_tests_simple_ITE'+str(ITE)+'.pickle'  #extra_original\n",
    "        with open(savefile_path2, 'wb') as f:\n",
    "            pickle.dump([one_predicted_results, one_predict_percentage], f)\n",
    "\n",
    "        # === save to log ===    \n",
    "        savelog = open(savelog_path, 'a+')\n",
    "        print(\"\\n\", savefile_path, file = savelog)\n",
    "        print(\"Saved parameters: Input_img, Input_img_len, one_predicted_results, one_predict_percentage\", file = savelog) #0722\n",
    "\n",
    "        # total time\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Completion time: \", datetime.datetime.now(), file = savelog)\n",
    "        print(\"Total Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)), file = savelog)\n",
    "\n",
    "        savelog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic_method(PATH5,NUM_region,region_label,table_1D):\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "    \n",
    "    dist_table_truth=np.zeros((NUM_region,NUM_region),dtype=int)\n",
    "    region_correct=[]\n",
    "    region_amount=[]\n",
    "    overall_correct=0\n",
    "    overall_amount=0\n",
    "\n",
    "    for i in range(NUM_region):\n",
    "        #(1) input\n",
    "        region_image=np.where(table_1D==i)[0]\n",
    "        #region_image=merged_region_image[i].copy()\n",
    "        \n",
    "        #(2) establish confusion matrix\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(test_label_answer[region_image]==j)[0]) #the number of images which equals to true answer \n",
    "        \n",
    "        #(3) statisitc\n",
    "        region_correct.append(dist_table_truth[i][region_label[i]])\n",
    "        region_amount.append(len(region_image))\n",
    "      \n",
    "    #(4) statistic for overall\n",
    "    overall_correct=sum(region_correct)\n",
    "    overall_amount=sum(region_amount)\n",
    "\n",
    "    return region_correct,region_amount,overall_correct,overall_amount,dist_table_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistic(PATH5,ITE):\n",
    "    # input 1:\n",
    "    # (1)merged_region_image_(ITE)\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    del merged_reg_and_nei\n",
    "    NUM_region=len(merged_region_image)\n",
    "    \n",
    "    # (2)test_label_answer\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "    # (3)get consistent result table\n",
    "    with open(newpath+'/(classes=' + str(NUM_region) + ')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "    LENGTH=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    " \n",
    "    # (4) get region_label \n",
    "    region_label=[] #true label by selecting dominate ones\n",
    "    for i in range(NUM_region):\n",
    "        region_image=merged_region_image[i].copy()\n",
    "        region_label.append(collections.Counter(test_label_answer[region_image]).most_common()[0][0])  #images --> true label --> most_common label\n",
    "\n",
    "\n",
    "   #========================================     \n",
    "   # (1)train + test\n",
    "    a2,b2,c2,d2,e2=statistic_method(PATH5,NUM_region,region_label,Original_result)\n",
    "    na2=np.asarray(a2)\n",
    "    nb2=np.asarray(b2)\n",
    "    nc2=np.asarray(c2)\n",
    "    nd2=np.asarray(d2)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c2, d2,      round(nc2/nd2    ,3), \"5con over all, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c2, all_num, round(nc2/all_num,3), \"5con over all\"], csv_path1)\n",
    " \n",
    "\n",
    "        \n",
    "    # (2)train\n",
    "    train_results=-1*np.ones(LENGTH,dtype=int)\n",
    "    for i in range(NUM_region):\n",
    "        images=merged_region_image[i]\n",
    "        train_results[images]=i\n",
    "        print(\"num of merged_region_image\",i,len(merged_region_image[i]))\n",
    "    print(collections.Counter(train_results))\n",
    "        \n",
    "    a1,b1,c1,d1,e1=statistic_method(PATH5,NUM_region,region_label,train_results)\n",
    "    na1=np.asarray(a1)  #region_correct\n",
    "    nb1=np.asarray(b1)  #region_amount\n",
    "    nc1=np.asarray(c1)  #overall_correct\n",
    "    nd1=np.asarray(d1)  #overall_amount\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c1, d1, round(nc1/nd1,3), \"5con over trained\"], csv_path1)\n",
    "    append_csv([ITE, c1, all_num, round(nc1/all_num,3), \"5con over all\"], csv_path1)\n",
    "    \n",
    "        \n",
    "    # (3)test\n",
    "    # remove training data(good images), only check test data(bad images)\n",
    "    from itertools import chain\n",
    "    used_image=merged_region_image.copy()\n",
    "    used_image=list(chain.from_iterable(used_image))\n",
    "    Original_result2=Original_result.copy()\n",
    "    Original_result2[used_image]=-2\n",
    "\n",
    "        \n",
    "    a4,b4,c4,d4,e4=statistic_method(PATH5,NUM_region,region_label, Original_result2)\n",
    "    na4=np.asarray(a4)\n",
    "    nb4=np.asarray(b4)\n",
    "    nc4=np.asarray(c4)\n",
    "    nd4=np.asarray(d4) #this is len(Original_result)-len(used_image)-len(unconsistent)\n",
    "    untrain=len(Original_result)-len(used_image)\n",
    "    all_num=len(Original_result)\n",
    "    append_csv([ITE, c4, untrain, round(nc4/untrain, 3), \"5con over untrained, but 5-consensus\"], csv_path1)\n",
    "    append_csv([ITE, c4, all_num, round(nc4/all_num, 3), \"5con over untrained (unclean)\"], csv_path1)\n",
    "\n",
    "        \n",
    "    # (4)set majority as label for each image        \n",
    "    # set majority from 5 trias as label for each image\n",
    "    predicted_results_major=np.zeros(LENGTH,dtype=int)\n",
    "    for i in range(LENGTH):\n",
    "        predicted_results_major[i]=collections.Counter(one_predicted_results.T[i]).most_common()[0][0]\n",
    "    \n",
    "\n",
    "    a3,b3,c3,d3,e3=statistic_method(PATH5,NUM_region,region_label,predicted_results_major)\n",
    "    na3=np.asarray(a3)\n",
    "    nb3=np.asarray(b3)\n",
    "    nc3=np.asarray(c3)\n",
    "    nd3=np.asarray(d3) #this is all in majority criterion\n",
    "    append_csv([ITE, c3, d3, round(nc3/nd3    ,3), \"majo over all\"], csv_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_and_expand(PATH5,ITE):\n",
    "# all4.     \n",
    "    # load\n",
    "    with open('./region_initials.pickle', 'rb') as f:\n",
    "        all_region_index, all_region_image = pickle.load(f)\n",
    "    MAX_region = max(all_region_index)\n",
    "\n",
    "    with open(newpath+'/merged_region_image_'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        merged_reg_and_nei, merged_region_image = pickle.load(f)\n",
    "    NUM_region = len(merged_reg_and_nei) # NUM_region is the number of clusters\n",
    "\n",
    "    with open(newpath+'/(classes='+str(NUM_region)+')_5_tests_simple_ITE'+str(ITE)+'.pickle', 'rb') as f:\n",
    "        one_predicted_results, one_predict_percentage = pickle.load(f)\n",
    "    del one_predict_percentage\n",
    "\n",
    "    with open(PATH5, 'rb') as f:\n",
    "        test_array, test_label_answer = pickle.load(f)\n",
    "    del test_array\n",
    "\n",
    "\n",
    "    # choose absolutely consistent images\n",
    "    NUM_test=np.shape(one_predicted_results)[1]\n",
    "    Original_result=np.zeros(NUM_test,dtype=int)\n",
    "\n",
    "    # (***)\n",
    "    # As set contains only unique elements, so convert the list to set.\n",
    "    # If set size is 1 then it means all elements in given list are same\n",
    "    for i in range(NUM_test):\n",
    "        if (len(set(one_predicted_results.T[i])) == 1):  # (***)\n",
    "            Original_result[i]=one_predicted_results[0][i]\n",
    "        else:\n",
    "            Original_result[i]=-1\n",
    "    \n",
    "    used_img=list(chain.from_iterable(merged_region_image))\n",
    "    used_img=np.sort(used_img)\n",
    "    working_img = np.asarray(list(  set(range(NUM_test))-set(used_img)  ))  #working_img means the unclean ones for working on the further adding process\n",
    "    print(\"===========  ITE =\",ITE, \"  ===========\")    \n",
    "    print(\"used_img\",len(used_img), len(set(used_img)))    \n",
    "    print(\"working_img(=other images=unclean images)\",len(working_img), len(set(working_img)))\n",
    "\n",
    "    # save clean and unclean images\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/clean_and_unclean_image_ITE='+str(ITE)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([used_img, working_img], f) #used_img is clean, working_img is others\n",
    "\n",
    "    # other_regions\n",
    "    # ==== Process of other regions. Generate \"other_regions\" ====\n",
    "    merged_reg_and_nei_flatten=list(chain.from_iterable(merged_reg_and_nei))\n",
    "    print(\"merged regions\", len(merged_reg_and_nei_flatten), len(set(merged_reg_and_nei_flatten)))\n",
    "    other_regions       = list(  set(range(1,MAX_region+1))-set(merged_reg_and_nei_flatten)  ) #region index exclude used regions. 1 to 200.\n",
    "    print(\"other_regions\",len(other_regions), len(set(other_regions)))\n",
    "    \n",
    "    dmn_img             = [] # Index of dmn_img is consistent with other_regions\n",
    "    NUM_other_regions   = len(other_regions) # number of clusters in other regions\n",
    "    dist_table_truth    = np.zeros((NUM_other_regions,NUM_region),dtype=int)\n",
    "    p_reg_label_dmn     = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in predicted lagels.\n",
    "    grd_reg_answer_dmn  = np.zeros(NUM_other_regions,dtype=int)   #one value. dominate label in true answers.\n",
    "    p_reg_dmn_rate      = np.zeros(NUM_other_regions,dtype=float) #one value. dominate ratio in a region\n",
    "\n",
    "    #(1) other_regions      --> establish all other region table \n",
    "    for i,region_name in enumerate(other_regions): #check all other regions\n",
    "\n",
    "        #(a)===== predicted images (multiple values) =====\n",
    "        p_img        = all_region_image[region_name-1] # In this region, get their images. \"region_name-1\" is due to region index starts from 1 to 200\n",
    "        p_img_label  = Original_result[p_img]   # Predicted labels in the region.\n",
    "        p_img_total  = len(p_img)\n",
    "        # the value of predicted labels is the index of trainning region. These indices are the labels\n",
    "        # but these p_img_answer are predicted, may not always be the truth.\n",
    "\n",
    "\n",
    "        #(b)===== region dominate; one value =====\n",
    "        #region label\n",
    "        p_reg_label_dmn[i] = collections.Counter(p_img_label).most_common()[0][0] # one value\n",
    "        # region dominate rate\n",
    "        if(p_reg_label_dmn[i]>=0):\n",
    "            p_reg_dmn_rate[i] = collections.Counter(p_img_label).most_common()[0][1]/p_img_total\n",
    "        else:              # means invalid label\n",
    "            p_reg_dmn_rate[i] = 0\n",
    "\n",
    "\n",
    "        #(c)==== ground truth =====\n",
    "        grd_label                 = test_label_answer[p_img]  #multiple values\n",
    "        grd_reg_answer_dmn[i]     = collections.Counter(grd_label).most_common()[0][0] #one value\n",
    "\n",
    "\n",
    "        #(d)==== establish confusion table=====\n",
    "        for j in range(NUM_region):\n",
    "            dist_table_truth[i][j]=len(np.where(grd_label==j)[0])\n",
    "\n",
    "\n",
    "        #(e)=== collect dominated images =============\n",
    "        addr2=np.where( (p_img_label==p_reg_label_dmn[i]) & (p_img_label>=0) )[0] # ignore -1 which are non-consistency\n",
    "        #         the labels which  == 7               the labels which >= 0\n",
    "        temp=[]\n",
    "        for k in range(len(addr2)):\n",
    "            temp.append(p_img[addr2[k]])\n",
    "        dmn_img.append(temp)\n",
    "        #=============================================\n",
    "\n",
    "\n",
    "    df1 = pandas.DataFrame({\"other index\":other_regions}) # 1 to 200   other region index\n",
    "    df2 = pandas.DataFrame({\"pred label\":p_reg_label_dmn})      \n",
    "    df4 = pandas.DataFrame({\"truth\":grd_reg_answer_dmn})\n",
    "    df6 = pandas.DataFrame({\"rate\":np.round(p_reg_dmn_rate,2)})\n",
    "    df7 = pandas.DataFrame(dist_table_truth)\n",
    "    entire_table=pandas.concat([df1, df2, df4, df6, df7], axis=1)\n",
    "    print(\"All other regions\")\n",
    "    display(entire_table)\n",
    "\n",
    "\n",
    "\n",
    "    #(2)get regions according to conditions\n",
    "    NN=5 #choose top 5 regions\n",
    "    RATE=0.7\n",
    "    candidate_reg_by_top_NN=[]\n",
    "\n",
    "    # === get candidate regions by the order of dmn label 0 to 9 ====\n",
    "    for i in range(NUM_region):\n",
    "        # (2-1) ==== select region by rate > 0.7 and top 5 ====\n",
    "        index     = np.where(p_reg_label_dmn==i)[0] # index is the index of other_regions(0~183), rather than original entire region index 1 to 200        \n",
    "        working_table = entire_table.iloc[index]\n",
    "        working_table = working_table.sort_values(by=['rate'], ascending=False)\n",
    "        working_table = working_table.loc[working_table['rate'] > RATE]  #rate > 0.7\n",
    "        NUM_region_in_one_class = len(working_table.iloc[:NN])  #top 5\n",
    "               \n",
    "        # (2-2) ==== get candidate regions ====\n",
    "        # get top N records; save only the column 'other_reg', and transfer it to list from DataFrame by \"tolist()\"\n",
    "        candidate_reg_by_top_NN.append(working_table[:NN]['other index'].tolist())\n",
    "         \n",
    "    #(3) add regions and images\n",
    "    for i in range(NUM_region):\n",
    "        added_img=[]\n",
    "        if (len(candidate_reg_by_top_NN[i])>0):\n",
    "            for j in range(len(candidate_reg_by_top_NN[i])):\n",
    "                reg_addr  = np.where( np.array(other_regions)==candidate_reg_by_top_NN[i][j] )[0][0].tolist()\n",
    "                added_img = added_img + dmn_img[reg_addr]\n",
    "            # (3-1) add image\n",
    "            temp=len(merged_region_image[i])\n",
    "            merged_region_image[i] = merged_region_image[i] + added_img\n",
    "            merged_region_image[i] = list(set(merged_region_image[i]))\n",
    "            img_amount=len(merged_region_image[i])-temp\n",
    "            \n",
    "            # (3-2) add region\n",
    "            merged_reg_and_nei[i]  = merged_reg_and_nei[i] + candidate_reg_by_top_NN[i]\n",
    "            \n",
    "            # (3-3) print out\n",
    "            print(\"added label, regions, img amount:\", set(Original_result[added_img]), candidate_reg_by_top_NN[i], img_amount)\n",
    "\n",
    "            \n",
    "    # (4) collect residual images\n",
    "    # This works only for CIFAR10. All images in the MNIST and MNIST-TRAN are clean. No this issue.        \n",
    "    #20240105\n",
    "    if (not MNIST):\n",
    "    #if ((DATASET==2) or (DATASET==4)):\n",
    "        if (len(list(chain.from_iterable(candidate_reg_by_top_NN))) == 0):  #if no extra regions\n",
    "            #20240105\n",
    "            if (True):\n",
    "            #if(DATASET==4):\n",
    "                df = pandas.read_csv(PATH4)\n",
    "                tSNE_table = df.to_numpy()[:,:3]\n",
    "            else:\n",
    "                df = pandas.read_csv(PATH8)\n",
    "                tSNE_table = df.to_numpy()\n",
    "            print(\"tSNE_table\",np.shape(tSNE_table))\n",
    "\n",
    "            working_table=tSNE_table[working_img]\n",
    "            pairwise_dist=squareform(pdist(working_table, 'euclidean'))\n",
    "            print(\"pairwise_dist\",np.shape(pairwise_dist)) #value of data point, rather than image index\n",
    "\n",
    "            TopN=10\n",
    "            M=len(working_img)\n",
    "            nei_table_images  = np.zeros((M,TopN),dtype=int)  #contain top 10 images\n",
    "            nei_table_label   = np.zeros((M,TopN),dtype=int)\n",
    "            working_img_label = np.zeros(M,dtype=int)\n",
    "            for i in range(M):   \n",
    "                # fill up top 10 \n",
    "                addr=np.argsort(pairwise_dist[i])\n",
    "                for j in range(TopN):\n",
    "                    nei_table_images[i][j]=working_img[ addr[j+1] ] #Ignore first one. First one is itself\n",
    "                    nei_table_label[i][j] =Original_result[nei_table_images[i][j]]\n",
    "                # consistent\n",
    "                if (len(set(nei_table_label[i])) == 1): #only get the one which is entire consistent\n",
    "                    working_img_label[i]=nei_table_label[i][0]\n",
    "                else:\n",
    "                    working_img_label[i]=-1\n",
    "\n",
    "            print(\"nei_table_images\",np.shape(nei_table_images))\n",
    "            print(\"working_img_label\",working_img_label)\n",
    "\n",
    "            new_img=[] # just  for monitoring\n",
    "            for i in range(NUM_region):\n",
    "                addr=np.where(working_img_label==i)[0].tolist()\n",
    "                new_img.append(working_img[addr])\n",
    "                merged_region_image[i].extend(working_img[addr])\n",
    "            print(\"add residuals \",len(list(chain.from_iterable(new_img))))\n",
    "            print(\"number of next merged_region_image\", len(list(chain.from_iterable(merged_region_image))))\n",
    "        else:\n",
    "            print(\"Not getting into residuals\")\n",
    "\n",
    "    #save\n",
    "    if (SAVE_bool):\n",
    "        with open(newpath + '/merged_region_image_'+str(ITE+1)+'.pickle', 'wb') as f:\n",
    "            pickle.dump([merged_reg_and_nei, merged_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup region_initials.pickle\n",
    "#### For both single network and integrate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv(PATH4)\n",
    "display(df.head())\n",
    "#all_region_index = df.to_numpy().T[REGION_INDEX_LOC].astype(int)\n",
    "#print(len(all_region_index))\n",
    "all_region_index  = df[REG_COLUMN].to_numpy().astype(int)\n",
    "print(len(all_region_index))\n",
    "print(\"all_region_index\\n\",all_region_index[:5])\n",
    "\n",
    "all_region_image=[]\n",
    "MAX_region=max(all_region_index)\n",
    "for i in range(MAX_region):\n",
    "    addr=list(np.where(all_region_index==i+1)[0])\n",
    "    all_region_image.append(addr)    \n",
    "\n",
    "#save\n",
    "if (SAVE_bool):\n",
    "    with open('./region_initials.pickle', 'wb') as f:\n",
    "        pickle.dump([all_region_index, all_region_image], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for case_i in range(NUM_CASE):\n",
    "\n",
    "    #===== create folder case1, case2, case3...\n",
    "    print(\"case=\",case_i+1)\n",
    "    newpath = './case' + str(case_i+1)\n",
    "    if (not INTE_bool):\n",
    "        if not os.path.exists(newpath):   #No necessary in Integration\n",
    "            os.makedirs(newpath)\n",
    "    \n",
    "    #==== open csv 1\n",
    "    csv_path1 = newpath+'/' + 'accu_history.csv'\n",
    "    with open(csv_path1, 'a', newline='') as f:\n",
    "        csv_file = csv.writer(f)\n",
    "        csv_file.writerow(['ITE', 'correct', 'denominator', 'accu', 'description'])\n",
    "\n",
    "# 1.\n",
    "    if (not INTE_bool):\n",
    "        create_image_0(PATH6, case_i)   #No necessary in Integration\n",
    "\n",
    "\n",
    "    for ITE in range(ITE_START, ITE_END):\n",
    "# 2. CNN\n",
    "        CNN_part(PATH5,ITE)\n",
    "\n",
    "# 3. statistic\n",
    "        statistic(PATH5,ITE)\n",
    "\n",
    "# 4. merged_and_expand \n",
    "        merged_and_expand(PATH5,ITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
