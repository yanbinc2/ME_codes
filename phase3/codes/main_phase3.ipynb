{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; Feb, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 3 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@ntu.edu.tw </p>\n",
    "### <p> Master Program in Statistics, National Taiwan University, Taipei, Taiwan.</p> \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import random \n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "region: [ 33  95 131  24 125  10  94 192 135  25 139 168 110  60 134  27  58  28]\n",
      "region amount: 18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>Class</th>\n",
       "      <th>Label</th>\n",
       "      <th>Spec200</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-6.131136</td>\n",
       "      <td>-11.714952</td>\n",
       "      <td>14.559869</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.653914</td>\n",
       "      <td>-15.913769</td>\n",
       "      <td>8.271565</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.666949</td>\n",
       "      <td>-15.911179</td>\n",
       "      <td>8.316633</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.160013</td>\n",
       "      <td>-8.344077</td>\n",
       "      <td>19.694381</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.430728</td>\n",
       "      <td>-5.483346</td>\n",
       "      <td>17.380125</td>\n",
       "      <td>Cherry</td>\n",
       "      <td>2</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1         X2         X3   Class  Label  Spec200\n",
       "0 -6.131136 -11.714952  14.559869  Cherry      2      100\n",
       "1  4.653914 -15.913769   8.271565  Cherry      2        6\n",
       "2  4.666949 -15.911179   8.316633  Cherry      2        6\n",
       "3 -1.160013  -8.344077  19.694381  Cherry      2      166\n",
       "4 -0.430728  -5.483346  17.380125  Cherry      2      170"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_region_index\n",
      " 43217 [100   6   6 ... 186  83  32]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    5000\n",
       "1    4386\n",
       "2    5000\n",
       "3    5000\n",
       "4    4457\n",
       "5    4876\n",
       "6    5000\n",
       "7    4498\n",
       "8    5000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Input\n",
    "PATH1='../data/seedinds.txt'\n",
    "PATH2='../data/bilabels.txt'\n",
    "PATH3='../data/seedinds_neighborregions.txt'\n",
    "PATH4='../data/ResNet18_PlantDisease_45K_Spec200.csv'\n",
    "PATH5='../data/ResNet18_PlantDisease_45K_Values.csv'\n",
    "\n",
    "\n",
    "#===  parameters ========================================================\n",
    "TRIALS    = 1\n",
    "timestr   = ''\n",
    "REG_COLUMN = \"Spec200\"\n",
    "RAW_2D_DATA = False\n",
    "\n",
    "if RAW_2D_DATA: # 2D\n",
    "    from CNN_Modules import ME_CNN\n",
    "else: # 1D\n",
    "    from CNN_Modules_1D import ME_CNN\n",
    "\n",
    "\n",
    "# (1)PATH1\n",
    "df1 = pandas.read_csv(PATH1, header=None, delimiter = \"\\t\")\n",
    "region = df1.to_numpy().T[0]\n",
    "NUM_region = len(region)\n",
    "\n",
    "# (2)PATH2\n",
    "df2 = pandas.read_csv(PATH2, header=None, delimiter = \"\\t\")\n",
    "cen = df2.to_numpy()\n",
    "\n",
    "print('region:', region)\n",
    "print('region amount:', len(region))\n",
    "\n",
    "\n",
    "# (4)PATH4. Have to be here. The following neighboring process needs this information\n",
    "df4 = pandas.read_csv(PATH4)\n",
    "display(df4.head())\n",
    "#all_region_index  = df.iloc[:,REGION_INDEX_LOC].to_numpy().astype(int)\n",
    "all_region_index  = df4[REG_COLUMN].to_numpy().astype(int)\n",
    "print(\"all_region_index\\n\",len(all_region_index), all_region_index)\n",
    "\n",
    "\n",
    "# (4)PATH4 reset label\n",
    "label=df4['Label'].value_counts(sort=False).keys()\n",
    "_=[df4['Label'].replace(to_replace=label[i], value=i, inplace=True) for i in range(len(label))]\n",
    "display(df4['Label'].value_counts(sort=False))\n",
    "test_label_answer = df4[\"Label\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.3\n"
     ]
    }
   ],
   "source": [
    "df1\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 8 8 8] \n",
      " [[ -6.131136   -11.71495238  14.55986935]\n",
      " [  4.65391405 -15.91376892   8.27156454]\n",
      " [  4.66694856 -15.91117949   8.31663274]\n",
      " ...\n",
      " [ -1.49778226  27.08358869 -12.30754204]\n",
      " [ -0.53695937  14.94667994  -7.44467661]\n",
      " [ -6.71398626   0.54063622   0.42243439]]\n"
     ]
    }
   ],
   "source": [
    "# (5)PATH5     convert the embedded data into the pickle file\n",
    "df5 = pandas.read_csv(PATH5)\n",
    "test_array = df5.to_numpy()\n",
    "\n",
    "\n",
    "#save\n",
    "PATH5='../data/embedded_data.pickle'   #replace original PATH 5\n",
    "with open(PATH5, 'wb') as f:\n",
    "    pickle.dump([test_array, test_label_answer], f)\n",
    "print(test_label_answer, \"\\n\", test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neighbor amount:  90\n",
      "[[138  55  48  87   1]\n",
      " [ 71 158 144  81  69]\n",
      " [ 80  65  77  57 164]\n",
      " [ 15  63 146 115 109]\n",
      " [ 68 127  36  67 174]\n",
      " [197 120  96 130  50]\n",
      " [169 145 137 151  72]\n",
      " [116 147  91  64 198]\n",
      " [165  37  73 117 171]\n",
      " [ 17 156 175  88  66]\n",
      " [185 162  21 136  13]\n",
      " [113  20  84 199  16]\n",
      " [ 54 108 111  52   7]\n",
      " [ 61 160 121  26 146]\n",
      " [155 161  41  86 157]\n",
      " [178  56   5  42  75]\n",
      " [126 180  89 179  72]\n",
      " [ 59 153  44 167  18]]\n",
      "neighbor amount:  88\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[138, 55, 48, 87, 1],\n",
       " [71, 158, 144, 81, 69],\n",
       " [80, 65, 77, 57, 164],\n",
       " [15, 63, 146, 115, 109],\n",
       " [68, 127, 36, 67, 174],\n",
       " [197, 120, 96, 130, 50],\n",
       " [169, 145, 137, 151, 72],\n",
       " [116, 147, 91, 64, 198],\n",
       " [165, 37, 73, 117, 171],\n",
       " [17, 156, 175, 88, 66],\n",
       " [185, 162, 21, 136, 13],\n",
       " [113, 20, 84, 199, 16],\n",
       " [54, 108, 111, 52, 7],\n",
       " [61, 160, 121, 26],\n",
       " [155, 161, 41, 86, 157],\n",
       " [178, 56, 5, 42, 75],\n",
       " [126, 180, 89, 179],\n",
       " [59, 153, 44, 167, 18]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if (PATH3):\n",
    "    df = pandas.read_csv(PATH3, delim_whitespace=' ', header=None,  index_col=0)\n",
    "    neighbors = df.to_numpy()\n",
    "    NUM_NEI=df.shape[1]\n",
    "    \n",
    "    print('neighbor amount: ',len(list(chain.from_iterable(neighbors))))\n",
    "    print(neighbors)\n",
    "\n",
    "# filter out duplicated ones\n",
    "    test_list=list(chain.from_iterable(neighbors))\n",
    "    res2=[]\n",
    "    [res2.append(n) for n, i in enumerate(test_list) if i in test_list[:n]]\n",
    "\n",
    "    res2.reverse()\n",
    "    neighbors=neighbors.tolist()\n",
    "    [neighbors[x//NUM_NEI].pop(x%NUM_NEI) for x in res2]\n",
    "    print('neighbor amount: ',len(list(chain.from_iterable(neighbors))))\n",
    "    display(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[33, 138, 55, 48, 87, 1],\n",
       " [95, 71, 158, 144, 81, 69],\n",
       " [131, 80, 65, 77, 57, 164],\n",
       " [24, 15, 63, 146, 115, 109],\n",
       " [125, 68, 127, 36, 67, 174],\n",
       " [10, 197, 120, 96, 130, 50],\n",
       " [94, 169, 145, 137, 151, 72],\n",
       " [192, 116, 147, 91, 64, 198],\n",
       " [135, 165, 37, 73, 117, 171],\n",
       " [25, 17, 156, 175, 88, 66],\n",
       " [139, 185, 162, 21, 136, 13],\n",
       " [168, 113, 20, 84, 199, 16],\n",
       " [110, 54, 108, 111, 52, 7],\n",
       " [60, 61, 160, 121, 26],\n",
       " [134, 155, 161, 41, 86, 157],\n",
       " [27, 178, 56, 5, 42, 75],\n",
       " [58, 126, 180, 89, 179],\n",
       " [28, 59, 153, 44, 167, 18]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine seed regions and neighbors\n",
    "if (PATH3):\n",
    "    reg_nei=[]\n",
    "    for i in range(len(neighbors)):\n",
    "        a=[region[i]]\n",
    "        b=neighbors[i]\n",
    "        if len(b):\n",
    "            c=list(np.concatenate((a,b),axis=0))\n",
    "        else:\n",
    "            c=a.copy()\n",
    "        reg_nei.append(c)\n",
    "else:\n",
    "    reg_nei=region.copy()\n",
    "\n",
    "reg_nei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect images from target region and neighboring regions\n",
    "# input : region, NUM_region, cen, all_region_index, neighbors\n",
    "# output: region_image: to save image indices corresponding to seed regions.\n",
    "#         region_answer: to save answer\n",
    "\n",
    "region_image_before=[]\n",
    "region_image=[]\n",
    "region_image_pure=[]\n",
    "for i in range(NUM_region):\n",
    "    \n",
    "    \n",
    "    #(1)neighbor  nei\n",
    "    if (PATH3):\n",
    "        addr_nei=[]\n",
    "        for j in range(len(neighbors[i])):\n",
    "            addr_nei=addr_nei+list(np.where(all_region_index==neighbors[i][j])[0])\n",
    "            #check whether it has duplicates\n",
    "            if (len(addr_nei) != len(set(addr_nei))):\n",
    "                print(\"neighbor duplicate at i=\",i,\"j=\",j)\n",
    "                addr_nei=list(set(addr_nei))\n",
    "\n",
    "    #(2)original\n",
    "    addr=list(np.where(  (all_region_index==region[i])  &  (cen.T[1]==1)  )[0])\n",
    "    if (PATH3):\n",
    "        addr=addr+addr_nei\n",
    "    #check whether it has duplicates\n",
    "    if (len(addr) != len(set(addr))):\n",
    "        print(\"** original duplicate at i=\",i,\";region\",region[i],\";duplicate size\",len(addr)-len(set(addr)))\n",
    "        addr=list(set(addr))\n",
    "    region_image.append(addr)\n",
    "\n",
    "    #(3)original before centroid (this is only for check, rather than for main codes)\n",
    "    addr_before=list(np.where(all_region_index==region[i])[0])\n",
    "    if (PATH3):\n",
    "        addr_before=addr_before+addr_nei\n",
    "    #check whether it has duplicates\n",
    "    if (len(addr_before) != len(set(addr_before))):\n",
    "        print(\"** same duplicate situation\")\n",
    "        addr_before=list(set(addr_before))\n",
    "    region_image_before.append(addr_before)\n",
    "\n",
    "    #(4)pure\n",
    "    addr_pure=list(np.where(  (all_region_index==region[i])  &  (cen.T[1]==1)  )[0])\n",
    "    region_image_pure.append(addr_pure)\n",
    "\n",
    "\n",
    "with open('../data/' + timestr + 'region_for_phase5.pickle', 'wb') as f:\n",
    "    pickle.dump([region, reg_nei, region_image_pure, region_image,], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_array: (43217, 3)\n",
      "training size: 22653\n"
     ]
    }
   ],
   "source": [
    "# ==== test_array ====\n",
    "with open(PATH5, 'rb') as f:\n",
    "    test_array, test_label_answer = pickle.load(f)\n",
    "print(\"test_array:\",np.shape(test_array))\n",
    "\n",
    "\n",
    "# 1213 add auto judge /255\n",
    "# 20240319\n",
    "if RAW_2D_DATA: # 2D\n",
    "    print(\"\")\n",
    "else: # 1D\n",
    "    test_array = np.expand_dims(test_array, axis = -1)\n",
    "test_array /= 255\n",
    "print(\"training size:\",len(np.array(list(chain.from_iterable(region_image)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CNN Original =====\n",
    "ROUND_start = time.time()\n",
    "\n",
    "for n in range(5):\n",
    "    region=region_image.copy()\n",
    "    region=list(region)\n",
    "    selected_region = list(range(NUM_region))\n",
    "\n",
    "    p1=0\n",
    "    p2=0\n",
    "\n",
    "    print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    # input image and label\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "\n",
    "    # fill up training array\n",
    "        \n",
    "    # 20240319\n",
    "    if RAW_2D_DATA: # 2D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        H           = np.shape(test_array[0])[1]\n",
    "        train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "    else: # 1D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "        \n",
    "\n",
    "\n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'.pickle'  #extra_original\n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== shift label =======\n",
    "N=5\n",
    "Original_result=[]\n",
    "Original_prob=[]\n",
    "for i in range(N):\n",
    "    with open('./' + timestr + '(classes=' + str(NUM_region) + ')_n0_R0+R0_trial' + str(i)+ '.pickle', 'rb') as f:\n",
    "        Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history = pickle.load(f)\n",
    "    label_B = one_predicted_results[0].copy()\n",
    "    label_B_prob = one_predict_percentage[0].copy()\n",
    "\n",
    "    #(1)\n",
    "    Original_result.append(label_B)\n",
    "\n",
    "    #(2)\n",
    "    Original_prob.append(label_B_prob)\n",
    "print(np.shape(Original_result), np.shape(Original_prob))\n",
    "\n",
    "# save pickle\n",
    "with open('./' + timestr + 'results_of_original.pickle', 'wb') as f:\n",
    "    pickle.dump([Original_result, Original_prob], f)\n",
    "    \n",
    "# save mat\n",
    "from scipy.io import savemat\n",
    "savemat('./' + timestr + 'results_of_original.mat', {'result_for_original':Original_result, 'prob_for_original':Original_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN combination and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====CNN combination c(20,2)======\n",
    "comb=[]\n",
    "for subset in itertools.combinations(range(NUM_region), 2):\n",
    "    comb.append(subset)\n",
    "NUM_comb=len(comb)\n",
    "display(NUM_comb)            \n",
    "\n",
    "for n in range(NUM_comb+1):\n",
    "    region=region_image.copy()\n",
    "    region=list(region)\n",
    "    selected_region = list(range(NUM_region))\n",
    "\n",
    "    if (n > 0):\n",
    "        p1=comb[n-1][0]\n",
    "        p2=comb[n-1][1]\n",
    "        region[p1]=region[p1]+region[p2]\n",
    "        region.pop(p2)\n",
    "        selected_region.pop(-1)\n",
    "    else:\n",
    "        p1=0\n",
    "        p2=0\n",
    "\n",
    "    print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "    # 20240319\n",
    "    if RAW_2D_DATA: # 2D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        H           = np.shape(test_array[0])[1]\n",
    "        train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "    else: # 1D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "\n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_n' + str(n) + '_R' + str(p1) + '+R'+ str(p2) +'.pickle'\n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "\n",
    "\n",
    "# ==== CNN Removal =====\n",
    "for n in range(NUM_region):\n",
    "    region=region_image.copy()   #reset\n",
    "    selected_region = list(range(NUM_region))\n",
    "    selected_region.pop(n)\n",
    "\n",
    "    print(\"n=\", n)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    # input image and label\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "\n",
    "    # 20240319\n",
    "    if RAW_2D_DATA: # 2D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        H           = np.shape(test_array[0])[1]\n",
    "        train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "    else: # 1D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "\n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "    \n",
    "    \n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_Remove' + str(n) +'.pickle' \n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CNN Removal =====\n",
    "for n in range(NUM_region):\n",
    "    region=region_image.copy()   #reset\n",
    "    selected_region = list(range(NUM_region))\n",
    "    selected_region.pop(n)\n",
    "\n",
    "    print(\"n=\", n)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    # input image and label\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "    # 20240319\n",
    "    if RAW_2D_DATA: # 2D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        H           = np.shape(test_array[0])[1]\n",
    "        train_array = np.zeros((len(Input_img), W, H), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W,H)\n",
    "    else: # 1D\n",
    "        W           = np.shape(test_array[0])[0]\n",
    "        train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "        for i in range (len(Input_img)):\n",
    "            train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "            \n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "    \n",
    "    \n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_Remove' + str(n) +'.pickle' \n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====shift label=====\n",
    "#=== combination =====\n",
    "comb=[]\n",
    "for oneset in itertools.combinations(range(NUM_region), 2):\n",
    "    comb.append(oneset)\n",
    "NUM_comb=len(comb)\n",
    "\n",
    "Merged_result=[]\n",
    "Merged_prob=[]\n",
    "Merged_prob_label=[]\n",
    "\n",
    "for n in range(NUM_comb):\n",
    "    label = list(range(NUM_region))\n",
    "    p1=comb[n][0]\n",
    "    p2=comb[n][1]\n",
    "\n",
    "    with open('./' + timestr + '(classes=' + str(NUM_region-1) + ')_n'+str(n+1)+'_R'+str(p1)+'+R'+str(p2)+'.pickle', 'rb') as f:\n",
    "        Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history = pickle.load(f)\n",
    "    label_B = one_predicted_results[0].copy()\n",
    "    label_B_prob = one_predict_percentage[0].copy()\n",
    "\n",
    "    #(1)shift label index\n",
    "    for p in reversed(range(p2,NUM_region-1)):\n",
    "        addr=np.where(label_B==p)[0]\n",
    "        label_B[addr]+=1\n",
    "    Merged_result.append(label_B)\n",
    "\n",
    "    #(2)\n",
    "    Merged_prob.append(label_B_prob)\n",
    "\n",
    "    #(3)\n",
    "    label.pop(p2)\n",
    "    Merged_prob_label.append(label)\n",
    "\n",
    "print(np.shape(Merged_result), np.shape(Merged_prob), np.shape(Merged_prob_label))\n",
    "\n",
    "# save pickle\n",
    "with open('./' + timestr + 'results_of_combination.pickle', 'wb') as f:\n",
    "    pickle.dump([comb, Merged_result, Merged_prob, Merged_prob_label], f)\n",
    "\n",
    "# save mat\n",
    "from scipy.io import savemat\n",
    "if (np.shape(Merged_prob)[0]<=300):\n",
    "    savemat('./' + timestr + 'results_of_combination.mat', {'combination_pairs':comb, 'result_for_merge':Merged_result, 'prob_for_merge':Merged_prob,'prob_label_for_merge': Merged_prob_label})\n",
    "    print(\"normal size = \", np.shape(Merged_prob))\n",
    "else:\n",
    "    savemat('./' + timestr + 'results_of_combination.mat', {'combination_pairs':comb, 'result_for_merge':Merged_result, 'prob_label_for_merge': Merged_prob_label})\n",
    "    savemat('./' + timestr + 'results_of_combination_prob1.mat', {'prob_for_merge1':Merged_prob[:200]})\n",
    "    savemat('./' + timestr + 'results_of_combination_prob2.mat', {'prob_for_merge2':Merged_prob[200:400]})\n",
    "    savemat('./' + timestr + 'results_of_combination_prob3.mat', {'prob_for_merge3':Merged_prob[400:]})\n",
    "    print(\"large size = \", np.shape(Merged_prob))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#======== removal ===========\n",
    "Removal_result=[]\n",
    "Removal_prob=[]\n",
    "Removal_prob_label=[]\n",
    "\n",
    "for n in range(NUM_region):\n",
    "    label = list(range(NUM_region))    \n",
    "    #reset\n",
    "    with open('./' + timestr + '(classes=' + str(NUM_region-1) + ')_Remove' + str(n) + '.pickle', 'rb') as f:\n",
    "        Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history = pickle.load(f)\n",
    "    label_B = one_predicted_results[0].copy()\n",
    "    label_B_prob = one_predict_percentage[0].copy()\n",
    "\n",
    "    for p in reversed(range(n,NUM_region-1)):\n",
    "        addr=np.where(label_B==p)[0]\n",
    "        label_B[addr]+=1\n",
    "    Removal_result.append(label_B)\n",
    "\n",
    "    #(2)\n",
    "    Removal_prob.append(label_B_prob)\n",
    "\n",
    "    #(3)\n",
    "    label.pop(n)\n",
    "    Removal_prob_label.append(label)\n",
    "\n",
    "print(np.shape(Removal_result), np.shape(Removal_prob), np.shape(Removal_prob_label))\n",
    "\n",
    "# save pickle\n",
    "with open('./' + timestr + 'results_of_removal.pickle', 'wb') as f:\n",
    "    pickle.dump([Removal_result, Removal_prob, Removal_prob_label], f)\n",
    "    \n",
    "# save mat\n",
    "from scipy.io import savemat\n",
    "savemat('./' + timestr + 'results_of_removal.mat', {'result_for_removal':Removal_result,'prob_for_removal':Removal_prob, 'prob_label_for_removal':Removal_prob_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
