{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### <span style='color:green'> ME Algorithm  &emsp;&emsp; Feb, 2024 </span>\n",
    "### <span style='color:Blue'> Phase 3 </span>\n",
    "### <p> Yan-Bin Chen (陳彥賓) &emsp; yanbin@stat.sinica.edu.tw </p>\n",
    "### <p> Institute of Statistical Science, Academia Sinica, Taipei, Taiwan.</p>  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import random \n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "TRIALS    = 1\n",
    "timestr   = ''\n",
    "#Region_Index_loc = 6 # column location; Others is 6\n",
    "REGION_INDEX_LOC = 4 # column location; MNIST is 4\n",
    "\n",
    "PATH1='../data/Plant_seedinds.txt'\n",
    "PATH2='../data/Plant_bilabels.txt'\n",
    "PATH3='../data/Plant_seedinds_neighborregions.txt'\n",
    "PATH4='../data/ResNet18_PlantDisease_45K_Spec200.csv'\n",
    "PATH5='../data/Plant_embeddings.pickle'\n",
    "#PATH5='../phase3_data/ResNet18_PlantDisease_45K_Labels.csv'\n",
    "#=================================================================\n",
    "\n",
    "# (1)PATH1\n",
    "df = pandas.read_csv(PATH1, header=None, delimiter = \"\\t\")\n",
    "region = df.to_numpy().T[0]\n",
    "NUM_region = len(region)\n",
    "\n",
    "# (2)PATH2\n",
    "df = pandas.read_csv(PATH2, header=None, delimiter = \"\\t\")\n",
    "cen = df.to_numpy()\n",
    "\n",
    "print('region:', region)\n",
    "print('region amount:', len(region))\n",
    "\n",
    "\n",
    "# (4)PATH4. Have to be here. The following neighboring process needs this information\n",
    "df = pandas.read_csv(PATH4)\n",
    "#1213 add auto judge\n",
    "all_region_index = df.to_numpy().T[REGION_INDEX_LOC].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# (5)PATH5     conver the embedded data into the pickle file\n",
    "df1 = pandas.read_csv(PATH5)\n",
    "test_array        = df1.iloc[:,:3].to_numpy().copy()\n",
    "test_label_answer = df1.iloc[:,3].to_numpy().copy()\n",
    "\n",
    "#save\n",
    "with open('../phase3_data/Small_MNIST_tSNE_embeddings.pickle', 'wb') as f:\n",
    "    pickle.dump([test_array, test_label_answer], f)\n",
    "print (test_label_answer, test_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (PATH3):\n",
    "    df = pandas.read_csv(PATH3, delim_whitespace=' ', header=None,  index_col=0)\n",
    "    neighbors = df.to_numpy()\n",
    "    NUM_NEI=df.shape[1]\n",
    "    \n",
    "    print('neighbor amount: ',len(list(chain.from_iterable(neighbors))))\n",
    "    print(neighbors)\n",
    "\n",
    "# filter out duplicated ones\n",
    "    test_list=list(chain.from_iterable(neighbors))\n",
    "    res2=[]\n",
    "    [res2.append(n) for n, i in enumerate(test_list) if i in test_list[:n]]\n",
    "\n",
    "    res2.reverse()\n",
    "    neighbors=neighbors.tolist()\n",
    "    [neighbors[x//NUM_NEI].pop(x%NUM_NEI) for x in res2]\n",
    "    print('neighbor amount: ',len(list(chain.from_iterable(neighbors))))\n",
    "    display(neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine seed regions and neighbors\n",
    "if (PATH3):\n",
    "    reg_nei=[]\n",
    "    for i in range(len(neighbors)):\n",
    "        a=[region[i]]\n",
    "        b=neighbors[i]\n",
    "        if len(b):\n",
    "            c=list(np.concatenate((a,b),axis=0))\n",
    "        else:\n",
    "            c=a.copy()\n",
    "        reg_nei.append(c)\n",
    "else:\n",
    "    reg_nei=region.copy()\n",
    "\n",
    "reg_nei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect images from target region and neighboring regions\n",
    "# input : region, NUM_region, cen, all_region_index, neighbors\n",
    "# output: region_image: to save image indices corresponding to seed regions.\n",
    "#         region_answer: to save answer\n",
    "\n",
    "region_image_before=[]\n",
    "region_image=[]\n",
    "region_image_pure=[]\n",
    "for i in range(NUM_region):\n",
    "    \n",
    "    \n",
    "    #(1)neighbor  nei\n",
    "    if (PATH3):\n",
    "        addr_nei=[]\n",
    "        for j in range(len(neighbors[i])):\n",
    "            addr_nei=addr_nei+list(np.where(all_region_index==neighbors[i][j])[0])\n",
    "            #check whether it has duplicates\n",
    "            if (len(addr_nei) != len(set(addr_nei))):\n",
    "                print(\"neighbor duplicate at i=\",i,\"j=\",j)\n",
    "                addr_nei=list(set(addr_nei))\n",
    "\n",
    "    #(2)original\n",
    "    addr=list(np.where(  (all_region_index==region[i])  &  (cen.T[1]==1)  )[0])\n",
    "    if (PATH3):\n",
    "        addr=addr+addr_nei\n",
    "    #check whether it has duplicates\n",
    "    if (len(addr) != len(set(addr))):\n",
    "        print(\"** original duplicate at i=\",i,\";region\",region[i],\";duplicate size\",len(addr)-len(set(addr)))\n",
    "        addr=list(set(addr))\n",
    "    region_image.append(addr)\n",
    "\n",
    "    #(3)original before centroid (this is only for check, rather than for main codes)\n",
    "    addr_before=list(np.where(all_region_index==region[i])[0])\n",
    "    if (PATH3):\n",
    "        addr_before=addr_before+addr_nei\n",
    "    #check whether it has duplicates\n",
    "    if (len(addr_before) != len(set(addr_before))):\n",
    "        print(\"** same duplicate situation\")\n",
    "        addr_before=list(set(addr_before))\n",
    "    region_image_before.append(addr_before)\n",
    "\n",
    "    #(4)pure\n",
    "    addr_pure=list(np.where(  (all_region_index==region[i])  &  (cen.T[1]==1)  )[0])\n",
    "    region_image_pure.append(addr_pure)\n",
    "\n",
    "\n",
    "with open('../data/' + timestr + 'region_for_phase5.pickle', 'wb') as f:\n",
    "    pickle.dump([region, reg_nei, region_image_pure, region_image,], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== test_array ====\n",
    "with open(PATH5, 'rb') as f:\n",
    "    test_array, test_label_answer = pickle.load(f)\n",
    "print(\"test_array:\",np.shape(test_array))\n",
    "\n",
    "\n",
    "# 1213 add auto judge /255\n",
    "test_array = np.expand_dims(test_array, axis = -1)\n",
    "test_array /= 255\n",
    "print(\"training size:\",len(np.array(list(chain.from_iterable(region_image)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================\n",
    "\n",
    "#1213 add auto judge\n",
    "from CNN_Modules_1D import ME_CNN\n",
    "\n",
    "# ==== CNN Original =====\n",
    "ROUND_start = time.time()\n",
    "\n",
    "for n in range(5):\n",
    "    region=region_image.copy()\n",
    "    region=list(region)\n",
    "    selected_region = list(range(NUM_region))\n",
    "\n",
    "    p1=0\n",
    "    p2=0\n",
    "\n",
    "    print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    # input image and label\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "    # 1213\n",
    "    # fill up training array\n",
    "    W           = np.shape(test_array[0])[0]\n",
    "    train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "    for i in range (len(Input_img)):\n",
    "        train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "\n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_n0_R' + str(p1) + '+R'+ str(p2) +'_trial' + str(n)+'.pickle'  #extra_original\n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== shift label =======\n",
    "N=5\n",
    "Original_result=[]\n",
    "Original_prob=[]\n",
    "for i in range(N):\n",
    "    with open('./' + timestr + '(classes=' + str(NUM_region) + ')_n0_R0+R0_trial' + str(i)+ '.pickle', 'rb') as f:\n",
    "        Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history = pickle.load(f)\n",
    "    label_B = one_predicted_results[0].copy()\n",
    "    label_B_prob = one_predict_percentage[0].copy()\n",
    "\n",
    "    #(1)\n",
    "    Original_result.append(label_B)\n",
    "\n",
    "    #(2)\n",
    "    Original_prob.append(label_B_prob)\n",
    "print(np.shape(Original_result), np.shape(Original_prob))\n",
    "\n",
    "# save pickle\n",
    "with open('./' + timestr + 'results_of_original.pickle', 'wb') as f:\n",
    "    pickle.dump([Original_result, Original_prob], f)\n",
    "    \n",
    "# save mat\n",
    "from scipy.io import savemat\n",
    "savemat('./' + timestr + 'results_of_original.mat', {'result_for_original':Original_result, 'prob_for_original':Original_prob})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN combination and removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#====CNN combination c(20,2)======\n",
    "comb=[]\n",
    "for subset in itertools.combinations(range(NUM_region), 2):\n",
    "    comb.append(subset)\n",
    "NUM_comb=len(comb)\n",
    "display(NUM_comb)            \n",
    "\n",
    "for n in range(NUM_comb+1):\n",
    "    region=region_image.copy()\n",
    "    region=list(region)\n",
    "    selected_region = list(range(NUM_region))\n",
    "\n",
    "    if (n > 0):\n",
    "        p1=comb[n-1][0]\n",
    "        p2=comb[n-1][1]\n",
    "        region[p1]=region[p1]+region[p2]\n",
    "        region.pop(p2)\n",
    "        selected_region.pop(-1)\n",
    "    else:\n",
    "        p1=0\n",
    "        p2=0\n",
    "\n",
    "    print(\"n, p1, p2\", n, p1, p2)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "    #1213\n",
    "    # fill up training array\n",
    "    W           = np.shape(test_array[0])[0]\n",
    "    train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "    for i in range (len(Input_img)):\n",
    "        train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "\n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_n' + str(n) + '_R' + str(p1) + '+R'+ str(p2) +'.pickle'\n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "\n",
    "\n",
    "\n",
    "# ==== CNN Removal =====\n",
    "for n in range(NUM_region):\n",
    "    region=region_image.copy()   #reset\n",
    "    selected_region = list(range(NUM_region))\n",
    "    selected_region.pop(n)\n",
    "\n",
    "    print(\"n=\", n)\n",
    "\n",
    "\n",
    "    # ===== one CNN =============\n",
    "    NUM_CLASSES = len(selected_region)\n",
    "\n",
    "    # input image and label\n",
    "    Input_img     = []\n",
    "    Input_img_len = []\n",
    "    for c,sel in enumerate(selected_region, start=0):\n",
    "        Input_img = Input_img + list(region[sel])\n",
    "        Input_img_len.append(len(region[sel]))\n",
    "\n",
    "    #1213\n",
    "    # fill up training array\n",
    "    W           = np.shape(test_array[0])[0]\n",
    "    train_array = np.zeros((len(Input_img), W), dtype=float)\n",
    "    for i in range (len(Input_img)):\n",
    "        train_array[i] = test_array[Input_img[i]].reshape(W)\n",
    "\n",
    "    train_array = np.expand_dims(train_array, axis = -1)\n",
    "\n",
    "\n",
    "    # fill up the training label to each training image\n",
    "    current_train_label = np.zeros(len(train_array), dtype=int)\n",
    "    accum_base=0\n",
    "    for label in range(1,NUM_CLASSES):\n",
    "        pre_cursor = Input_img_len[label-1]\n",
    "        accum_base = accum_base + pre_cursor\n",
    "        current_train_label[accum_base:] = label\n",
    "\n",
    "    # CNN\n",
    "    #===============================================\n",
    "    one_predicted_results  = np.zeros((TRIALS, len(test_label_answer)), dtype=int)\n",
    "    one_predict_percentage = np.zeros((TRIALS, len(test_label_answer), NUM_CLASSES), dtype=float)    \n",
    "    model_history = np.zeros(TRIALS, dtype=list)\n",
    "\n",
    "    for r in range(TRIALS):\n",
    "        one_predicted_results[r], one_predict_percentage[r], model_history[r] = ME_CNN(\n",
    "                x_train     = train_array,\n",
    "                train_label = current_train_label,\n",
    "                test_array  = test_array,\n",
    "                true_answer = test_label_answer,\n",
    "                Num_Classes = NUM_CLASSES\n",
    "                )\n",
    "        print(type(model_history))\n",
    "\n",
    "\n",
    "        # ===== delete CNN tensors =====\n",
    "        from keras import backend as K\n",
    "        K.clear_session()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"One CNN, r: \",r)\n",
    "        ROUND_duration = time.time() - ROUND_start\n",
    "        print(\"Computing Time: \", str(datetime.timedelta(seconds=ROUND_duration)))\n",
    "\n",
    "    # === save to file ===\n",
    "    savefile_path = './' + str(timestr) + '(classes=' + str(NUM_CLASSES)+')_Remove' + str(n) +'.pickle' \n",
    "    with open(savefile_path, 'wb') as f:\n",
    "        pickle.dump([Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history], f)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====shift label=====\n",
    "#=== combination =====\n",
    "comb=[]\n",
    "for oneset in itertools.combinations(range(NUM_region), 2):\n",
    "    comb.append(oneset)\n",
    "NUM_comb=len(comb)\n",
    "\n",
    "Merged_result=[]\n",
    "Merged_prob=[]\n",
    "Merged_prob_label=[]\n",
    "\n",
    "for n in range(NUM_comb):\n",
    "    label = list(range(NUM_region))\n",
    "    p1=comb[n][0]\n",
    "    p2=comb[n][1]\n",
    "\n",
    "    with open('./' + timestr + '(classes=' + str(NUM_region-1) + ')_n'+str(n+1)+'_R'+str(p1)+'+R'+str(p2)+'.pickle', 'rb') as f:\n",
    "        Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history = pickle.load(f)\n",
    "    label_B = one_predicted_results[0].copy()\n",
    "    label_B_prob = one_predict_percentage[0].copy()\n",
    "\n",
    "    #(1)shift label index\n",
    "    for p in reversed(range(p2,NUM_region-1)):\n",
    "        addr=np.where(label_B==p)[0]\n",
    "        label_B[addr]+=1\n",
    "    Merged_result.append(label_B)\n",
    "\n",
    "    #(2)\n",
    "    Merged_prob.append(label_B_prob)\n",
    "\n",
    "    #(3)\n",
    "    label.pop(p2)\n",
    "    Merged_prob_label.append(label)\n",
    "\n",
    "print(np.shape(Merged_result), np.shape(Merged_prob), np.shape(Merged_prob_label))\n",
    "\n",
    "# save pickle\n",
    "with open('./' + timestr + 'results_of_combination.pickle', 'wb') as f:\n",
    "    pickle.dump([comb, Merged_result, Merged_prob, Merged_prob_label], f)\n",
    "\n",
    "# save mat\n",
    "from scipy.io import savemat\n",
    "if (np.shape(Merged_prob)[0]<=300):\n",
    "    savemat('./' + timestr + 'results_of_combination.mat', {'combination_pairs':comb, 'result_for_merge':Merged_result, 'prob_for_merge':Merged_prob,'prob_label_for_merge': Merged_prob_label})\n",
    "    print(\"normal size = \", np.shape(Merged_prob))\n",
    "else:\n",
    "    savemat('./' + timestr + 'results_of_combination.mat', {'combination_pairs':comb, 'result_for_merge':Merged_result, 'prob_label_for_merge': Merged_prob_label})\n",
    "    savemat('./' + timestr + 'results_of_combination_prob1.mat', {'prob_for_merge1':Merged_prob[:200]})\n",
    "    savemat('./' + timestr + 'results_of_combination_prob2.mat', {'prob_for_merge2':Merged_prob[200:400]})\n",
    "    savemat('./' + timestr + 'results_of_combination_prob3.mat', {'prob_for_merge3':Merged_prob[400:]})\n",
    "    print(\"large size = \", np.shape(Merged_prob))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#======== removal ===========\n",
    "Removal_result=[]\n",
    "Removal_prob=[]\n",
    "Removal_prob_label=[]\n",
    "\n",
    "for n in range(NUM_region):\n",
    "    label = list(range(NUM_region))    \n",
    "    #reset\n",
    "    with open('./' + timestr + '(classes=' + str(NUM_region-1) + ')_Remove' + str(n) + '.pickle', 'rb') as f:\n",
    "        Input_img, Input_img_len, one_predicted_results, one_predict_percentage, model_history = pickle.load(f)\n",
    "    label_B = one_predicted_results[0].copy()\n",
    "    label_B_prob = one_predict_percentage[0].copy()\n",
    "\n",
    "    for p in reversed(range(n,NUM_region-1)):\n",
    "        addr=np.where(label_B==p)[0]\n",
    "        label_B[addr]+=1\n",
    "    Removal_result.append(label_B)\n",
    "\n",
    "    #(2)\n",
    "    Removal_prob.append(label_B_prob)\n",
    "\n",
    "    #(3)\n",
    "    label.pop(n)\n",
    "    Removal_prob_label.append(label)\n",
    "\n",
    "print(np.shape(Removal_result), np.shape(Removal_prob), np.shape(Removal_prob_label))\n",
    "\n",
    "# save pickle\n",
    "with open('./' + timestr + 'results_of_removal.pickle', 'wb') as f:\n",
    "    pickle.dump([Removal_result, Removal_prob, Removal_prob_label], f)\n",
    "    \n",
    "# save mat\n",
    "from scipy.io import savemat\n",
    "savemat('./' + timestr + 'results_of_removal.mat', {'result_for_removal':Removal_result,'prob_for_removal':Removal_prob, 'prob_label_for_removal':Removal_prob_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
